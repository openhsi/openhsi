[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Getting Started",
    "section": "",
    "text": "This Python library is licensed under the Apache v2 License. The documentation is licensed under a Creative Commons Attribution 3.0 Australia License.\nDocumentation can be found here: https://openhsi.github.io/openhsi/.\n\n\npip install openhsi\nor\nconda install -c conda-forge openhsi\nThe source code can be found on GitHub. To install a development version see Contributing.\n\n\n\n\nPython 3.7+\n\nDepending on your camera sensor, install:\n\nXimea SDK (See https://www.ximea.com/support/wiki/apis/Python)\nFLIR Spinnaker SDK with the python package (See https://www.flir.com/products/spinnaker-sdk/)\nLUCID SDK (See https://thinklucid.com/downloads-hub/)\n\n\n\n\n\n\n\nNote\n\n\n\nA descriptive installation guide on Linux platforms can be found at https://openhsi.github.io/openhsi/tutorial_installing_linux.html\n\n\n\n\n\nThis whole software library, testing suite, documentation website, and PyPI/conda package was developed in Jupyter Notebooks using nbdev.\n\n\n\n\n\n\nNote\n\n\n\nWe have moved to nbdev2 which uses Quarto to generate this documentation site.\n\n\n\n\n\n\nIf OpenHSI has been useful for your research, please acknowledge the project in your academic publication. The OpenHSI paper has been published in MDPI Remote Sensing and can be accessed at https://doi.org/10.3390/rs14092244.\n@article{mao2022openhsi,\n  title={OpenHSI: A Complete Open-Source Hyperspectral Imaging Solution for Everyone},\n  author={Mao, Yiwei and Betters, Christopher H and Evans, Bradley and Artlett, Christopher P and Leon-Saval, Sergio G and Garske, Samuel and Cairns, Iver H and Cocks, Terry and Winter, Robert and Dell, Timothy},\n  journal={Remote Sensing},\n  volume={14},\n  number={9},\n  pages={2244},\n  year={2022},\n  publisher={MDPI}\n}\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor more detailed instructions, please see the tutorials in the sidebar of the documentation site.\n\n\n\n\nThe example shown here uses a simulated camera for testing purposes. Replace SimulatedCamera with the appropriate Python class for your own camera to work with real hardware. For example, use LucidCamera imported from openhsi.cameras inplace of SimulatedCamera.\nfrom openhsi.capture import *\n\nwith SimulatedCamera(img_path=\"assets/rocky_beach.png\", n_lines=1024, processing_lvl = 3,\n                    json_path=\"assets/cam_settings.json\",cal_path=\"assets/cam_calibration.nc\") as cam:\n    cam.collect()\n    fig = cam.show(plot_lib=\"matplotlib\",robust=True)\nThe RGB representation is made using selected bands for red, green, and blue. These bands can be customised along with options for displaying the image without outliers affecting the colour scale/contrast. The example flag used here is robust which takes the 2-98% percentile. There is also the hist_eq flag which performs histogram equalisation. If none of these flags are set, then the colour scale uses the min and max value.\nfig.opts(fig_inches=7,title=\"simulated hyperspectral datacube\")\n\n\n\n\n\n\nTip\n\n\n\nFor more information on how to use this library, check out our Quick Start Guide.\n\n\n\n\n\n\n\nThe hardware consists of a collimator tube with a slit (1) mounted in a 3D printed housing (2). A diffraction grating (3) is used to split the incoming light into its component colours to be detected on the camera sensor (4).\nWe have the following implementations in openhsi.cameras:\n\nWebCamera\nXimeaCamera\nLucidCamera\nFlirCamera\n\nThese all have the same interface so in principle, these OpenHSI cameras can be used interchangeably as long as you have the right calibration files.\n\n\n\nI’m having trouble with the software install. Do you have a guide?\nCheck out our Linux Installation Guide and Windows Installation Guide.\nWhere can I get a quick overview of what openhsi can do?\nOur Quick Start Guide is the best place to start. The sidebar also includes documentation for each software module in more detail.\nThe OpenHSI camera is a pushbroom sensor and requires motion to scan a 2D space. What kind of motion should I apply?\nAny motion that allows you to scan a 2D space is okay. This can be from translating the camera is space or from applying a rotation. (or both translation and rotation) The developers of openhsi has flown the OpenHSI camera on a drone which sweeps across an area of interest in multiple overlapping swaths. You can fly this camera on other platforms, vehicles, etc…\nHow fast should I move the camera?\nIt’ll depend on what your case is. This answer assumes you want square pixels. Assuming the cross-track (scan line) spatial resolution is 0.42 mrad in the field of view, and your altitude is 120 m, the ground sample distance is:\nGSD \\(\\approx\\) 0.00042 \\(\\times\\) 120 (using the small angle approximation) = 5 cm\nAssuming your frame rate is 98 FPS at your desired processing level, and you want to get square pixels, you want to be flying at speed\n\\(v\\) = 98 \\(\\times\\) 0.05 = 4.9 m/s\nIf you fly faster/slower than this, your datacube will appear to be stretched spatially.",
    "crumbs": [
      "Home",
      "Getting Started"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Getting Started",
    "section": "",
    "text": "pip install openhsi\nor\nconda install -c conda-forge openhsi\nThe source code can be found on GitHub. To install a development version see Contributing.",
    "crumbs": [
      "Home",
      "Getting Started"
    ]
  },
  {
    "objectID": "index.html#requirements",
    "href": "index.html#requirements",
    "title": "Getting Started",
    "section": "",
    "text": "Python 3.7+\n\nDepending on your camera sensor, install:\n\nXimea SDK (See https://www.ximea.com/support/wiki/apis/Python)\nFLIR Spinnaker SDK with the python package (See https://www.flir.com/products/spinnaker-sdk/)\nLUCID SDK (See https://thinklucid.com/downloads-hub/)\n\n\n\n\n\n\n\nNote\n\n\n\nA descriptive installation guide on Linux platforms can be found at https://openhsi.github.io/openhsi/tutorial_installing_linux.html",
    "crumbs": [
      "Home",
      "Getting Started"
    ]
  },
  {
    "objectID": "index.html#development-and-contributions",
    "href": "index.html#development-and-contributions",
    "title": "Getting Started",
    "section": "",
    "text": "This whole software library, testing suite, documentation website, and PyPI/conda package was developed in Jupyter Notebooks using nbdev.\n\n\n\n\n\n\nNote\n\n\n\nWe have moved to nbdev2 which uses Quarto to generate this documentation site.",
    "crumbs": [
      "Home",
      "Getting Started"
    ]
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "Getting Started",
    "section": "",
    "text": "If OpenHSI has been useful for your research, please acknowledge the project in your academic publication. The OpenHSI paper has been published in MDPI Remote Sensing and can be accessed at https://doi.org/10.3390/rs14092244.\n@article{mao2022openhsi,\n  title={OpenHSI: A Complete Open-Source Hyperspectral Imaging Solution for Everyone},\n  author={Mao, Yiwei and Betters, Christopher H and Evans, Bradley and Artlett, Christopher P and Leon-Saval, Sergio G and Garske, Samuel and Cairns, Iver H and Cocks, Terry and Winter, Robert and Dell, Timothy},\n  journal={Remote Sensing},\n  volume={14},\n  number={9},\n  pages={2244},\n  year={2022},\n  publisher={MDPI}\n}",
    "crumbs": [
      "Home",
      "Getting Started"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "Getting Started",
    "section": "",
    "text": "Tip\n\n\n\nFor more detailed instructions, please see the tutorials in the sidebar of the documentation site.\n\n\n\n\nThe example shown here uses a simulated camera for testing purposes. Replace SimulatedCamera with the appropriate Python class for your own camera to work with real hardware. For example, use LucidCamera imported from openhsi.cameras inplace of SimulatedCamera.\nfrom openhsi.capture import *\n\nwith SimulatedCamera(img_path=\"assets/rocky_beach.png\", n_lines=1024, processing_lvl = 3,\n                    json_path=\"assets/cam_settings.json\",cal_path=\"assets/cam_calibration.nc\") as cam:\n    cam.collect()\n    fig = cam.show(plot_lib=\"matplotlib\",robust=True)\nThe RGB representation is made using selected bands for red, green, and blue. These bands can be customised along with options for displaying the image without outliers affecting the colour scale/contrast. The example flag used here is robust which takes the 2-98% percentile. There is also the hist_eq flag which performs histogram equalisation. If none of these flags are set, then the colour scale uses the min and max value.\nfig.opts(fig_inches=7,title=\"simulated hyperspectral datacube\")\n\n\n\n\n\n\nTip\n\n\n\nFor more information on how to use this library, check out our Quick Start Guide.",
    "crumbs": [
      "Home",
      "Getting Started"
    ]
  },
  {
    "objectID": "index.html#hardware-cameras",
    "href": "index.html#hardware-cameras",
    "title": "Getting Started",
    "section": "",
    "text": "The hardware consists of a collimator tube with a slit (1) mounted in a 3D printed housing (2). A diffraction grating (3) is used to split the incoming light into its component colours to be detected on the camera sensor (4).\nWe have the following implementations in openhsi.cameras:\n\nWebCamera\nXimeaCamera\nLucidCamera\nFlirCamera\n\nThese all have the same interface so in principle, these OpenHSI cameras can be used interchangeably as long as you have the right calibration files.",
    "crumbs": [
      "Home",
      "Getting Started"
    ]
  },
  {
    "objectID": "index.html#frequently-asked-questions",
    "href": "index.html#frequently-asked-questions",
    "title": "Getting Started",
    "section": "",
    "text": "I’m having trouble with the software install. Do you have a guide?\nCheck out our Linux Installation Guide and Windows Installation Guide.\nWhere can I get a quick overview of what openhsi can do?\nOur Quick Start Guide is the best place to start. The sidebar also includes documentation for each software module in more detail.\nThe OpenHSI camera is a pushbroom sensor and requires motion to scan a 2D space. What kind of motion should I apply?\nAny motion that allows you to scan a 2D space is okay. This can be from translating the camera is space or from applying a rotation. (or both translation and rotation) The developers of openhsi has flown the OpenHSI camera on a drone which sweeps across an area of interest in multiple overlapping swaths. You can fly this camera on other platforms, vehicles, etc…\nHow fast should I move the camera?\nIt’ll depend on what your case is. This answer assumes you want square pixels. Assuming the cross-track (scan line) spatial resolution is 0.42 mrad in the field of view, and your altitude is 120 m, the ground sample distance is:\nGSD \\(\\approx\\) 0.00042 \\(\\times\\) 120 (using the small angle approximation) = 5 cm\nAssuming your frame rate is 98 FPS at your desired processing level, and you want to get square pixels, you want to be flying at speed\n\\(v\\) = 98 \\(\\times\\) 0.05 = 4.9 m/s\nIf you fly faster/slower than this, your datacube will appear to be stretched spatially.",
    "crumbs": [
      "Home",
      "Getting Started"
    ]
  },
  {
    "objectID": "api/atmos.html",
    "href": "api/atmos.html",
    "title": "atmos",
    "section": "",
    "text": "Tip\n\n\n\nThis module can be imported using: from openhsi.atmos import *\n\n\nThis module contains a 6SV1.1 Atmospheric Correction class that computes the pixel radiance given some parameters and a servable interactive SNR widget. 6SV is an open souce radiative transfer code that predicts the solar spectrum received by a sensor facing the Earth. Since it is written in Fortran, we use a Python wrapper called Py6S to expose the functionality and that means we are limited to using 6SV1.1 rather than the newer 6SV2.1.\nAdditionally, an interactive widget to compute the reflectance using Empirical Line Calibration (ELC) is provided.\n\n\nFind the closest station at http://weather.uwyo.edu/upperair/sounding.html to use the atmospheric sounding on the day. find the station number and region code (for example, south pacific is pac and new zealand is nz) Default is Willis Island in Queensland, Australia. https://py6s.readthedocs.io/en/latest/helpers.html#importing-atmospheric-profiles-from-radiosonde-data\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nPy6S states that custom aerosol profiles can be ingested but this is broken and there is no easy fix. Don’t waste a day trying…\n\n\nThe Model6SV class is callable on itself. Each time the class is initialised or called, the radiance will be recalculated. The results can be viewed using Model6SV.show which returns a figure object created using the bokeh or matplotlib backends.\n\nsource\n\n\n\n Model6SV.show (plot_lib:str='bokeh')\n\nplot the calculated radiance\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nplot_lib\nstr\nbokeh\nchoose between the “bokeh” or “matplotlib” plotting backend\n\n\nReturns\nCurve\n\noutput plot\n\n\n\nPy6S provides a method to loop over an array and return the 6SV result. However, it does not have a progress bar. For something that can take several minutes, not knowing how long it will take is genuinely frustrating. Therefore, we provide a modified version of SixSHelpers.Wavelengths.run_wavelengths called Model6SV.run_wavelengths that has a progress bar.\nTo use 6SV, Py6S will write input files and parse the output files. All this I/O is slow but we got to deal with it. Py6S is clever about it though and uses threading to do some compute while waiting for files - this ends up calling the 6SV executable in parallel. The modified version follows the same method except with the addition of a callback to update the progress bar when interations are done.\n\nsource\n\n\n\n\n Model6SV.run_wavelengths (wavelengths:&lt;built-infunctionarray&gt;,\n                           n_threads:int=None)\n\nModified version of SixSHelpers.Wavelengths.run_wavelengths that has a progress bar. This implementation uses threading (through Python’s multiprocessing API).\n\n\n\n\n\n\nTip\n\n\n\nIf you used conda to install Py6S, then the Fortran code is compiled for you with the executable path set in the install. If installing from pip, then you’ll need to follow additional instructions here and quote the path to the executable.\n\n\n\nmodel = Model6SV(wavelength_array=np.arange(350, 1000, 10)) \nmodel.show(\"matplotlib\")\n\n100%|██████████| 65/65 [00:09&lt;00:00,  7.19it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow that the at sensor radiance is calculated, the apparent reflectance can be found by dividing your measured radiance by it.\n\n\n\n\nIn order to use Empirical Line Calibration (ELC), there needs to be targets with known spectra in the scene such as calibration targets. A spectral library is what we need to feed into the ELC algorithm.\n\nsource\n\n\n\n remap (x, in_min, in_max, out_min, out_max)\n\nconvert x from between input range (in_min,in_max) to output range (out_min,out_max).\n\nsource\n\n\n\n\n SpectralLibrary (speclib_path:str=None)\n\nManages all the spectral library and interpolations as necessary.\n\nsource\n\n\n\n\n SpectralLibrary.dump (save_path:str=None)\n\nDump the spectral library to file. If no save_path provided, overwrite the original file.\nThe spectral library should be a pandas DataFrame with the first column called wavelength and the rest of the columns labelled by the name. The data should be in reflectance (for example, measured using an ASD spectrometer or similiar).\n\nsl = SpectralLibrary(\"../assets/speclib.pkl\")\nsl.speclib\n\n\n\n\n\n\n\n\nwavelength\nspectralon\ngray_small\ngreen_small\nred_small\ncyan_small\ngray\ncharcoal\nblue\nblue_take_off\n\n\n\n\n0\n350\n1.004860\n0.088191\n0.071970\n0.098128\n0.085883\n0.101887\n0.058318\n0.098823\n0.092749\n\n\n1\n351\n1.004398\n0.089546\n0.072579\n0.098610\n0.085919\n0.102933\n0.059781\n0.100108\n0.093942\n\n\n2\n352\n1.004371\n0.089546\n0.071572\n0.096913\n0.084025\n0.102238\n0.059273\n0.099544\n0.093412\n\n\n3\n353\n1.004620\n0.087913\n0.069308\n0.093765\n0.080976\n0.099957\n0.057630\n0.097805\n0.091682\n\n\n4\n354\n1.004296\n0.085593\n0.067664\n0.091687\n0.079096\n0.098291\n0.057134\n0.097217\n0.090731\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2146\n2496\n1.006842\n0.239516\n0.333679\n0.338185\n0.334990\n0.274833\n0.062169\n0.322644\n0.273633\n\n\n2147\n2497\n1.006868\n0.240093\n0.334529\n0.339180\n0.335822\n0.275595\n0.062141\n0.324242\n0.275030\n\n\n2148\n2498\n1.007111\n0.240608\n0.335397\n0.340243\n0.336710\n0.276414\n0.062232\n0.325424\n0.276053\n\n\n2149\n2499\n1.007263\n0.241110\n0.336293\n0.341387\n0.337602\n0.277276\n0.062312\n0.326167\n0.276782\n\n\n2150\n2500\n1.007191\n0.241731\n0.337202\n0.342628\n0.338453\n0.278245\n0.062381\n0.326506\n0.277072\n\n\n\n\n2151 rows × 10 columns\n\n\n\nThe USGS spectral library contains many more spectra provided as csv files. We allow you to import a folder of them.\n\nsource\n\n\n\n\n SpectralLibrary.import_USGS (directory:str, sort:bool=False)\n\nImport the ASD files from directory with optional names sorted.\n\nsource\n\n\n\n\n SpectralLibrary.import_USGS (directory:str, sort:bool=False)\n\nImport the ASD files from directory with optional names sorted.\n\nusgs_path = \"ASCIIdata_splib07b/ChapterV_Vegetation/\"\n\nsl = SpectralLibrary()\nsl.import_USGS(usgs_path)\n\nUserWarning: Spectral Library file required! Please re-initialised with the file.\n  sl = SpectralLibrary()\n\n\nAdded folder of ASD spectra to spectral library\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe plotted spectra have all be interpolated to remove all the NaNs.\n\n\nHere we plot all the ASD data in the USGS Splib07b Chapter V Vegetation database.\n\nsl.show(\"matplotlib\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is not easy to visualise all of the USGS spectra in one plot. There are so many that the legend decided to pack up and go home. The one spectra with the sharp rise near 1000 nm is from a spectra that did not include any information in the visible region which messed with the interpolation. If you are only interested in spectral matching in the infrared, this is not a problem. Likewise, if you are interested in spectral matching in the visible, then this spectra shouldn’t be detected close to anything real.\n\nsl.orig_speclib.plot(x=\"wavelength\",y=\"P.australis_CRMS-0153_dryNPV\",figsize=(8,6),\n                      xlabel=\"wavelength (nm)\",ylabel=\"reflectance\",\n                      ylim=(0,1)).legend(loc=\"upper right\")\n\n\n\n\n\n\n\n\n\n\n\n\nUsed later for interactive empirical line calibration. Since OpenHSI camera data can be processed to radiance, we use the 6SV radiance to guess what the spectral library will be in radiance. Then we compare radiances to calculate what known target to use in the ELC algorithm and hence find reflectance.\n\nsource\n\n\n\n SpectralMatcher (cal_path:str, speclib_path:str=None)\n\nMatch OpenHSI spectra against spectral library using Spectral Angle Mapper algorithm. Requires calibration pkl file with the saved 6SV model.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncal_path\nstr\n\npath to spectral library (pandas DataFrame)\n\n\nspeclib_path\nstr\nNone\n\n\n\n\n\nsource\n\n\n\n\n SpectralMatcher.topk_spectra (spectrum:&lt;built-infunctionarray&gt;, k:int=5,\n                               refine=True)\n\nMatch a spectrum against a spectral library spectra. Return the top k. Set refine to find closest match using mean squared error on the top k results.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nspectrum\narray\n\nspectrum array to compare against the loaded spectral library\n\n\nk\nint\n5\nnumber of top matches to return\n\n\nrefine\nbool\nTrue\nminimise the amplitude differences once similar spectra are found\n\n\nReturns\nDataFrame\n\nresults sorted from best match to worst\n\n\n\n\nsource\n\n\n\n\n SpectralMatcher.topk_spectra (spectrum:&lt;built-infunctionarray&gt;, k:int=5,\n                               refine=True)\n\nMatch a spectrum against a spectral library spectra. Return the top k. Set refine to find closest match using mean squared error on the top k results.\nYou can use this code snippet to find the top k=5 spectral matches. The Spectral Angle Mapper will say all gray calibration tarps have very similar spectra (spectrally flat) regardless of their percentage reflectance. The refine is there to refine the closest matches by mean squared error (which is not as fast).\nsm = SpectralMatcher(cal_path=\"../assets/cam_calibration.nc\",speclib_path=\"../assets/speclib.pkl\")\ntopk = sm.topk_spectra( some_radiance_array, k=5, refine=True)\nprint(topk)\ntopk is a pd.DataFrame with the first column as the labels in the spectral library. The second column is the score between [-1,+1] with better matches having score almost at 1.0.\nYou can visualise the matches with\nsm.show(is_rad=True)\nand the transparency level depends on the score so poorer matches are faded out.\n\n\n\n\nEssentially, Empirical Line Calibration (ELC) aims to solve a system of linear equations \\[\nL_i(\\lambda) = a(\\lambda)\\rho_i(\\lambda) + b(\\lambda)\n\\] where the radiance \\(L\\) is from what we measure, \\(\\rho\\) is the known reflectance, and \\(a\\) and \\(b\\) are the multiplier and offset respectively. This is calculated for multiple pixels \\(i\\) and wavelengths \\(\\lambda\\).\nThe implementation here involves solving these equations using linear algebra methods to find a least squares solution. It is fast enough to use this tool interactively.\n\n\n\n\n\n\nImportant\n\n\n\nOnly load datacubes that have already been corrected to radiance to this interactive widget.\n\n\n\nsource\n\n\n\n ELC (nc_path:str, old_style:bool=False, speclib_path:str=None)\n\nApply ELC for radiance datacubes\n\nsource\n\n\n\n\n ELC.setup_callbacks ()\n\nSetup the mouseover crosshair, and mouseclick callbacks.\n\n\n\n\n\n\nWarning\n\n\n\nThis interactive tool requires a Python backend. There is no interactivity in the documentation site.\n\n\n\nelc = ELC(nc_path=\"../process_data/2021_05_26-03_26_26_radiance.nc\",\n          speclib_path=\"../assets/speclib.pkl\",cal_path=\"../assets/cam_calibration.nc\")\nelc()\n\nAllocated 572.06 MB of RAM.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\nThe top panel consists of an RGB view of your dataset using a robust visualisation technique to avoid outliers blowing out the contrast. When you mouseover the RGB view, a crosshair appears. There is a toolbar on the bottom left of the RGB view. From left to right, there are the: move tool, area select tool, scroll zoom tool, tap tool, box draw tool, figure save tool (requires additional packages to be installed), a reset tool, and a Bokeh logo.\n\n\n\n\n\n\nTip\n\n\n\nMake sure you have the tap tool selected when viewing spectra so you don’t accidentally draw boxes.\n\n\nTo select the bright and dark targets for the ELC algorithm, select the box draw tool and double tap to start drawing (double tap again to finish drawing a box). All the pixels within the bounding boxes are used and updated immediately. A limit of 5 boxes are allowed. To delete drawn boxes, single click on a box and hold. While you holding, press the backspace key.\nThe bottom panel consists of a Reflectance and Radiance tab that you can switch between. When you click on a pixel on the RGB view, the spectra will update. If you are in the Reflectance tab, you will see the ELC and 6SV estimate alongside with the closest spectral matches in your spectral library. On the other hand, if you are in the Radiance tab, you will see the spectra labelled as “tap point” alongside estimates of the radiance (using the 6SV model) of the closest matches in your spectral library. You can tap on the legend to select or fade certain curves in the plot.\nFinally, on the very bottom, there are two buttons to export the ELC or 6SV datacubes to NetCDF format (keeping the original capture times, camera temperatures if there are any, and metadata). These will save to the directory the original radiance datacube is loaded from.\nLet’s see what the multiplier \\(a(\\lambda)\\) and offset \\(b(\\lambda)\\) was obtained using the gray and charcoal tarps as the bright and dark target.\n\na_plot = hv.Curve( zip(np.arange(len(elc.a_ELC)),elc.a_ELC), label=\"multiplier\").opts(\n                    xlabel=\"spectral band\",ylabel=\"value\")\nb_plot = hv.Curve( zip(np.arange(len(elc.b_ELC)),elc.b_ELC), label=\"offset\").opts(\n                    xlabel=\"spectral band\",ylabel=\"value\")\na_plot * b_plot\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nThe multiplier looks really similar to the 6SV estimate of the radiance which is not a surprise. Now to plot the results for all the calibration tarps.\n\ndf = pd.read_pickle(\"../assets/openhsi_radiance_select_targets.pkl\")\ndf.plot(x='wavelength (nm)', y=df.columns[1:],figsize=(8,6),\n          xlabel=\"wavelength (nm)\",ylabel=\"radiance (uW/cm^2/sr/nm)\",\n          ylim=(0,None),title=\"Calibration targets\").legend(loc=\"upper right\",fontsize='xx-small')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nFor exploring your datacubes.\n\nsource\n\n\n\n DataCubeViewer (nc_path:str=None, old_style:bool=False,\n                 img_aspect_ratio:float=0.25, box_sz:tuple=(1, 1),\n                 **kwargs)\n\n*Explore datacubes Optional key/val pair arguement overrides (**kwargs) ylim ylabel*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnc_path\nstr\nNone\npath to the NetCDF file\n\n\nold_style\nbool\nFalse\nif datacube is stored as cross-track, along-track, wavelength coords\n\n\nimg_aspect_ratio\nfloat\n0.25\naspect ratio for the datacube viewer\n\n\nbox_sz\ntuple\n(1, 1)\nAny binning (nrows, ncols) around the tap point. Default is a single pixel\n\n\nkwargs\nVAR_KEYWORD\n\n\n\n\n\n\nsource\n\n\n\n\n DataCubeViewer.setup_callbacks ()\n\nSetup the mouseover crosshair, and mouseclick callbacks.\n\n\n\n\n\n\nWarning\n\n\n\nThis interactive tool requires a Python backend. There is no interactivity in the documentation site.\n\n\n\ndcv = DataCubeViewer(nc_path=\"../../process_data/2021-05-26 03_26_26.011211.nc\",old_style=True,box_sz=(4,5))\ndcv()\n\nAllocated 762.75 MB of RAM.",
    "crumbs": [
      "Home",
      "api",
      "atmos"
    ]
  },
  {
    "objectID": "api/atmos.html#atmospheric-profile-from-balloon-sounding",
    "href": "api/atmos.html#atmospheric-profile-from-balloon-sounding",
    "title": "atmos",
    "section": "",
    "text": "Find the closest station at http://weather.uwyo.edu/upperair/sounding.html to use the atmospheric sounding on the day. find the station number and region code (for example, south pacific is pac and new zealand is nz) Default is Willis Island in Queensland, Australia. https://py6s.readthedocs.io/en/latest/helpers.html#importing-atmospheric-profiles-from-radiosonde-data",
    "crumbs": [
      "Home",
      "api",
      "atmos"
    ]
  },
  {
    "objectID": "api/atmos.html#aerosol-profile-from-aeronet",
    "href": "api/atmos.html#aerosol-profile-from-aeronet",
    "title": "atmos",
    "section": "",
    "text": "Warning\n\n\n\nPy6S states that custom aerosol profiles can be ingested but this is broken and there is no easy fix. Don’t waste a day trying…\n\n\nThe Model6SV class is callable on itself. Each time the class is initialised or called, the radiance will be recalculated. The results can be viewed using Model6SV.show which returns a figure object created using the bokeh or matplotlib backends.\n\nsource\n\n\n\n Model6SV.show (plot_lib:str='bokeh')\n\nplot the calculated radiance\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nplot_lib\nstr\nbokeh\nchoose between the “bokeh” or “matplotlib” plotting backend\n\n\nReturns\nCurve\n\noutput plot\n\n\n\nPy6S provides a method to loop over an array and return the 6SV result. However, it does not have a progress bar. For something that can take several minutes, not knowing how long it will take is genuinely frustrating. Therefore, we provide a modified version of SixSHelpers.Wavelengths.run_wavelengths called Model6SV.run_wavelengths that has a progress bar.\nTo use 6SV, Py6S will write input files and parse the output files. All this I/O is slow but we got to deal with it. Py6S is clever about it though and uses threading to do some compute while waiting for files - this ends up calling the 6SV executable in parallel. The modified version follows the same method except with the addition of a callback to update the progress bar when interations are done.\n\nsource\n\n\n\n\n Model6SV.run_wavelengths (wavelengths:&lt;built-infunctionarray&gt;,\n                           n_threads:int=None)\n\nModified version of SixSHelpers.Wavelengths.run_wavelengths that has a progress bar. This implementation uses threading (through Python’s multiprocessing API).\n\n\n\n\n\n\nTip\n\n\n\nIf you used conda to install Py6S, then the Fortran code is compiled for you with the executable path set in the install. If installing from pip, then you’ll need to follow additional instructions here and quote the path to the executable.\n\n\n\nmodel = Model6SV(wavelength_array=np.arange(350, 1000, 10)) \nmodel.show(\"matplotlib\")\n\n100%|██████████| 65/65 [00:09&lt;00:00,  7.19it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow that the at sensor radiance is calculated, the apparent reflectance can be found by dividing your measured radiance by it.",
    "crumbs": [
      "Home",
      "api",
      "atmos"
    ]
  },
  {
    "objectID": "api/atmos.html#spectral-library",
    "href": "api/atmos.html#spectral-library",
    "title": "atmos",
    "section": "",
    "text": "In order to use Empirical Line Calibration (ELC), there needs to be targets with known spectra in the scene such as calibration targets. A spectral library is what we need to feed into the ELC algorithm.\n\nsource\n\n\n\n remap (x, in_min, in_max, out_min, out_max)\n\nconvert x from between input range (in_min,in_max) to output range (out_min,out_max).\n\nsource\n\n\n\n\n SpectralLibrary (speclib_path:str=None)\n\nManages all the spectral library and interpolations as necessary.\n\nsource\n\n\n\n\n SpectralLibrary.dump (save_path:str=None)\n\nDump the spectral library to file. If no save_path provided, overwrite the original file.\nThe spectral library should be a pandas DataFrame with the first column called wavelength and the rest of the columns labelled by the name. The data should be in reflectance (for example, measured using an ASD spectrometer or similiar).\n\nsl = SpectralLibrary(\"../assets/speclib.pkl\")\nsl.speclib\n\n\n\n\n\n\n\n\nwavelength\nspectralon\ngray_small\ngreen_small\nred_small\ncyan_small\ngray\ncharcoal\nblue\nblue_take_off\n\n\n\n\n0\n350\n1.004860\n0.088191\n0.071970\n0.098128\n0.085883\n0.101887\n0.058318\n0.098823\n0.092749\n\n\n1\n351\n1.004398\n0.089546\n0.072579\n0.098610\n0.085919\n0.102933\n0.059781\n0.100108\n0.093942\n\n\n2\n352\n1.004371\n0.089546\n0.071572\n0.096913\n0.084025\n0.102238\n0.059273\n0.099544\n0.093412\n\n\n3\n353\n1.004620\n0.087913\n0.069308\n0.093765\n0.080976\n0.099957\n0.057630\n0.097805\n0.091682\n\n\n4\n354\n1.004296\n0.085593\n0.067664\n0.091687\n0.079096\n0.098291\n0.057134\n0.097217\n0.090731\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2146\n2496\n1.006842\n0.239516\n0.333679\n0.338185\n0.334990\n0.274833\n0.062169\n0.322644\n0.273633\n\n\n2147\n2497\n1.006868\n0.240093\n0.334529\n0.339180\n0.335822\n0.275595\n0.062141\n0.324242\n0.275030\n\n\n2148\n2498\n1.007111\n0.240608\n0.335397\n0.340243\n0.336710\n0.276414\n0.062232\n0.325424\n0.276053\n\n\n2149\n2499\n1.007263\n0.241110\n0.336293\n0.341387\n0.337602\n0.277276\n0.062312\n0.326167\n0.276782\n\n\n2150\n2500\n1.007191\n0.241731\n0.337202\n0.342628\n0.338453\n0.278245\n0.062381\n0.326506\n0.277072\n\n\n\n\n2151 rows × 10 columns\n\n\n\nThe USGS spectral library contains many more spectra provided as csv files. We allow you to import a folder of them.\n\nsource\n\n\n\n\n SpectralLibrary.import_USGS (directory:str, sort:bool=False)\n\nImport the ASD files from directory with optional names sorted.\n\nsource\n\n\n\n\n SpectralLibrary.import_USGS (directory:str, sort:bool=False)\n\nImport the ASD files from directory with optional names sorted.\n\nusgs_path = \"ASCIIdata_splib07b/ChapterV_Vegetation/\"\n\nsl = SpectralLibrary()\nsl.import_USGS(usgs_path)\n\nUserWarning: Spectral Library file required! Please re-initialised with the file.\n  sl = SpectralLibrary()\n\n\nAdded folder of ASD spectra to spectral library\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe plotted spectra have all be interpolated to remove all the NaNs.\n\n\nHere we plot all the ASD data in the USGS Splib07b Chapter V Vegetation database.\n\nsl.show(\"matplotlib\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is not easy to visualise all of the USGS spectra in one plot. There are so many that the legend decided to pack up and go home. The one spectra with the sharp rise near 1000 nm is from a spectra that did not include any information in the visible region which messed with the interpolation. If you are only interested in spectral matching in the infrared, this is not a problem. Likewise, if you are interested in spectral matching in the visible, then this spectra shouldn’t be detected close to anything real.\n\nsl.orig_speclib.plot(x=\"wavelength\",y=\"P.australis_CRMS-0153_dryNPV\",figsize=(8,6),\n                      xlabel=\"wavelength (nm)\",ylabel=\"reflectance\",\n                      ylim=(0,1)).legend(loc=\"upper right\")",
    "crumbs": [
      "Home",
      "api",
      "atmos"
    ]
  },
  {
    "objectID": "api/atmos.html#spectral-matcher",
    "href": "api/atmos.html#spectral-matcher",
    "title": "atmos",
    "section": "",
    "text": "Used later for interactive empirical line calibration. Since OpenHSI camera data can be processed to radiance, we use the 6SV radiance to guess what the spectral library will be in radiance. Then we compare radiances to calculate what known target to use in the ELC algorithm and hence find reflectance.\n\nsource\n\n\n\n SpectralMatcher (cal_path:str, speclib_path:str=None)\n\nMatch OpenHSI spectra against spectral library using Spectral Angle Mapper algorithm. Requires calibration pkl file with the saved 6SV model.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncal_path\nstr\n\npath to spectral library (pandas DataFrame)\n\n\nspeclib_path\nstr\nNone\n\n\n\n\n\nsource\n\n\n\n\n SpectralMatcher.topk_spectra (spectrum:&lt;built-infunctionarray&gt;, k:int=5,\n                               refine=True)\n\nMatch a spectrum against a spectral library spectra. Return the top k. Set refine to find closest match using mean squared error on the top k results.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nspectrum\narray\n\nspectrum array to compare against the loaded spectral library\n\n\nk\nint\n5\nnumber of top matches to return\n\n\nrefine\nbool\nTrue\nminimise the amplitude differences once similar spectra are found\n\n\nReturns\nDataFrame\n\nresults sorted from best match to worst\n\n\n\n\nsource\n\n\n\n\n SpectralMatcher.topk_spectra (spectrum:&lt;built-infunctionarray&gt;, k:int=5,\n                               refine=True)\n\nMatch a spectrum against a spectral library spectra. Return the top k. Set refine to find closest match using mean squared error on the top k results.\nYou can use this code snippet to find the top k=5 spectral matches. The Spectral Angle Mapper will say all gray calibration tarps have very similar spectra (spectrally flat) regardless of their percentage reflectance. The refine is there to refine the closest matches by mean squared error (which is not as fast).\nsm = SpectralMatcher(cal_path=\"../assets/cam_calibration.nc\",speclib_path=\"../assets/speclib.pkl\")\ntopk = sm.topk_spectra( some_radiance_array, k=5, refine=True)\nprint(topk)\ntopk is a pd.DataFrame with the first column as the labels in the spectral library. The second column is the score between [-1,+1] with better matches having score almost at 1.0.\nYou can visualise the matches with\nsm.show(is_rad=True)\nand the transparency level depends on the score so poorer matches are faded out.",
    "crumbs": [
      "Home",
      "api",
      "atmos"
    ]
  },
  {
    "objectID": "api/atmos.html#interactive-datacube-explorer-with-empirical-line-calibration",
    "href": "api/atmos.html#interactive-datacube-explorer-with-empirical-line-calibration",
    "title": "atmos",
    "section": "",
    "text": "Essentially, Empirical Line Calibration (ELC) aims to solve a system of linear equations \\[\nL_i(\\lambda) = a(\\lambda)\\rho_i(\\lambda) + b(\\lambda)\n\\] where the radiance \\(L\\) is from what we measure, \\(\\rho\\) is the known reflectance, and \\(a\\) and \\(b\\) are the multiplier and offset respectively. This is calculated for multiple pixels \\(i\\) and wavelengths \\(\\lambda\\).\nThe implementation here involves solving these equations using linear algebra methods to find a least squares solution. It is fast enough to use this tool interactively.\n\n\n\n\n\n\nImportant\n\n\n\nOnly load datacubes that have already been corrected to radiance to this interactive widget.\n\n\n\nsource\n\n\n\n ELC (nc_path:str, old_style:bool=False, speclib_path:str=None)\n\nApply ELC for radiance datacubes\n\nsource\n\n\n\n\n ELC.setup_callbacks ()\n\nSetup the mouseover crosshair, and mouseclick callbacks.\n\n\n\n\n\n\nWarning\n\n\n\nThis interactive tool requires a Python backend. There is no interactivity in the documentation site.\n\n\n\nelc = ELC(nc_path=\"../process_data/2021_05_26-03_26_26_radiance.nc\",\n          speclib_path=\"../assets/speclib.pkl\",cal_path=\"../assets/cam_calibration.nc\")\nelc()\n\nAllocated 572.06 MB of RAM.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\nThe top panel consists of an RGB view of your dataset using a robust visualisation technique to avoid outliers blowing out the contrast. When you mouseover the RGB view, a crosshair appears. There is a toolbar on the bottom left of the RGB view. From left to right, there are the: move tool, area select tool, scroll zoom tool, tap tool, box draw tool, figure save tool (requires additional packages to be installed), a reset tool, and a Bokeh logo.\n\n\n\n\n\n\nTip\n\n\n\nMake sure you have the tap tool selected when viewing spectra so you don’t accidentally draw boxes.\n\n\nTo select the bright and dark targets for the ELC algorithm, select the box draw tool and double tap to start drawing (double tap again to finish drawing a box). All the pixels within the bounding boxes are used and updated immediately. A limit of 5 boxes are allowed. To delete drawn boxes, single click on a box and hold. While you holding, press the backspace key.\nThe bottom panel consists of a Reflectance and Radiance tab that you can switch between. When you click on a pixel on the RGB view, the spectra will update. If you are in the Reflectance tab, you will see the ELC and 6SV estimate alongside with the closest spectral matches in your spectral library. On the other hand, if you are in the Radiance tab, you will see the spectra labelled as “tap point” alongside estimates of the radiance (using the 6SV model) of the closest matches in your spectral library. You can tap on the legend to select or fade certain curves in the plot.\nFinally, on the very bottom, there are two buttons to export the ELC or 6SV datacubes to NetCDF format (keeping the original capture times, camera temperatures if there are any, and metadata). These will save to the directory the original radiance datacube is loaded from.\nLet’s see what the multiplier \\(a(\\lambda)\\) and offset \\(b(\\lambda)\\) was obtained using the gray and charcoal tarps as the bright and dark target.\n\na_plot = hv.Curve( zip(np.arange(len(elc.a_ELC)),elc.a_ELC), label=\"multiplier\").opts(\n                    xlabel=\"spectral band\",ylabel=\"value\")\nb_plot = hv.Curve( zip(np.arange(len(elc.b_ELC)),elc.b_ELC), label=\"offset\").opts(\n                    xlabel=\"spectral band\",ylabel=\"value\")\na_plot * b_plot\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nThe multiplier looks really similar to the 6SV estimate of the radiance which is not a surprise. Now to plot the results for all the calibration tarps.\n\ndf = pd.read_pickle(\"../assets/openhsi_radiance_select_targets.pkl\")\ndf.plot(x='wavelength (nm)', y=df.columns[1:],figsize=(8,6),\n          xlabel=\"wavelength (nm)\",ylabel=\"radiance (uW/cm^2/sr/nm)\",\n          ylim=(0,None),title=\"Calibration targets\").legend(loc=\"upper right\",fontsize='xx-small')",
    "crumbs": [
      "Home",
      "api",
      "atmos"
    ]
  },
  {
    "objectID": "api/atmos.html#generic-interactive-datacube-viewer",
    "href": "api/atmos.html#generic-interactive-datacube-viewer",
    "title": "atmos",
    "section": "",
    "text": "For exploring your datacubes.\n\nsource\n\n\n\n DataCubeViewer (nc_path:str=None, old_style:bool=False,\n                 img_aspect_ratio:float=0.25, box_sz:tuple=(1, 1),\n                 **kwargs)\n\n*Explore datacubes Optional key/val pair arguement overrides (**kwargs) ylim ylabel*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnc_path\nstr\nNone\npath to the NetCDF file\n\n\nold_style\nbool\nFalse\nif datacube is stored as cross-track, along-track, wavelength coords\n\n\nimg_aspect_ratio\nfloat\n0.25\naspect ratio for the datacube viewer\n\n\nbox_sz\ntuple\n(1, 1)\nAny binning (nrows, ncols) around the tap point. Default is a single pixel\n\n\nkwargs\nVAR_KEYWORD\n\n\n\n\n\n\nsource\n\n\n\n\n DataCubeViewer.setup_callbacks ()\n\nSetup the mouseover crosshair, and mouseclick callbacks.\n\n\n\n\n\n\nWarning\n\n\n\nThis interactive tool requires a Python backend. There is no interactivity in the documentation site.\n\n\n\ndcv = DataCubeViewer(nc_path=\"../../process_data/2021-05-26 03_26_26.011211.nc\",old_style=True,box_sz=(4,5))\ndcv()\n\nAllocated 762.75 MB of RAM.",
    "crumbs": [
      "Home",
      "api",
      "atmos"
    ]
  },
  {
    "objectID": "api/capture.html",
    "href": "api/capture.html",
    "title": "capture",
    "section": "",
    "text": "Tip\n\n\n\nThis module can be imported using from openhsi.capture import *\n\n\nThe OpenHSI class defines the interface between custom camera implementations and all the processing and calibration needed to run a pushbroom hyperspectral imager.\n\nsource\n\n\n\n OpenHSI (n_lines:int=16, processing_lvl:int=-1, warn_mem_use:bool=True,\n          json_path:str=None, cal_path:str=None,\n          print_settings:bool=False)\n\nBase Class for the OpenHSI Camera.\n\nsource\n\n\n\n\n OpenHSI.collect ()\n\nCollect the hyperspectral datacube.\n\nsource\n\n\n\n\n OpenHSI.avgNimgs (n:int)\n\nTake n images and find the average\n\n\n\n\nType\nDetails\n\n\n\n\nn\nint\nnumber of images to average\n\n\nReturns\nndarray\naveraged image\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nRunning in notebook slow downs the camera more than running in a script.\n\n\nTo add a custom camera, five methods need to be defined in a class to: 1. Initialise camera __init__, and 2. Open camera start_cam, and 3. Close camera stop_cam, and 4. Capture a picture as a numpy array get_img, and 5. Update the exposure settings set_exposure, and 6. [Optional] Poll the camera temperature get_temp.\nBy inheriting from the OpenHSI class, all the methods to load settings/calibration files, collect datacube, saving data to NetCDF, and viewing as RGB are integrated. Furthermore, the custom camera class can be passed to a SettingsBuilder class for calibration.\nFor example, we implement a simulated camera below.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nProcessRawDatacube only works for raw data captured using processing_lvl = -1.\n\n\n\nsource\n\n\n\n ProcessRawDatacube (fname:str, processing_lvl:int, json_path:str,\n                     cal_path:str, old_style:bool=False)\n\nPost-process datacubes\n\nsource\n\n\n\n\n ProcessRawDatacube.save (save_dir:str, preconfig_meta_path:str=None,\n                          prefix:str='', suffix:str='',\n                          old_style:bool=False)\n\nSaves to a NetCDF file (and RGB representation) to directory dir_path in folder given by date with file name given by UTC time. Override the processing buffer timestamps with the timestamps in original file, also for camera temperatures.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsave_dir\nstr\n\nPath to folder where all datacubes will be saved at\n\n\npreconfig_meta_path\nstr\nNone\nPath to a .json file that includes metadata fields to be saved inside datacube\n\n\nprefix\nstr\n\nPrepend a custom prefix to your file name\n\n\nsuffix\nstr\n\nAppend a custom suffix to your file name\n\n\nold_style\nbool\nFalse\nOrder of axis: True for (cross-track, along-track, wavelength), False for (wavelength, cross-track, along-track)\n\n\n\njson_path = '../calibration_files/OpenHSI-16_settings_Mono8_bin2.json'\ncal_path  = '../calibration_files/OpenHSI-16_calibration_Mono8_bin2_window.pkl'\n\nproc_dc = ProcessRawDatacube(fname = \"../../Downloads/16_pvn1_bin2_10ms2022_01_13-04_22_25.nc\", processing_lvl=4,\n                             json_path=json_path, cal_path=cal_path)\nproc_dc.collect()\n\nproc_dc.show(hist_eq=True)\nIf your saved datacubes have already been processed (for example, binned for smaller file size), you can further post-process your datacube using ProcessDatacube. A list of callable transforms can be provided to ProcessDatacube.load_next_tfms, the catch is to remember what transforms have already been applied during data collection and the final desired processing level (binning, radiance output, …). See the quick start guide for some documentation on what is done for each processing level.\n\n\n\n\n\n\nWarning\n\n\n\nnext_tfms needs to be valid. For instance, you cannot bin twice!\n\n\n\nsource\n\n\n\n\n ProcessDatacube (fname:str, processing_lvl:int, json_path:str,\n                  cal_path:str, old_style:bool=False)\n\nPost-process datacubes\n\nsource\n\n\n\n\n ProcessDatacube.load_next_tfms\n                                 (next_tfms:List[Callable[[numpy.ndarray],\n                                 numpy.ndarray]]=[])\n\nprovide the transforms you want to apply to this dataset\nproced_dc = ProcessDatacube(fname = \"../calibration_files/2022_01_13/2022_01_13-04_22_25_proc_lvl_2.nc\", processing_lvl=4,\n                             json_path=json_path, cal_path=cal_path)\nproced_dc.load_next_tfms([proced_dc.dn2rad])\n\nproced_dc.collect()\n\nproced_dc.show(hist_eq=True)\nDue to requiring double the amount of memory and more to facilitate saving in a separate process, make sure your datacubes can fit in your RAM. Have not tested this but I would suggest choosing n_lines &lt;= 1/3 the amount used using the regular OpenHSI.",
    "crumbs": [
      "Home",
      "api",
      "capture"
    ]
  },
  {
    "objectID": "api/capture.html#loading-and-processing-datacubes-further",
    "href": "api/capture.html#loading-and-processing-datacubes-further",
    "title": "capture",
    "section": "",
    "text": "Tip\n\n\n\nProcessRawDatacube only works for raw data captured using processing_lvl = -1.\n\n\n\nsource\n\n\n\n ProcessRawDatacube (fname:str, processing_lvl:int, json_path:str,\n                     cal_path:str, old_style:bool=False)\n\nPost-process datacubes\n\nsource\n\n\n\n\n ProcessRawDatacube.save (save_dir:str, preconfig_meta_path:str=None,\n                          prefix:str='', suffix:str='',\n                          old_style:bool=False)\n\nSaves to a NetCDF file (and RGB representation) to directory dir_path in folder given by date with file name given by UTC time. Override the processing buffer timestamps with the timestamps in original file, also for camera temperatures.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsave_dir\nstr\n\nPath to folder where all datacubes will be saved at\n\n\npreconfig_meta_path\nstr\nNone\nPath to a .json file that includes metadata fields to be saved inside datacube\n\n\nprefix\nstr\n\nPrepend a custom prefix to your file name\n\n\nsuffix\nstr\n\nAppend a custom suffix to your file name\n\n\nold_style\nbool\nFalse\nOrder of axis: True for (cross-track, along-track, wavelength), False for (wavelength, cross-track, along-track)\n\n\n\njson_path = '../calibration_files/OpenHSI-16_settings_Mono8_bin2.json'\ncal_path  = '../calibration_files/OpenHSI-16_calibration_Mono8_bin2_window.pkl'\n\nproc_dc = ProcessRawDatacube(fname = \"../../Downloads/16_pvn1_bin2_10ms2022_01_13-04_22_25.nc\", processing_lvl=4,\n                             json_path=json_path, cal_path=cal_path)\nproc_dc.collect()\n\nproc_dc.show(hist_eq=True)\nIf your saved datacubes have already been processed (for example, binned for smaller file size), you can further post-process your datacube using ProcessDatacube. A list of callable transforms can be provided to ProcessDatacube.load_next_tfms, the catch is to remember what transforms have already been applied during data collection and the final desired processing level (binning, radiance output, …). See the quick start guide for some documentation on what is done for each processing level.\n\n\n\n\n\n\nWarning\n\n\n\nnext_tfms needs to be valid. For instance, you cannot bin twice!\n\n\n\nsource\n\n\n\n\n ProcessDatacube (fname:str, processing_lvl:int, json_path:str,\n                  cal_path:str, old_style:bool=False)\n\nPost-process datacubes\n\nsource\n\n\n\n\n ProcessDatacube.load_next_tfms\n                                 (next_tfms:List[Callable[[numpy.ndarray],\n                                 numpy.ndarray]]=[])\n\nprovide the transforms you want to apply to this dataset\nproced_dc = ProcessDatacube(fname = \"../calibration_files/2022_01_13/2022_01_13-04_22_25_proc_lvl_2.nc\", processing_lvl=4,\n                             json_path=json_path, cal_path=cal_path)\nproced_dc.load_next_tfms([proced_dc.dn2rad])\n\nproced_dc.collect()\n\nproced_dc.show(hist_eq=True)\nDue to requiring double the amount of memory and more to facilitate saving in a separate process, make sure your datacubes can fit in your RAM. Have not tested this but I would suggest choosing n_lines &lt;= 1/3 the amount used using the regular OpenHSI.",
    "crumbs": [
      "Home",
      "api",
      "capture"
    ]
  },
  {
    "objectID": "api/snr.html",
    "href": "api/snr.html",
    "title": "snr",
    "section": "",
    "text": "Tip\n\n\n\nThis module can be imported using from openhsi.snr import *\n\n\nThis module contains a 6SV1.1 Atmospheric Correction class that computes the pixel radiance given some parameters and a servable interactive SNR widget. 6SV is an open souce radiative transfer code that predicts the solar spectrum received by a sensor facing the Earth. Since it is written in Fortran, we use a Python wrapper called Py6S to expose the functionality and that means we are limited to using 6SV1.1 rather than the newer 6SV2.1.\n\n\nThe F number of an optical system is a measure of overall light throughput (the ability to produce contrast at a given resolution) and is given by\n\\[ F/\\# = \\frac{f}{ \\varnothing _{\\text{EA}}} \\] where \\(f\\) is the focal length and \\(\\varnothing _{\\text{EA}}\\) is the effective aperture diameter.\nAssuming the dominant source of noise is photon shot noise \\(\\sigma_s=\\sqrt{S}\\), the SNR is given by \\[ \\text{SNR} \\approx S/\\sigma_s = \\sqrt{S}\\] where the signal \\[S= \\eta_{QE} N\\Delta t\\] with \\(\\eta_{QE,\\lambda}\\) the quantum efficiency, \\(N\\) the photons per second and \\(\\Delta t\\) the exposure time.\nThe number of photons per second is given by the formula \\[\nN_\\lambda = L_\\lambda \\rho_\\lambda \\eta_{opt} \\eta_G \\eta_{QE} A_d \\Delta\\lambda \\frac{\\lambda}{hc} \\frac{\\pi}{4(F/\\#)^2}\n\\] where \\(L_\\lambda\\) is the solar radiance at Earth’s surface given the geolocation and UTC time, \\(\\rho_\\lambda\\) is the surface reflectance, \\(\\eta_{opt}\\) is the optical efficiency, \\(\\eta_G\\) is the grating efficiency, \\(\\eta_{QE}\\) is the quantum efficiency of the sensor, \\(A_d\\) is the detector area, \\(\\Delta \\lambda\\) is the FWHM or bandwidth, and \\(\\lambda\\) is wavelength.\n\nref_model.show()\n\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:02&lt;00:00, 38.09it/s]\n\n\nCPU times: user 427 ms, sys: 358 ms, total: 785 ms\nWall time: 5.02 s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nsource\n\n\n\n Widget_SNR (ref_model:openhsi.atmos.Model6SV)\n\n*OpenHSI SNR calculator \u001b[1;32mParameters of ‘Widget_SNR’ ========================== \u001b[0m \u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m \u001b[1;36mSoft bound values are marked in cyan.\u001b[0m C/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\u001b[1;34mName Value Type Bounds Mode \u001b[0m\naperture_mm 4 Number (1, 200) V RW focal_length_mm 16 Number V RW pixel_length_x_μm 65 Number (1, 80) V RW pixel_length_y_μm 6.9 Number (1, 60) V RW integration_time_ms 10 Number (5, 100) V RW bandwidth_nm 1.5 Number (0.1, 20) V RW surface_albedo 0.3 Number (0, 1.0) V RW optical_trans_efficiency 0.9 Number (0.1, 1) V RW QE_model ‘./nbs/assets/imx252qe.csv’ Selector V RW DE_model ’./nbs/assets/600lpmm_28.7_grating.cs… Selector V RW\n\u001b[1;32mParameter docstrings: =====================\u001b[0m\n\u001b[1;34maperture_mm: aperture (mm)\u001b[0m \u001b[1;31mfocal_length_mm: focal length (mm)\u001b[0m \u001b[1;34mpixel_length_x_μm: pixel length x (μm)\u001b[0m \u001b[1;31mpixel_length_y_μm: pixel length y (μm)\u001b[0m \u001b[1;34mintegration_time_ms: integration time (ms)\u001b[0m \u001b[1;31mbandwidth_nm: FWHM bandwidth (nm)\u001b[0m \u001b[1;34msurface_albedo: constant surface albedo reflectance\u001b[0m \u001b[1;31moptical_trans_efficiency: Optical transmission efficiency\u001b[0m \u001b[1;34mQE_model: Camera QE model\u001b[0m \u001b[1;31mDE_model: Grating efficiency model\u001b[0m*\n\n\n\n\nType\nDetails\n\n\n\n\nref_model\nModel6SV\nthe 6SV model from openhsi.atmos\n\n\n\n\nwidget = Widget_SNR(ref_model=ref_model,name=\"Interactive SNR Widget\")\npn.Row(widget.param,widget.view)\n\nFutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n  self.QE.drop(\"type\", 1,inplace=True)\n&lt;ipython-input-7-93891e1ae51d&gt;:50: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n  self.DE.drop(\"type\", 1,inplace=True)",
    "crumbs": [
      "Home",
      "api",
      "snr"
    ]
  },
  {
    "objectID": "api/snr.html#theory",
    "href": "api/snr.html#theory",
    "title": "snr",
    "section": "",
    "text": "The F number of an optical system is a measure of overall light throughput (the ability to produce contrast at a given resolution) and is given by\n\\[ F/\\# = \\frac{f}{ \\varnothing _{\\text{EA}}} \\] where \\(f\\) is the focal length and \\(\\varnothing _{\\text{EA}}\\) is the effective aperture diameter.\nAssuming the dominant source of noise is photon shot noise \\(\\sigma_s=\\sqrt{S}\\), the SNR is given by \\[ \\text{SNR} \\approx S/\\sigma_s = \\sqrt{S}\\] where the signal \\[S= \\eta_{QE} N\\Delta t\\] with \\(\\eta_{QE,\\lambda}\\) the quantum efficiency, \\(N\\) the photons per second and \\(\\Delta t\\) the exposure time.\nThe number of photons per second is given by the formula \\[\nN_\\lambda = L_\\lambda \\rho_\\lambda \\eta_{opt} \\eta_G \\eta_{QE} A_d \\Delta\\lambda \\frac{\\lambda}{hc} \\frac{\\pi}{4(F/\\#)^2}\n\\] where \\(L_\\lambda\\) is the solar radiance at Earth’s surface given the geolocation and UTC time, \\(\\rho_\\lambda\\) is the surface reflectance, \\(\\eta_{opt}\\) is the optical efficiency, \\(\\eta_G\\) is the grating efficiency, \\(\\eta_{QE}\\) is the quantum efficiency of the sensor, \\(A_d\\) is the detector area, \\(\\Delta \\lambda\\) is the FWHM or bandwidth, and \\(\\lambda\\) is wavelength.\n\nref_model.show()\n\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:02&lt;00:00, 38.09it/s]\n\n\nCPU times: user 427 ms, sys: 358 ms, total: 785 ms\nWall time: 5.02 s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nsource\n\n\n\n Widget_SNR (ref_model:openhsi.atmos.Model6SV)\n\n*OpenHSI SNR calculator \u001b[1;32mParameters of ‘Widget_SNR’ ========================== \u001b[0m \u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m \u001b[1;36mSoft bound values are marked in cyan.\u001b[0m C/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\u001b[1;34mName Value Type Bounds Mode \u001b[0m\naperture_mm 4 Number (1, 200) V RW focal_length_mm 16 Number V RW pixel_length_x_μm 65 Number (1, 80) V RW pixel_length_y_μm 6.9 Number (1, 60) V RW integration_time_ms 10 Number (5, 100) V RW bandwidth_nm 1.5 Number (0.1, 20) V RW surface_albedo 0.3 Number (0, 1.0) V RW optical_trans_efficiency 0.9 Number (0.1, 1) V RW QE_model ‘./nbs/assets/imx252qe.csv’ Selector V RW DE_model ’./nbs/assets/600lpmm_28.7_grating.cs… Selector V RW\n\u001b[1;32mParameter docstrings: =====================\u001b[0m\n\u001b[1;34maperture_mm: aperture (mm)\u001b[0m \u001b[1;31mfocal_length_mm: focal length (mm)\u001b[0m \u001b[1;34mpixel_length_x_μm: pixel length x (μm)\u001b[0m \u001b[1;31mpixel_length_y_μm: pixel length y (μm)\u001b[0m \u001b[1;34mintegration_time_ms: integration time (ms)\u001b[0m \u001b[1;31mbandwidth_nm: FWHM bandwidth (nm)\u001b[0m \u001b[1;34msurface_albedo: constant surface albedo reflectance\u001b[0m \u001b[1;31moptical_trans_efficiency: Optical transmission efficiency\u001b[0m \u001b[1;34mQE_model: Camera QE model\u001b[0m \u001b[1;31mDE_model: Grating efficiency model\u001b[0m*\n\n\n\n\nType\nDetails\n\n\n\n\nref_model\nModel6SV\nthe 6SV model from openhsi.atmos\n\n\n\n\nwidget = Widget_SNR(ref_model=ref_model,name=\"Interactive SNR Widget\")\npn.Row(widget.param,widget.view)\n\nFutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n  self.QE.drop(\"type\", 1,inplace=True)\n&lt;ipython-input-7-93891e1ae51d&gt;:50: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n  self.DE.drop(\"type\", 1,inplace=True)",
    "crumbs": [
      "Home",
      "api",
      "snr"
    ]
  },
  {
    "objectID": "api/shared.html",
    "href": "api/shared.html",
    "title": "shared",
    "section": "",
    "text": "source",
    "crumbs": [
      "Home",
      "api",
      "shared"
    ]
  },
  {
    "objectID": "api/shared.html#openhsi-using-shared-multiprocessing.array-in-shareddatacube",
    "href": "api/shared.html#openhsi-using-shared-multiprocessing.array-in-shareddatacube",
    "title": "shared",
    "section": "OpenHSI using shared multiprocessing.Array in SharedDataCube",
    "text": "OpenHSI using shared multiprocessing.Array in SharedDataCube\nSharedOpenHSI has the same API as OpenHSI with the addition of a camera temperature buffer that automatically swaps over when a save is called.\n\nsource\n\nSharedOpenHSI\n\n SharedOpenHSI (n_lines:int=16, processing_lvl:int=-1, json_path:str=None,\n                cal_path:str=None, print_settings:bool=False)\n\nBase Class for the OpenHSI Camera.\n\n\nShared Cameras\nThese are exported by openhsi.cameras as SharedXXXCamera This should work just like standard OpenHSI.Camera. Here is an example using SharedSimulatedCamera.\n\nfrom openhsi.cameras import SharedSimulatedCamera\n    \nimport tempfile\nimport os\nimport time\n\nnum_saved = 0\nnum2save  = 3\nimgs=[]\n\nwith tempfile.TemporaryDirectory() as temp_dir:\n    print(temp_dir)\n    with SharedSimulatedCamera(img_path=\"../assets/great_hall_slide.png\", \n                               n_lines=535, \n                               processing_lvl = -1, \n                               json_path=\"../assets/cam_settings.json\",\n                               cal_path=\"../assets/cam_calibration.nc\",\n                               print_settings=False) as cam:\n        \n        for i in range(num2save):\n            if num_saved &gt; 0:\n                #p.join() # waiting for the last process to finish will make this slow. \n                pass\n                \n            cam.collect()\n            print(f\"collected from time: {cam.timestamps.data[0]} to {cam.timestamps.data[-1]}\")\n            \n            imgs.append(cam.show(\"bokeh\").opts(width=200))\n            \n            p = cam.save(temp_dir)\n            num_saved += 1\n            \n    \n    time.sleep(2) # let saving finish before check and cleanup\n    print(f\"finished saving {num2save} datacubes\")\n    for root, dirs, files in os.walk(temp_dir):\n        for file in files:\n            file_path = os.path.join(root, file)\n            relative_path = os.path.relpath(file_path, temp_dir)\n            print(f\"  {relative_path}\")\n            \nimgs[0]+imgs[1]+imgs[2]\n\n/var/folders/gn/gv16fym50pv5_g7s8gt7451c0000gp/T/tmpr2loh_ou\nAllocated 4676.68 MB of RAM.\n\n\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 535/535 [00:02&lt;00:00, 264.55it/s]\n\n\ncollected from time: 2025-06-19T05:29:31.987610000 to 2025-06-19T05:29:34.005276000\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nSaving /var/folders/gn/gv16fym50pv5_g7s8gt7451c0000gp/T/tmpr2loh_ou/2025_06_19/2025_06_19-05_29_31 in another process.\n\n\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 535/535 [00:01&lt;00:00, 295.23it/s]\n\n\ncollected from time: 2025-06-19T05:29:34.302093000 to 2025-06-19T05:29:36.109223000\n\n\n\n\n\n\n\n\n\n\n\nSaving /var/folders/gn/gv16fym50pv5_g7s8gt7451c0000gp/T/tmpr2loh_ou/2025_06_19/2025_06_19-05_29_34 in another process.\n\n\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 535/535 [00:02&lt;00:00, 264.70it/s]\n\n\ncollected from time: 2025-06-19T05:29:36.408037000 to 2025-06-19T05:29:38.423919000\n\n\n\n\n\n\n\n\n\n\n\nSaving /var/folders/gn/gv16fym50pv5_g7s8gt7451c0000gp/T/tmpr2loh_ou/2025_06_19/2025_06_19-05_29_36 in another process.\nfinished saving 3 datacubes\n  2025_06_19/2025_06_19-05_29_34.nc\n  2025_06_19/2025_06_19-05_29_31.nc\n  2025_06_19/2025_06_19-05_29_36.nc",
    "crumbs": [
      "Home",
      "api",
      "shared"
    ]
  },
  {
    "objectID": "api/cameras/cameras.html",
    "href": "api/cameras/cameras.html",
    "title": "Common and Webcam",
    "section": "",
    "text": "Tip\n\n\n\nThis module can be imported using from openhsi.cameras import *\n\n\nWrapper class and example code for getting images from the OpenHSI.\n\n\n\n\n\n\nTip\n\n\n\nTo use the camera, you will need some calibration files. You can also generate these files following this guide which uses the calibrate module.\n\n\nTo add a custom camera, five methods need to be defined in a class to: 1. Initialise camera __init__, and 2. Open camera start_cam, and 3. Close camera stop_cam, and 4. Capture a picture as a numpy array get_img, and 5. Update the exposure settings set_exposure, and 6. [Optional] Poll the camera temperature get_temp.\nBy inheriting from the OpenHSI class, all the methods to load settings/calibration files, collect datacube, saving data to NetCDF, and viewing as RGB are integrated. Furthermore, the custom camera class can be passed to a SettingsBuilder class for calibration.\nFor example, we implement a webcamera camera below.\n\n\n\nThis uses the OpenCV library to interface with a webcam including the one in your laptop. Mainly for testing but maybe this could be useful.\n\nsource\n\n\n\n WebCamera (mode:str=None, n_lines:int=16, processing_lvl:int=-1,\n            warn_mem_use:bool=True, json_path:str=None, cal_path:str=None,\n            print_settings:bool=False)\n\nInterface for webcam to test OpenHSI functionality\nYou can see a picture I held up to the webcam.\n\nplt.subplots(figsize=(12,4))\nplt.imshow(cam.dc.data[:,1,:],cmap=\"gray\",aspect=0.3)\n\n\n\n\n\n\n\n\n\n\n\nThis function is designed to be used using the Multiprocessing library to initiate camera recording when desired inside a script. When the switch is closed, this function then finishes releasing memory back to the system.\n\nsource\n\n\n\n\n switched_camera (cam_class:str=None, n_lines:int=128,\n                  processing_lvl:int=0, json_path:str='/media/pi/fastssd/c\n                  als/flir_settings.json', cal_path:str='/media/pi/fastssd\n                  /cals/flir_calibration.pkl', preconfig_meta:str='/media/\n                  pi/fastssd/cals/preconfig_metadata.json',\n                  ssd_dir:str='/media/pi/fastssd', toggle_interface=None)\n\nIf toggle_interface.status is True, collect with the camera until switched is False.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncam_class\nstr\nNone\nCamera Class Name from openhsi.cameras\n\n\nn_lines\nint\n128\nhow many along-track pixels\n\n\nprocessing_lvl\nint\n0\ndesired processing done in real time\n\n\njson_path\nstr\n/media/pi/fastssd/cals/flir_settings.json\npath to settings file\n\n\ncal_path\nstr\n/media/pi/fastssd/cals/flir_calibration.pkl\npath to calibration file\n\n\npreconfig_meta\nstr\n/media/pi/fastssd/cals/preconfig_metadata.json\npath to metadata file\n\n\nssd_dir\nstr\n/media/pi/fastssd\npath to SSD\n\n\ntoggle_interface\nNoneType\nNone\ntoggle_interface that controls collection",
    "crumbs": [
      "Home",
      "cameras",
      "Common and Webcam"
    ]
  },
  {
    "objectID": "api/cameras/cameras.html#common-code",
    "href": "api/cameras/cameras.html#common-code",
    "title": "Common and Webcam",
    "section": "",
    "text": "Tip\n\n\n\nThis module can be imported using from openhsi.cameras import *\n\n\nWrapper class and example code for getting images from the OpenHSI.\n\n\n\n\n\n\nTip\n\n\n\nTo use the camera, you will need some calibration files. You can also generate these files following this guide which uses the calibrate module.\n\n\nTo add a custom camera, five methods need to be defined in a class to: 1. Initialise camera __init__, and 2. Open camera start_cam, and 3. Close camera stop_cam, and 4. Capture a picture as a numpy array get_img, and 5. Update the exposure settings set_exposure, and 6. [Optional] Poll the camera temperature get_temp.\nBy inheriting from the OpenHSI class, all the methods to load settings/calibration files, collect datacube, saving data to NetCDF, and viewing as RGB are integrated. Furthermore, the custom camera class can be passed to a SettingsBuilder class for calibration.\nFor example, we implement a webcamera camera below.",
    "crumbs": [
      "Home",
      "cameras",
      "Common and Webcam"
    ]
  },
  {
    "objectID": "api/cameras/cameras.html#webcams",
    "href": "api/cameras/cameras.html#webcams",
    "title": "Common and Webcam",
    "section": "",
    "text": "This uses the OpenCV library to interface with a webcam including the one in your laptop. Mainly for testing but maybe this could be useful.\n\nsource\n\n\n\n WebCamera (mode:str=None, n_lines:int=16, processing_lvl:int=-1,\n            warn_mem_use:bool=True, json_path:str=None, cal_path:str=None,\n            print_settings:bool=False)\n\nInterface for webcam to test OpenHSI functionality\nYou can see a picture I held up to the webcam.\n\nplt.subplots(figsize=(12,4))\nplt.imshow(cam.dc.data[:,1,:],cmap=\"gray\",aspect=0.3)\n\n\n\n\n\n\n\n\n\n\n\nThis function is designed to be used using the Multiprocessing library to initiate camera recording when desired inside a script. When the switch is closed, this function then finishes releasing memory back to the system.\n\nsource\n\n\n\n\n switched_camera (cam_class:str=None, n_lines:int=128,\n                  processing_lvl:int=0, json_path:str='/media/pi/fastssd/c\n                  als/flir_settings.json', cal_path:str='/media/pi/fastssd\n                  /cals/flir_calibration.pkl', preconfig_meta:str='/media/\n                  pi/fastssd/cals/preconfig_metadata.json',\n                  ssd_dir:str='/media/pi/fastssd', toggle_interface=None)\n\nIf toggle_interface.status is True, collect with the camera until switched is False.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncam_class\nstr\nNone\nCamera Class Name from openhsi.cameras\n\n\nn_lines\nint\n128\nhow many along-track pixels\n\n\nprocessing_lvl\nint\n0\ndesired processing done in real time\n\n\njson_path\nstr\n/media/pi/fastssd/cals/flir_settings.json\npath to settings file\n\n\ncal_path\nstr\n/media/pi/fastssd/cals/flir_calibration.pkl\npath to calibration file\n\n\npreconfig_meta\nstr\n/media/pi/fastssd/cals/preconfig_metadata.json\npath to metadata file\n\n\nssd_dir\nstr\n/media/pi/fastssd\npath to SSD\n\n\ntoggle_interface\nNoneType\nNone\ntoggle_interface that controls collection",
    "crumbs": [
      "Home",
      "cameras",
      "Common and Webcam"
    ]
  },
  {
    "objectID": "api/cameras/flir.html",
    "href": "api/cameras/flir.html",
    "title": "FLIR",
    "section": "",
    "text": "Tip\n\n\n\nThis module can be imported using from openhsi.cameras import *\n\n\nWrapper class and example code for getting images from the OpenHSI.\n\n\n\n\n\n\nTip\n\n\n\nTo use the camera, you will need some calibration files. You can also generate these files following this guide which uses the calibrate module.\n\n\n\n\nThis camera requires the Spinnaker SDK and corepsonding Python pyspin .whl file from https://www.teledynevisionsolutions.com/products/spinnaker-sdk.\n\n\n\n\n\n\nNote\n\n\n\nPySpin only supports specific python versions. Be sure to pair your python version with the package you install. As of June 2025, the latest version of python supported is 3.10.\n\n\n\n\nFLIR cameras can have different property names across models and firmware versions. For example, some cameras use “AcquisitionFrameRateEnabled” while others use “AcquisitionFrameRateEnable”. The following utility functions provide a robust way to handle these differences.\n\nsource\n\n\n\n\n set_camera_attribute (camera, attribute_name, value, alternatives=None,\n                       required=True)\n\n*Set a camera attribute with support for alternative attribute names.\nFLIR cameras with different models or firmware versions may use slightly different property names for the same functionality. This function attempts to set an attribute using the primary name first, then falls back to alternative names if provided.\nArgs: camera: Camera object attribute_name: Primary attribute name to try value: Value to set alternatives: List of alternative attribute names to try if primary fails required: If True, raise an error if none of the attributes exist\nReturns: bool: True if attribute was set successfully*\n\nsource\n\n\n\n\n get_min_exposure (camera)\n\n*Get minimum exposure time in microseconds with graceful fallback.\nDifferent FLIR camera models expose the minimum exposure time through different property names. This function tries several known property names and provides a reasonable default if none are found.\nArgs: camera: Camera object\nReturns: float: Minimum exposure time in microseconds*\n\nsource\n\n\n\n\n FlirCameraBase ()\n\n*Interface for FLIR camera\nAny keyword-value pair arguments must match the those avaliable in settings file. FlirCamera expects the ones listed below:\n\nwin_resolution: size of area on detector to readout (width, height)\nwin_offset: offsets (x,y) from edge of detector for a selective\nexposure_ms: is the camera exposure time to use\npixel_format: format of pixels readout sensor, ie Mono8, Mono10, Mono10p, Mono10Packed, Mono12, Mono12p, Mono12Packed, Mono16*\n\n\nsource\n\n\n\n\n FlirCamera ()\n\n*Interface for FLIR camera\nAny keyword-value pair arguments must match the those avaliable in settings file. FlirCamera expects the ones listed below:\n\nwin_resolution: size of area on detector to readout (width, height)\nwin_offset: offsets (x,y) from edge of detector for a selective\nexposure_ms: is the camera exposure time to use\npixel_format: format of pixels readout sensor, ie Mono8, Mono10, Mono10p, Mono10Packed, Mono12, Mono12p, Mono12Packed, Mono16*\n\n\njson_path='../assets/cam_settings_flir.json'\ncal_path='../assets/cam_calibration_flir.pkl'\n\nwith FlirCamera(n_lines=256, \n                 processing_lvl = -1,\n                 cal_path=cal_path,json_path=json_path,\n                 exposure_ms=10,\n                ) as cam:\n    cam.collect()\n    fig = cam.show(hist_eq=True)\n    \nfig\n\nAllocated 472.31 MB of RAM. There was 14520.77 MB available.\n\n\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:02&lt;00:00, 90.37it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\nExport cameras using the SharedOpenHSI class.\n\nsource\n\n\n\n\n SharedFlirCamera ()\n\n*Interface for FLIR camera\nAny keyword-value pair arguments must match the those avaliable in settings file. FlirCamera expects the ones listed below:\n\nwin_resolution: size of area on detector to readout (width, height)\nwin_offset: offsets (x,y) from edge of detector for a selective\nexposure_ms: is the camera exposure time to use\npixel_format: format of pixels readout sensor, ie Mono8, Mono10, Mono10p, Mono10Packed, Mono12, Mono12p, Mono12Packed, Mono16*",
    "crumbs": [
      "Home",
      "cameras",
      "FLIR"
    ]
  },
  {
    "objectID": "api/cameras/flir.html#flir",
    "href": "api/cameras/flir.html#flir",
    "title": "FLIR",
    "section": "",
    "text": "This camera requires the Spinnaker SDK and corepsonding Python pyspin .whl file from https://www.teledynevisionsolutions.com/products/spinnaker-sdk.\n\n\n\n\n\n\nNote\n\n\n\nPySpin only supports specific python versions. Be sure to pair your python version with the package you install. As of June 2025, the latest version of python supported is 3.10.\n\n\n\n\nFLIR cameras can have different property names across models and firmware versions. For example, some cameras use “AcquisitionFrameRateEnabled” while others use “AcquisitionFrameRateEnable”. The following utility functions provide a robust way to handle these differences.\n\nsource\n\n\n\n\n set_camera_attribute (camera, attribute_name, value, alternatives=None,\n                       required=True)\n\n*Set a camera attribute with support for alternative attribute names.\nFLIR cameras with different models or firmware versions may use slightly different property names for the same functionality. This function attempts to set an attribute using the primary name first, then falls back to alternative names if provided.\nArgs: camera: Camera object attribute_name: Primary attribute name to try value: Value to set alternatives: List of alternative attribute names to try if primary fails required: If True, raise an error if none of the attributes exist\nReturns: bool: True if attribute was set successfully*\n\nsource\n\n\n\n\n get_min_exposure (camera)\n\n*Get minimum exposure time in microseconds with graceful fallback.\nDifferent FLIR camera models expose the minimum exposure time through different property names. This function tries several known property names and provides a reasonable default if none are found.\nArgs: camera: Camera object\nReturns: float: Minimum exposure time in microseconds*\n\nsource\n\n\n\n\n FlirCameraBase ()\n\n*Interface for FLIR camera\nAny keyword-value pair arguments must match the those avaliable in settings file. FlirCamera expects the ones listed below:\n\nwin_resolution: size of area on detector to readout (width, height)\nwin_offset: offsets (x,y) from edge of detector for a selective\nexposure_ms: is the camera exposure time to use\npixel_format: format of pixels readout sensor, ie Mono8, Mono10, Mono10p, Mono10Packed, Mono12, Mono12p, Mono12Packed, Mono16*\n\n\nsource\n\n\n\n\n FlirCamera ()\n\n*Interface for FLIR camera\nAny keyword-value pair arguments must match the those avaliable in settings file. FlirCamera expects the ones listed below:\n\nwin_resolution: size of area on detector to readout (width, height)\nwin_offset: offsets (x,y) from edge of detector for a selective\nexposure_ms: is the camera exposure time to use\npixel_format: format of pixels readout sensor, ie Mono8, Mono10, Mono10p, Mono10Packed, Mono12, Mono12p, Mono12Packed, Mono16*\n\n\njson_path='../assets/cam_settings_flir.json'\ncal_path='../assets/cam_calibration_flir.pkl'\n\nwith FlirCamera(n_lines=256, \n                 processing_lvl = -1,\n                 cal_path=cal_path,json_path=json_path,\n                 exposure_ms=10,\n                ) as cam:\n    cam.collect()\n    fig = cam.show(hist_eq=True)\n    \nfig\n\nAllocated 472.31 MB of RAM. There was 14520.77 MB available.\n\n\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:02&lt;00:00, 90.37it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\nExport cameras using the SharedOpenHSI class.\n\nsource\n\n\n\n\n SharedFlirCamera ()\n\n*Interface for FLIR camera\nAny keyword-value pair arguments must match the those avaliable in settings file. FlirCamera expects the ones listed below:\n\nwin_resolution: size of area on detector to readout (width, height)\nwin_offset: offsets (x,y) from edge of detector for a selective\nexposure_ms: is the camera exposure time to use\npixel_format: format of pixels readout sensor, ie Mono8, Mono10, Mono10p, Mono10Packed, Mono12, Mono12p, Mono12Packed, Mono16*",
    "crumbs": [
      "Home",
      "cameras",
      "FLIR"
    ]
  },
  {
    "objectID": "api/sensors.html",
    "href": "api/sensors.html",
    "title": "sensors",
    "section": "",
    "text": "Tip\n\n\n\nThis module can be imported using from openhsi.sensors import *\nThe OpenHSI camera requires motion to generate 2D spatial datacubes. Yet, motion also introduces other artefacts that need georectification. To correct for spurious motion, we need to collect absolute orientation and geolocation of the camera simultaneously with the camera capture. This is where this module comes in. An IMU, GPS, and Pressure/Humidity/Temperature sensor needs to be read and recorded.\nA Teensy 4.0 operates all the sensors, and devices using a Real Time Operating System (RTOS) like cooperative scheduler to run each component update at the desired frequency. By reducing the I/O, and CPU load on the main development board (the Raspberry Pi 4 with 8 GB RAM), the sensor updates are offloaded to a microcontroller with a real time clock to sync and timestamp each sensor measurement. The whole thing is assembled onto a PCB that stacks with the Raspberry Pi 4 and battery hat.\nAn XBee is also programmed to check sensor status remotely during operation. This could be useful to diagnose any issues without being physically connected to the microcontroller. A basic streaming dashboard is included.\nEach data packet contains timestamped sensor data. The item fields are then extracted from the raw binary serial stream.\nThe data packet is sent as a C struct so we need to decode the binary stream and interpret each byte as the corresponding C type. In each packet, there are status bytes to indicate which sensor has been updated.\nCreate a standrd interface for the sensor master loop to check if collection shoud stop.\nLet’s now test this using simulated ancillary sensor data packets.\nWe can simulate data packets for testing purposes. This will generate 77 data packets. You can then save the data - it will be cleaned up so each sensor has its own unique timestamp.\ntoggle_interface = GPIOInterface(start_pin=17)\n\nss = SensorStream(baudrate = 921_600,\n                  port = '/dev/serial0',\n                  toggle_interface = toggle_interface,\n                  ssd_dir = '.')\n\nss.packets = []\nfor i in tqdm(range(77)):\n    ss.packets.append(collect_sim(rtc_offset_ms=150))\n    time.sleep(0.01)\n\n#ss.save()\n\n/xavier_ssd/mambaforge/envs/openhsi_dev/lib/python3.10/site-packages/Jetson/GPIO/gpio.py:383: RuntimeWarning: This channel is already in use, continuing anyway. Use GPIO.setwarnings(False) to disable warnings\n  warnings.warn(\n/tmp/ipykernel_20274/2005748132.py:5: UserWarning: [Errno 2] could not open port /dev/serial0: [Errno 2] No such file or directory: '/dev/serial0': could not open port /dev/serial0.\n  ss = SensorStream(baudrate = 921_600,\n100%|████████████████████████████████████████████████████████████████████████| 77/77 [00:00&lt;00:00, 90.05it/s]\nWe can also use an infinite loop to continuously save sensor data when a hardware button is latched. When data is saved, a summary plot of the data is also saved alongside. Here I specifically exclude the OpenHSI camera by not providing the argument cam_name to SensorStream.__init__.\nfrom multiprocessing import Value\nfrom ctypes import c_bool\n\n\nimport Jetson.GPIO as GPIO\nGPIO.setmode(GPIO.BCM) # BCM pin-numbering scheme from Raspberry Pi\nGPIO.setup(17, GPIO.OUT)\nGPIO.output(17, True)\n\n\ntestMP = MPInterface(Value(c_bool, True))\n\nss = SensorStream(baudrate = 921_600,\n                  toggle_interface=testMP,\n                  port = '/dev/ttyTHS0',\n                  ssd_dir = '/xavier_ssd/hyperspectral_experiments')\ntime.sleep(1)\n# ss.master_loop()\nss.ser.write(b'y')\npackets=0\n\nwhile ss.ser.in_waiting &gt; 0 and packets &lt; 200:\n    packets+=1\n    # print(packets)\n    ss.packets.append( decode_packet(ss.read_packet()) )\n    time.sleep(0.1)\n        \nss.ser.write(b'n')\nss.save()\nOf course, you can also save ancillary sensor data with the OpenHSI camera datacubes - just provide cam_name and also the optional parameters in SensorStream.master_loop.\nss = SensorStream(baudrate = 921_600,\n                  port = '/dev/serial0',\n                  start_pin = 17,\n                  ssd_dir = '/media/pi/fastssd',\n                  cam_name=\"FlirCamera\")\n\nss.master_loop(n_lines=256,\n               processing_lvl=2,\n               json_path=\"/media/pi/fastssd/cals/OpenHSI-FLIR01/OpenHSI-FLIR01_settings_Mono8_bin1.json\",\n               cal_path=\"/media/pi/fastssd/cals/OpenHSI-FLIR01/OpenHSI-FLIR01_calibration_Mono8_bin1.pkl\",\n               preconfig_meta=None,\n               ssd_dir=\"/media/pi/fastssd\",\n               switch_pin=17)\nThe ancillary sensor timestamps are different from the datacube along-track timestamps so some interpolation is needed. Here is a function that does that for you.",
    "crumbs": [
      "Home",
      "api",
      "sensors"
    ]
  },
  {
    "objectID": "api/sensors.html#streaming-dashboard",
    "href": "api/sensors.html#streaming-dashboard",
    "title": "sensors",
    "section": "Streaming dashboard",
    "text": "Streaming dashboard\nView the XBee status on a dashboard that shows the last 100 points. This is a simple implementation (it is possible to improve the front end using something like Dash and plotly).\n\nsd = SensorDashboard()\nsd()\n\n\nsd.run()\n\nThe SensorDashboard will save the data coming in which can be accessed in a pd.DataFrame. Here is some experimental data with noise added to the latitude/longitude points so the ESRI map loads.\n\nsd.data_df.head(10)",
    "crumbs": [
      "Home",
      "api",
      "sensors"
    ]
  },
  {
    "objectID": "tutorials/convert_calibrations.html",
    "href": "tutorials/convert_calibrations.html",
    "title": "Convert .pkl calibration to new .nc format",
    "section": "",
    "text": "The new NetCDF (NC) format for calibration files addresses the fragile portability of PLK files. Changes in dependencies like xarray and NumPy often render PLK files usable only with the exact package versions they were created with. The new format eliminates this dependency issue by adopting a universal, self-contained structure, ensuring broader compatibility across different environments.\nThe change has been introduced in version 0.4.0 of openhsi. For now .pkl calibration are supported for backward comptability, but are converetd to .nc by default. A future version may remove this.",
    "crumbs": [
      "Home",
      "tutorials",
      "Convert .pkl calibration to new .nc format"
    ]
  },
  {
    "objectID": "tutorials/convert_calibrations.html#what-happened",
    "href": "tutorials/convert_calibrations.html#what-happened",
    "title": "Convert .pkl calibration to new .nc format",
    "section": "",
    "text": "The new NetCDF (NC) format for calibration files addresses the fragile portability of PLK files. Changes in dependencies like xarray and NumPy often render PLK files usable only with the exact package versions they were created with. The new format eliminates this dependency issue by adopting a universal, self-contained structure, ensuring broader compatibility across different environments.\nThe change has been introduced in version 0.4.0 of openhsi. For now .pkl calibration are supported for backward comptability, but are converetd to .nc by default. A future version may remove this.",
    "crumbs": [
      "Home",
      "tutorials",
      "Convert .pkl calibration to new .nc format"
    ]
  },
  {
    "objectID": "tutorials/convert_calibrations.html#how-to-convert",
    "href": "tutorials/convert_calibrations.html#how-to-convert",
    "title": "Convert .pkl calibration to new .nc format",
    "section": "How to Convert",
    "text": "How to Convert\n\nSetup A Clean Enviroment\nThe easiest way to convert an existing calibration file is to upgrade OpenHSI to version 0.4.0 in a working environment. When you first load the PKL file, it will automatically be converted to the NC format (unless you ask OpenHSI not too).\nIf starting in a fresh environment, first install OpenHSI version 0.3.2 along with its dependencies. Then, upgrade to OpenHSI version 0.4.0 without installing additional dependencies.\nAn example of doing this on Google Colab is avalaible below, but similar actions in a clean python enviroment should also work.\n\nGoogle Colab is a simple way to make the conversion\n!pip install openhsi==0.3.2\n!pip install openhsi==0.4.0 --no-deps\n\nfrom openhsi.data import CameraProperties\nwith CameraProperties(cal_path='cam_calibration.pkl') as cam:\n  pass",
    "crumbs": [
      "Home",
      "tutorials",
      "Convert .pkl calibration to new .nc format"
    ]
  },
  {
    "objectID": "tutorials/installing_windows.html",
    "href": "tutorials/installing_windows.html",
    "title": "Installing OpenHSI on Windows",
    "section": "",
    "text": "Warning\n\n\n\nThis tutorial is a work in progress.",
    "crumbs": [
      "Home",
      "tutorials",
      "Installing OpenHSI on Windows"
    ]
  },
  {
    "objectID": "tutorials/installing_windows.html#requirements",
    "href": "tutorials/installing_windows.html#requirements",
    "title": "Installing OpenHSI on Windows",
    "section": "Requirements",
    "text": "Requirements\n\nWindows 10 computer\n\n\n\n\n\n\n\nImportant\n\n\n\nUsing openhsi requires generating large datacubes which take up lots of RAM. It is recommended to use a computer with as much RAM as possible.\n\n\n\n\n\n\n\n\nNote\n\n\n\nTutorial assumes you have a working OS on your system.\n\n\n\n\n\n\n\n\nTip\n\n\n\nHyperspectral datacubes are really large so best to save them to an external SSD.",
    "crumbs": [
      "Home",
      "tutorials",
      "Installing OpenHSI on Windows"
    ]
  },
  {
    "objectID": "tutorials/installing_windows.html#python-environment",
    "href": "tutorials/installing_windows.html#python-environment",
    "title": "Installing OpenHSI on Windows",
    "section": "1. Python Environment",
    "text": "1. Python Environment\nWindows does not have Python by default, so you will need to setup an enviroment. We recommend Miniforge as good way to get a working install (that avoid having to build dependaces like Py6S). An alternative is pyenv, if you prfer vanilla cpython.\n\n\n\n\n\n\nNote\n\n\n\nhttps://pythonspeed.com/articles/conda-vs-pip/ - some notes on conda (miniforge) vs pip (pyenv).\n\n\n\nInstaling with Miniforge or Miniconda\nMiniforge is an easy way to get a full working Python install working on windows.\nTo install download and run the following: https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Windows-x86_64.exe\nSee https://github.com/conda-forge/miniforge for more information on miniforge.\nOnce installed you shoudl have a link in your start menu or on your desktop for a “Miniforge Prompt”. Use tis to get a command line with python.\n\n\nInstall Jupyter Lab\nAll the interactive tools in openhsi require Jupyter Notebooks because it uses web tools. To install, follow the steps in https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html\nIn short, it boils down to\nconda install -c conda-forge jupyterlab\nTo run jupyterlab, use the below command when in the directory you want to work in. This will start jupyterlab and open a browser window to the workspace.\njupyter lab\nYou will also need some Jupyter Extensions. Navigate to the Jupyter Extensions tab and search for jupyterlab-manager, and jupyter_bokeh.",
    "crumbs": [
      "Home",
      "tutorials",
      "Installing OpenHSI on Windows"
    ]
  },
  {
    "objectID": "tutorials/installing_windows.html#install-openhsi",
    "href": "tutorials/installing_windows.html#install-openhsi",
    "title": "Installing OpenHSI on Windows",
    "section": "2. Install OpenHSI",
    "text": "2. Install OpenHSI\n\nvia conda\nCreate a new environment and install OpenHSI (including python dependancies and 6SV, except for cameras). You can change python version as required as well.\nconda create -n openhsi python=3.8 openhsi\nconda activate openhsi\nor using environment.yml in the main repo\nconda env create -f environment.yml\nSee CONTIRBUTE file in repo for examples of how to setup development mode if you want to mke chnages to OpenHSI code.",
    "crumbs": [
      "Home",
      "tutorials",
      "Installing OpenHSI on Windows"
    ]
  },
  {
    "objectID": "tutorials/installing_windows.html#install-camera-sdk",
    "href": "tutorials/installing_windows.html#install-camera-sdk",
    "title": "Installing OpenHSI on Windows",
    "section": "3. Install Camera SDK",
    "text": "3. Install Camera SDK\n\nInstall Lucid Vision Arena SDK (Sydney Photonics/Robonation OpenHSI)\nThe Robonation OpenHSI uses the detector from Lucid Vision Labs Inc. Their full SDK is required to use the sensor with the OpenHSI libary. This can be acquired from https://thinklucid.com/downloads-hub/.\nRun the installer and follow the prompts. We recommend picking delveoper install, so you get AreaView progam, which may be helpful in troublshooting.\n\nArena Additional Setup\nAt the end of the innstaller click the option to show the Arena Getting Started Guide, which has some instructons for final setup. (the path to these will be somethink like this: C:/Program Files/Lucid Vision Labs/Arena SDK/docs/html/arena_sdk_windows.html)\nIn the instructions you will be shown how setup the LUCID Lightweight Filter Driver for Windows and enable jumbo frames.\n\n\nInstall Arean Python Library\nAnd then install the arena python library (this is the same on a conda based or pip based python env):\npip install --no-deps arena_api-&lt;version&gt;-py3-none-any.whl\n\n\n\nInstall Spinnaker SDK (FLIR based OpenHSI)\nDownload and install the Spinnaker SDK and python wheel. - https://www.flir.com.au/products/spinnaker-sdk/\n\n\n\n\n\n\nTip\n\n\n\nOn an Intel/Amd, you want amd64/x86_64 version on raspi you want aarch64 or arm64.\n\n\nFolow the Spinaker SDK install instructions and then install the python package:\npip install --no-deps spinnaker_python-&lt;your needed version&gt;.whl\n\n\nInstall Ximea SDK (Ximea based OpenHSI)\nNavigate to https://www.ximea.com/support/wiki/apis/Python and click on the Windows installation instructions. This will bring you to the page https://www.ximea.com/support/wiki/apis/Python_inst_win which lists everything you need to install the Ximea API.\nWe recommend using conda instead of pip when following these instructions for installing packages (when using miniforge).",
    "crumbs": [
      "Home",
      "tutorials",
      "Installing OpenHSI on Windows"
    ]
  },
  {
    "objectID": "tutorials/installing_windows.html#ready-to-go",
    "href": "tutorials/installing_windows.html#ready-to-go",
    "title": "Installing OpenHSI on Windows",
    "section": "Ready to go!",
    "text": "Ready to go!\nCongratulations, you now have a system that can run your openhsi!",
    "crumbs": [
      "Home",
      "tutorials",
      "Installing OpenHSI on Windows"
    ]
  },
  {
    "objectID": "tutorials/calibrate_steps.html",
    "href": "tutorials/calibrate_steps.html",
    "title": "Generating Calibration Files",
    "section": "",
    "text": "Warning\n\n\n\nThis is not for general use. Requires technical expertise.\nTools required: - An integrating sphere (we use a Spectra PT from LabSphere) - A HgAr lamp\nAdjust the camera class as needed. This example uses the LucidCamera. The are some LucidCamera specifc items in the below code that would need to be removed.\n#all_hardware\nimport os\n\nimport holoviews as hv\nimport numpy as np\n\nfrom openhsi.calibrate import SettingsBuilderMixin, SpectraPTController, sum_gaussians\nfrom openhsi.cameras import LucidCamera\n\nhv.extension(\"bokeh\", logo=False)\nimport panel as pn\n\n\nclass CalibrateCamera(SettingsBuilderMixin, LucidCamera):\n    pass\n\njson_path_template = \"../cals/cam_settings_lucid_template.json\"\ncal_path = \"\"\n\nmodelno = 18\n\nprint(\"\".format(modelno))\n\njson_path_target = \"../cals/OpenHSI-{0:02d}/OpenHSI-{0:02d}_settings_Mono8_bin1.json\".format(\n    modelno\n)\ncal_path_target = \"../cals/OpenHSI-{0:02d}/OpenHSI-{0:02d}_calibration_Mono8_bin1.pkl\".format(\n    modelno\n)\n\nif not os.path.isdir(os.path.dirname(json_path_target)):\n    os.mkdir(os.path.dirname(json_path_target))\n\nspt = SpectraPTController()",
    "crumbs": [
      "Home",
      "tutorials",
      "Generating Calibration Files"
    ]
  },
  {
    "objectID": "tutorials/calibrate_steps.html#find-illuminated-sensor-area",
    "href": "tutorials/calibrate_steps.html#find-illuminated-sensor-area",
    "title": "Generating Calibration Files",
    "section": "Find illuminated sensor area",
    "text": "Find illuminated sensor area\nThe vertical directon/y axis of the detector array corrspeonds the across-track direction of the sensor. If the image of slit is shorter then the heigh we can crop the top and bottom to save bandwidth/disk space (similar to letterboxing video).\nThere are two ways to do this, croping after the fact using row_minmax or by setting up a window on the sensor. Setting up a window will reduce the ammount of data transfered from the sensor and can improve maximum framerate depending on the sensor so is recomended.\n\n1. Take a flat field\nFirst step is to provide a uniform illumination to the slit, ideally spectrally broadband, like a halogen lamp or the sun.\n\n# Select luminance value 10000 on the SpectraPT\nspt.selectPreset(10000)\n\n# Initialize the CalibrateCamera class with specified parameters\nwith CalibrateCamera(\n    json_path=json_path_template,  # Path to a JSON file containing camera settings template\n    cal_path=\"\",                  # Path to a pickle file (not used here since it's an empty string)\n    processing_lvl=-1,            # Processing level (specific to CalibrateCamera, meaning unknown)\n    exposure_ms=20                # Exposure time in milliseconds\n) as cam:\n    \n    # Retake the flat field image and display it using Holoviews (hvim_flat)\n    hvim_flat = cam.retake_flat_field(show=True)\n    hvim_flat.opts(width=600, height=600, axiswise=True)\n\n    # Update the row min/max values and display them (hvim_row_minmax)\n    hvim_row_minmax = cam.update_row_minmax(edgezone=0)\n    hvim_row_minmax.opts(width=600, height=600, axiswise=True)\n\n    # Calculate the window height based on the row slice and ensure it's a multiple of 4 (required for LucidCameras)\n    windowheight = int(\n        np.ceil((cam.settings[\"row_slice\"][1] - cam.settings[\"row_slice\"][0]) / 4.0) * 4\n    )\n    print(\"Windowheight {}\".format(windowheight))\n    \n    # Update camera settings based on the calculated window height:\n    # - Set the window resolution (height, width)\n    cam.settings[\"win_resolution\"] = [windowheight + 16, cam.settings[\"resolution\"][1]]\n    \n    # - Set the window offset (adjusting for potential padding)\n    cam.settings[\"win_offset\"] = [\n        int(np.ceil((cam.settings[\"row_slice\"][0]) / 4.0) * 4) - 8,\n        cam.settings[\"win_offset\"][1],\n    ]\n    \n    # - Update the row slice (region of interest)\n    cam.settings[\"row_slice\"] = [16, windowheight - 8]\n\n    # Set the overall camera resolution to match the window resolution\n    cam.settings[\"resolution\"] = cam.settings[\"win_resolution\"]\n\n    # Save the updated camera settings to JSON and pickle files\n    cam.dump(json_path=json_path_target, cal_path=cal_path_target)\n    \n# Display the row min/max and flat field images side-by-side using Panel\npn.Column(hvim_row_minmax, hvim_flat)\n\n\n# Initialize the CalibrateCamera class with specified parameters. and setting from previous cell.\nwith CalibrateCamera(\n    n_lines=50,\n    processing_lvl=0,\n    cal_path=cal_path_target,\n    json_path=json_path_target,\n    exposure_ms=10,\n) as cam:\n    # cam.collect()\n    cam.start_cam()\n    img = cam.get_img()\n    img = cam.crop(img)\n    cam.stop_cam()\n    # cam.show(hist_eq=True)\n\n# check the window looks ok.\nhv.Image(img, bounds=(0, 0, *img.shape)).opts(\n    xlabel=\"wavelength index\",\n    ylabel=\"cross-track\",\n    cmap=\"gray\",\n    title=\"test frame\",\n    width=400,\n    height=400,\n)\n\n\n\n2. Take Arc and setup wavelength scale, and get window for 430 to 900nm\n\nwith CalibrateCamera(\n    json_path=json_path_target,  # Path to the JSON file with camera settings\n    cal_path=\"\",                  # Path to a pickle file (not used here)\n    processing_lvl=-1            # Processing level (specific to CalibrateCamera)\n) as cam:\n    \n    # Set the camera gain value to 10.0 - LUCIDCAMERA ONLY\n    cam.deviceSettings[\"Gain\"].value = 10.0  \n\n    # Capture a HgAr (Mercury-Argon) spectrum image and display it\n    hvimg = cam.retake_HgAr(show=True, nframes=18)  # Capture 18 frames and average them\n    hvimg.opts(width=600, height=600)  # Set display options for the image\n\n    # Print the maximum pixel value in the captured HgAr image\n    print(cam.calibration[\"HgAr_pic\"].max()) \n\n    # Calculate and update the \"smile\" shifts (geometric distortion correction)\n    smile_fit_hv = cam.update_smile_shifts()  \n\n    # Reset the smile shifts to zero (likely for testing or specific calibration purposes)\n    cam.calibration[\"smile_shifts\"] = cam.calibration[\"smile_shifts\"] * 0 \n\n    # Perform wavelength calibration using the HgAr spectrum\n    wavefit_hv = cam.fit_HgAr_lines(\n        top_k=15,                  # Use the top 15 brightest peaks for fitting\n        brightest_peaks=[546.96, 435.833, (579.960 + 579.066) / 2, 763.511],  # Known HgAr peak wavelengths\n        find_peaks_height=10,     # Parameters for peak detection \n        prominence=1,\n        width=1.5,\n        interactive_peak_id=True,  # Allow interactive selection of peaks\n    ) \n\n    # Define the desired wavelength range for the \"window\"\n    waveminmax = [430, 900]  # Wavelength range in nanometers\n\n    # Find the corresponding indices in the wavelength array\n    waveminmax_ind = [\n        np.argmin(np.abs(cam.calibration[\"wavelengths_linear\"] - λ)) for λ in waveminmax\n    ]\n\n    # Calculate the window width and offset based on the wavelength indices\n    window_width = int(np.ceil((waveminmax_ind[1] - waveminmax_ind[0] + 8) / 4.0) * 4)\n    offset_x = int(np.floor((waveminmax_ind[0] - 4) / 4.0) * 4)\n    print(\"Window Width {}, offset x {}\".format(window_width, offset_x))\n\n    # Update camera settings with the new window parameters\n    cam.settings[\"win_resolution\"][1] = window_width  # Set window width\n    cam.settings[\"win_offset\"][1] = offset_x        # Set horizontal offset\n    cam.settings[\"resolution\"] = cam.settings[\"win_resolution\"]  # Update overall resolution\n\n    # Display the HgAr image, smile fit data, and wavelength calibration results using Panel\n    pn.Column(\n        hvimg,\n        smile_fit_hv,\n        wavefit_hv.opts(xlim=(390, 1000), ylim=(-10, 255)).opts(shared_axes=False),\n    )\n\n\n# check the window looks ok.\npn.Column(\n    hvimg.opts(shared_axes=False),\n    smile_fit_hv.opts(shared_axes=False),\n    wavefit_hv.opts(xlim=(400, 900), ylim=(-10, 255)).opts(shared_axes=False),\n)\n\n\n# save wavefit if things look ok\ncam.dump(json_path=json_path_target, cal_path=cal_path_target)\n\n\n\n3. Retake flat field and arc with windows\n\nspt.selectPreset(10000)\n\n# retake flat frame with wavelegth window set.\nwith CalibrateCamera(\n    json_path=json_path_target, cal_path=cal_path_target, processing_lvl=-1\n) as cam:\n    hvim_flat = cam.retake_flat_field(show=True)\n    hvim_flat.opts(width=600, height=600, axiswise=True)\n\n    hvim_row_minmax = cam.update_row_minmax(edgezone=8)\n    hvim_row_minmax.opts(width=600, height=600, axiswise=True)\n\n    cam.update_resolution()\n    cam.dump(json_path=json_path_target, cal_path=cal_path_target)\n\nspt.turnOffLamp()\n\n# display and check all looks ok.\nhvim_row_minmax + hvim_flat\n\n\n\nRedo Arc with window.\n\n# retake arc frames and set wavelength scale foir window.\n\nwith CalibrateCamera(\n    json_path=json_path_target, cal_path=cal_path_target, processing_lvl=-1\n) as cam:\n    cam.deviceSettings[\"Gain\"].value = 15.0\n    hvimg = cam.retake_HgAr(show=True)\n\n    hvimg.opts(width=400, height=400)\n    print(cam.calibration[\"HgAr_pic\"].max())\n    smile_fit_hv = cam.update_smile_shifts()\n\n    wavefit_hv = cam.fit_HgAr_lines(\n        top_k=12,\n        brightest_peaks=[546.96, 435.833, (579.960 + 579.066) / 2, 871.66, 763.511],\n        find_peaks_height=10,\n        prominence=1,\n        width=1.5,\n        max_match_error=2,\n        interactive_peak_id=True,\n    )  # [435.833,546.074,(579.960+579.066)/2,763.511]\n    \n    cam.update_intsphere_fit()\n\n    cam.dump(json_path=json_path_target, cal_path=cal_path_target)\n\n(hvimg + smile_fit_hv + wavefit_hv.opts(xlim=(400, 900), ylim=(-10, 255))).opts(\n    shared_axes=False\n)\n\n\n\n3. Get Integrating Sphere data for radiance calibration\n4D datacube with coordinates of cross-track, wavelength, exposure, and luminance.\n\nluminances = np.fromiter(lum_preset_dict.keys(), dtype=int)\n# luminances = np.append(luminances,0)\nexposures = [0, 5, 8, 10, 15, 20]\n\nwith CalibrateCamera(\n    json_path=json_path_target, cal_path=cal_path_target, processing_lvl=-1\n) as cam:\n\n    cam.calibration[\"rad_ref\"] = cam.update_intsphere_cube(\n        exposures, luminances, noframe=50, lum_chg_func=spt.selectPreset\n    )\n\n    # remove saturated images\n    cam.calibration[\"rad_ref\"] = cam.calibration[\"rad_ref\"].where(\n        ~(\n            np.sum((cam.calibration[\"rad_ref\"][:, :, :, :, :] == 255), axis=(1, 2))\n            &gt; 1000\n        )\n    )\n    cam.dump(json_path=json_path_target, cal_path=cal_path_target)\n\nspt.turnOffLamp()\n\n\ncam.calibration[\"rad_ref\"].plot(\n    y=\"cross_track\", x=\"wavelength_index\", col=\"exposure\", row=\"luminance\", cmap=\"gray\"\n)\n\n\nprint(\"rad_ref is {} MB\".format(cam.calibration[\"rad_ref\"].size / 1024 / 1024 * 4))\n\n\ncam.update_intsphere_fit()\ncam.dump(json_path=json_path_target, cal_path=cal_path_target)",
    "crumbs": [
      "Home",
      "tutorials",
      "Generating Calibration Files"
    ]
  },
  {
    "objectID": "quick_start.html",
    "href": "quick_start.html",
    "title": "Quick Start Guide",
    "section": "",
    "text": "To use an OpenHSI camera, you will need a settings .json file that describes how the camera is initialised and other details you can edit to suit your use case. You will also need a .pkl file that includes some arrays produced during calibration that allow the OpenHSI camera to do smile corrections, and conversions to radiance and reflectance.\nFor example, this is how you would use an OpenHSI camera (packaged with a Lucid sensor) and collect a hyperspectral datacube. The context manager automatically handles the initialisation and closing of the camera.\nSince we have a pushbroom sensor, we capture a line of spatial information at a time. Motion is required to obtain 2D spatial information and how many lines we collect is specified by n_lines. After LucidCamera.collect is run, the data is stored in a 3D numpy array LucidCamera.dc.data which is implemented as a circular buffer. The next section explains the processing_lvl parameter.",
    "crumbs": [
      "Home",
      "Quick Start Guide"
    ]
  },
  {
    "objectID": "quick_start.html#processing-levels",
    "href": "quick_start.html#processing-levels",
    "title": "Quick Start Guide",
    "section": "Processing levels",
    "text": "Processing levels\nThe library comes with some predefined recipes you can use to output a datacube with the desired level of processing. Depending on your use case, you may want to use save raw data, or choose a faster binning scheme. The available options are listed below.\n\n\n\n\n\n\n\nprocessing_lvl\nDescription\n\n\n\n\n-1\ndo not apply any transforms (default)\n\n\n0\nraw digital numbers cropped to useable sensor area\n\n\n1\ncrop + fast smile\n\n\n2\ncrop + fast smile + fast binning\n\n\n3\ncrop + fast smile + slow binning\n\n\n4\ncrop + fast smile + fast binning + conversion to radiance in units of uW/cm^2/sr/nm\n\n\n5\ncrop + fast smile + radiance + fast binning\n\n\n6\ncrop + fast smile + fast binning + radiance + reflectance\n\n\n7\ncrop + fast smile + radiance + slow binning\n\n\n8\ncrop + fast smile + radiance + slow binning + reflectance\n\n\n\nMain difference between these is the order the transforms are used in the pipeline. This summaries the binning procedure and output:\n\n\n\nprocessing_lvl\nBinning\nOutput\n\n\n\n\n-1,0,1\nNone\nDigital Numbers\n\n\n2\nFast\nDigital Numbers\n\n\n3\nSlow\nDigital Numbers\n\n\n4,5\nFast\nRadiance (uW/cm^2/sr/nm)\n\n\n6\nFast\nReflectance\n\n\n7\nSlow\nRadiance (uW/cm^2/sr/nm)\n\n\n8\nSlow\nReflectance\n\n\n\nAlternatively, you can supply a custom pipeline of transforms custom_tfms:List[Callable[[np.ndarray],np.ndarray]] to LucidCamera.set_processing_lvl(custom_tfms).\n\nA note on binning schemes\nWe provide a fast binning scheme that only involves one memory allocation to speed things up and it assumes that the wavelength profile along the spectral axis is linear. In practice, it is not exacly linear so we also provide a slow binning scheme that does it properly at the cost of requiring more memory allocations. We found that the extra time needed was around 2 ms on a Jetson Xavier board.\n\n\nPost-processing Datacubes\nIf you are collecting raw data and want to post-process them into radiance or reflectance, you can use ProcessDatacube from the capture module. For example, we have a datacube of digital numbers and we want to convert them to radiance. We need to pass a processing_lvl that includes the radiance conversion (so the the dn2rad method is initialised). Then we can pass in a list of transforms to load_next_tfms which will be applied to the whole datacube.\nfrom openhsi.capture import ProcessDatacube\n\ndc2process = ProcessDatacube(fname = \"path_to_datacube_file.nc\", processing_lvl=4,\n                             json_path=json_path, cal_path=cal_path)\ndc2process.load_next_tfms([proced_dc.dn2rad])\ndc2process.collect()\nJust like the SimulatedCamera, you can then view your post-processed datacube by using\ndc2process.show(hist_eq=True)\nor similar. More on visualisation in the next section.",
    "crumbs": [
      "Home",
      "Quick Start Guide"
    ]
  },
  {
    "objectID": "quick_start.html#visualisation",
    "href": "quick_start.html#visualisation",
    "title": "Quick Start Guide",
    "section": "Visualisation",
    "text": "Visualisation\nAfter collection, the datacube can be visualised as an RGB image using LucidCamera.show which returns a figure object created using your chosen plotting backend plot_lib. The red, green, and blue wavelengths can be specified and the RGB channels will be chosen from the nearest wavelenth bands.\nYou may find that the contrast is low because of some outlier pixels from, for instance, specular reflection. To increase the contrast, we provide two options:\n\nrobust: saturated linear stretch. For example, robust=True will rescale colours to the 2–98% percentile. Alternatively, you can specify the percentage too like so robust=5 will rescale to 5-95%.\nhist_eq: apply histogram equalisation\n\n\n\n\n\n\n\nNote\n\n\n\nDefault behaviour is no contrast adjustments.\n\n\nIf you just want to view a datacube without any cameras attached. You can do so using:\nfrom openhsi.data import *\ndc = DataCube()\ndc.load_nc(\"path_to_datacube_file.nc\")\ndc.show(robust=True)\nIf you want to interactively view your datacubes (tap and see spectra), you can do so using:\nfrom openhsi.atmos import *\ndcv = DataCubeViewer(\"path_to_datacube_file.nc\")\ndcv()",
    "crumbs": [
      "Home",
      "Quick Start Guide"
    ]
  },
  {
    "objectID": "quick_start.html#saving-datacubes",
    "href": "quick_start.html#saving-datacubes",
    "title": "Quick Start Guide",
    "section": "Saving datacubes",
    "text": "Saving datacubes\nTo save the datacube to NetCDF format (alongside an RGB picture), use LucidCamera.save. For example:\ncam.save( save_dir = \"beach_data\" )\nwill save a NetCDF file as f”beach_data/{current_date}/{current_datetime}.nc” and also an RGB image alongside. The save function also allows you to customise the file prefix and suffix. Preconfigured metadata can also be indicated to be saved into the NetCDF file. The camera temperature (in Celcius) and datatime for each camera frame is automatically included.",
    "crumbs": [
      "Home",
      "Quick Start Guide"
    ]
  },
  {
    "objectID": "quick_start.html#getting-surface-reflectance",
    "href": "quick_start.html#getting-surface-reflectance",
    "title": "Quick Start Guide",
    "section": "Getting surface reflectance",
    "text": "Getting surface reflectance\nGenerally, processing to radiance is recommended. To process to reflectance in real-time, one requires knowledge of the atmospheric conditions at the time of collect. While this is facilitated by setting the processing_lvl to 6, internally, the algorithm relies on the pre-computed at sensor radiance saved in the calibration .pkl file (in the Python dictionary, rad_fit is the key). The other option is to use Empirical Line Calibration (see the atmos module in the sidebar).\n\nUpdating the radiative transfer model\nA radiative transfer model predicts the behaviour of sunlight as it enters the Earth’s atmosphere. Some of the light will be absorbed, re-emitted, scattered, etc, from oxygen, nitrogen, carbon dioxide, methane, and aerosols to name a few. You don’t want any clouds obstructing the sunlight.\nSince the atmospheric conditions will change, you may need to recompute this every so often. Here is how to do it assuming your camera object is called cam from the example above:\n\n\n\n\n\n\nTip\n\n\n\nIf you don’t have a physical camera initialised, you can use the base CameraProperties class from openhsi.data to load, modify, and dump these files.\ncam = CameraProperties(json_path=\"path_to_settings_file.json\",cal_path=\"path_to_calibration_file.pkl\")\n\n\n\n# camera initialised...\n\nfrom datetime import datetime\nfrom openhsi.atmos import *\nfrom Py6S import *\nfrom scipy.interpolate import interp1d\n\nmodel = Model6SV(lat = cam.settings[\"latitude\"], lon = cam.settings[\"longitude\"],\n                 z_time = datetime.strptime(cam.settings[\"datetime_str\"],\"%Y-%m-%d %H:%M\"),\n                 station_num = cam.settings[\"radiosonde_station_num\"], region = cam.settings[\"radiosonde_region\"],\n                 alt = cam.settings[\"altitude\"], zen = 0., azi = 0., # viewing zenith and azimuth angles\n                 aero_profile = AeroProfile.Maritime,\n                 wavelength_array = np.linspace(350,900,num=2000), # choose larger range than sensor range\n                 sixs_path = cam.settings[\"sixs_path\"])\n\ncam.calibration[\"rad_fit\"] = interp1d(np.linspace(350,900,num=2000), model.radiance/10, kind='cubic')\n\n#cam.dump(json_path,cal_path) # update the settings and calibration files\n\n\n\n\n\n\nImportant\n\n\n\nYou will need the 6SV excutable somewhere on your system. You can specify the path to the executable with sixs_path. If you installed via conda you should be fine without specifying the path.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe 6SV model calculates radiance in units of (W/m^2/sr/μm), whereas the integrating sphere calibration is in (μW/cm^2/sr/nm) hence the extra divide by 10 in model.radiance/10. Use a wavelength_array that extends beyond the sensor range on both sides.\n\n\n\n\nEmpirical Line Calibration\nThe ELC widget is defined in the openhsi.atmos module (link to documentation). This method basically uses known spectral targets (typically one dark and one light) to extrapolate the reflectance for the other pixels. Users can draw several bounding boxes telling the ELC algorithm to use those pixels as the reference targets. Part of automatically identifying the spectral target, and thus an interactive widget, is to use a spectral matching technique. I use Spectral Angle Mapper and implemented it efficiently enough to be used interactively.\n\n\n\n\n\n\nImportant\n\n\n\nOnly ingests radiance datacubes. To view a digital number or reflectance datacube interactively, use DataCubeViewer in the openhsi.atmos module.\n\n\nfrom openhsi.atmos import *\nelc = ELC(nc_path=\"path_to_radiance_datacube.nc\",\n          speclib_path=\"path_to_spectral_library.pkl\",cal_path=\"path_to_camera_calibration_file.pkl\")\nelc()\nThe speclib_path parameter identifies the lab measured spectra of a few calibration tarps. This method also requires a radiance estimate model_6SV so we can spectrally match radiance from lab based reflectance - close enough is good enough.",
    "crumbs": [
      "Home",
      "Quick Start Guide"
    ]
  },
  {
    "objectID": "quick_start.html#usage-tips",
    "href": "quick_start.html#usage-tips",
    "title": "Quick Start Guide",
    "section": "Usage Tips",
    "text": "Usage Tips\nHere are some tips from those who have used this library and camera in the field.\n\nRunning the camera collect software from Jupyter Notebooks will impose some delays and slow down the frame rate. For best performance, run the camera collect from a script.\nThe interactive manner of Jupyter Notebooks mean memory usage can grow with successive datacube allocations. Restart the kernel if your memory is getting full helps.",
    "crumbs": [
      "Home",
      "Quick Start Guide"
    ]
  },
  {
    "objectID": "architecture_overview.html",
    "href": "architecture_overview.html",
    "title": "OpenHSI Architecture Overview",
    "section": "",
    "text": "OpenHSI uses a sophisticated object-oriented architecture built around several key design patterns:\n\nInheritance Hierarchies: Clear class hierarchies with well-defined responsibilities\nMixin Pattern: Composable functionality through mixins\nFactory Pattern: Dynamic class creation for different configurations\nTemplate Method: Configurable processing pipelines\nAdapter Pattern: Unified interface across different hardware",
    "crumbs": [
      "Home",
      "OpenHSI Architecture Overview"
    ]
  },
  {
    "objectID": "architecture_overview.html#core-architecture-patterns",
    "href": "architecture_overview.html#core-architecture-patterns",
    "title": "OpenHSI Architecture Overview",
    "section": "",
    "text": "OpenHSI uses a sophisticated object-oriented architecture built around several key design patterns:\n\nInheritance Hierarchies: Clear class hierarchies with well-defined responsibilities\nMixin Pattern: Composable functionality through mixins\nFactory Pattern: Dynamic class creation for different configurations\nTemplate Method: Configurable processing pipelines\nAdapter Pattern: Unified interface across different hardware",
    "crumbs": [
      "Home",
      "OpenHSI Architecture Overview"
    ]
  },
  {
    "objectID": "architecture_overview.html#inheritance-hierarchies",
    "href": "architecture_overview.html#inheritance-hierarchies",
    "title": "OpenHSI Architecture Overview",
    "section": "Inheritance Hierarchies",
    "text": "Inheritance Hierarchies\nThe core inheritance structure follows a logical progression from basic camera properties to full hyperspectral imaging capabilities:\nCameraProperties (data.py)\n    ├── DataCube (data.py)\n    │   └── OpenHSI (capture.py)\n    │       └── [Camera Implementations]\n    └── SharedDataCube (shared.py)\n        └── SharedOpenHSI (shared.py)\n            └── [Shared Camera Implementations]\n\nKey Base Classes\n\nCameraProperties (data.py)\n\nPurpose: Core settings management, calibration handling, processing pipeline\nKey Features:\n\nCamera settings (exposure, gain, etc.)\nCalibration parameters (flat field, dark current, spectral)\nProcessing pipeline configuration\nTransform methods (cropping, binning, radiometric conversion)\n\n\n\n\nDataCube (data.py)\n\nPurpose: Hyperspectral data collection and storage\nInherits: CameraProperties\nAdds:\n\nData buffering with CircArrayBuffer\nSave/load functionality\nVisualization methods\nXArray integration\n\n\n\n\nOpenHSI (capture.py)\n\nPurpose: Main camera interface for data collection\nInherits: DataCube\nAdds:\n\nCamera control and acquisition\nReal-time processing\nLive preview capabilities\n\n\n\n# Example: Basic inheritance structure\nfrom openhsi.data import CameraProperties, DataCube\nfrom openhsi.capture import OpenHSI\n\n# Show the method resolution order (MRO)\nprint(\"CameraProperties MRO:\", CameraProperties.__mro__)\nprint(\"DataCube MRO:\", DataCube.__mro__)\nprint(\"OpenHSI MRO:\", OpenHSI.__mro__)\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nCameraProperties MRO: (&lt;class 'openhsi.data.CameraProperties'&gt;, &lt;class 'object'&gt;)\nDataCube MRO: (&lt;class 'openhsi.data.DataCube'&gt;, &lt;class 'openhsi.data.CameraProperties'&gt;, &lt;class 'object'&gt;)\nOpenHSI MRO: (&lt;class 'openhsi.capture.OpenHSI'&gt;, &lt;class 'openhsi.data.DataCube'&gt;, &lt;class 'openhsi.data.CameraProperties'&gt;, &lt;class 'object'&gt;)\n\n\n\n# Example: How mixins are composed dynamically\ndef create_settings_builder(clsname: str, cam_class: type) -&gt; type:\n    \"\"\"Factory function to create a camera class with calibration capabilities\"\"\"\n    return type(clsname, (cam_class, SettingsBuilderMixin), {})\n\n# This creates classes like:\n# FlirSettings = create_settings_builder('FlirSettings', FlirCamera)\n# XimeaSettings = create_settings_builder('XimeaSettings', XimeaCamera)",
    "crumbs": [
      "Home",
      "OpenHSI Architecture Overview"
    ]
  },
  {
    "objectID": "architecture_overview.html#camera-architecture",
    "href": "architecture_overview.html#camera-architecture",
    "title": "OpenHSI Architecture Overview",
    "section": "Camera Architecture",
    "text": "Camera Architecture\nThe camera system uses a dual-inheritance pattern that combines hardware-specific implementations with processing capabilities:\n\nHardware Base Classes\n\nFlirCameraBase - FLIR camera hardware interface\nLucidCameraBase - Lucid Vision camera interface\n\nXimeaCameraBase - Ximea camera interface\nWebCamera - Testing/simulation camera\n\n\n\nFinal Camera Classes\nMultiple inheritance combines hardware interfaces with processing capabilities:\nclass FlirCamera(FlirCameraBase, OpenHSI): pass\nclass SharedFlirCamera(FlirCameraBase, SharedOpenHSI): pass\n\nclass LucidCamera(LucidCameraBase, OpenHSI): pass\nclass SharedLucidCamera(LucidCameraBase, SharedOpenHSI): pass\nThis pattern allows: - Same hardware interface to work with standard or shared memory architectures - Easy addition of new camera types - Consistent API across different hardware\n\n# Example: Camera architecture in action\nfrom openhsi.cameras import FlirCamera, SharedFlirCamera\n\n# Both classes inherit from the same hardware base but different processing bases\nprint(\"FlirCamera MRO:\", FlirCamera.__mro__)\nprint(\"SharedFlirCamera MRO:\", SharedFlirCamera.__mro__)\n\n# They provide the same interface but different memory management\n# cam = FlirCamera()  # Standard memory\n# shared_cam = SharedFlirCamera()  # Shared memory for multiprocessing\n\nFlirCamera MRO: (&lt;class 'openhsi.cameras.FlirCamera'&gt;, &lt;class 'openhsi.cameras.FlirCameraBase'&gt;, &lt;class 'openhsi.capture.OpenHSI'&gt;, &lt;class 'openhsi.data.DataCube'&gt;, &lt;class 'openhsi.data.CameraProperties'&gt;, &lt;class 'object'&gt;)\nSharedFlirCamera MRO: (&lt;class 'openhsi.cameras.SharedFlirCamera'&gt;, &lt;class 'openhsi.cameras.FlirCameraBase'&gt;, &lt;class 'openhsi.shared.SharedOpenHSI'&gt;, &lt;class 'openhsi.shared.SharedDataCube'&gt;, &lt;class 'openhsi.data.CameraProperties'&gt;, &lt;class 'object'&gt;)",
    "crumbs": [
      "Home",
      "OpenHSI Architecture Overview"
    ]
  },
  {
    "objectID": "architecture_overview.html#processing-pipeline",
    "href": "architecture_overview.html#processing-pipeline",
    "title": "OpenHSI Architecture Overview",
    "section": "Processing Pipeline",
    "text": "Processing Pipeline\nOpenHSI implements a Template Method pattern for the image processing pipeline:\n\nTransform Pipeline\n\nProcessing steps are defined as methods that can be chained together\nPipeline is configurable through tfm_list parameter\nProcessing levels (0-8) provide preset combinations\nCustom transform lists allow complete flexibility\n\n\n\nStandard Processing Chain\nRaw Image → crop → fast_smile → fast_bin → dn2rad → rad2ref_6SV → Processed Image\n\n\nProcessing Levels\n\nLevel 0: Raw data (no processing)\nLevel 1: Basic corrections (crop, smile)\nLevel 2: Radiometric conversion (dn2rad)\nLevel 3: Reflectance conversion (rad2ref)\nHigher levels: Advanced atmospheric corrections\n\n\n# Example: Processing pipeline configuration\nfrom openhsi.data import CameraProperties\n\n# Show available processing methods\nprocessing_methods = [method for method in dir(CameraProperties) \n                     if not method.startswith('_') and \n                     method in ['crop', 'fast_smile', 'fast_bin', 'dn2rad', 'rad2ref_6SV']]\nprint(\"Available processing methods:\", processing_methods)\n\n# Example custom pipeline\ncustom_pipeline = ['crop', 'fast_bin', 'dn2rad']\nprint(\"Custom pipeline:\", custom_pipeline)\n\nAvailable processing methods: ['crop', 'dn2rad', 'fast_bin', 'fast_smile', 'rad2ref_6SV']\nCustom pipeline: ['crop', 'fast_bin', 'dn2rad']",
    "crumbs": [
      "Home",
      "OpenHSI Architecture Overview"
    ]
  },
  {
    "objectID": "architecture_overview.html#module-organization",
    "href": "architecture_overview.html#module-organization",
    "title": "OpenHSI Architecture Overview",
    "section": "Module Organization",
    "text": "Module Organization\nThe codebase is organized into logical modules with clear responsibilities:\n\nCore Modules\n\ndata.py - Data Layer\n\nClasses: CameraProperties, DataCube, CircArrayBuffer\nPurpose: Settings management, data storage, buffering\nKey Features: Processing pipeline, calibration handling, circular buffers\n\n\n\ncapture.py - Capture Layer\n\nClasses: OpenHSI, SimulatedCamera, processing classes\nPurpose: Camera control and data acquisition\nKey Features: Live capture, preview, simulation\n\n\n\ncameras.py - Hardware Abstraction\n\nClasses: Camera-specific implementations\nPurpose: Hardware interface abstraction\nKey Features: Unified API across different camera brands\n\n\n\nshared.py - Parallel Processing\n\nClasses: SharedDataCube, SharedOpenHSI\nPurpose: Multiprocessing-safe implementations\nKey Features: Shared memory, background processes\n\n\n\ncalibrate.py - Calibration System\n\nClasses: SettingsBuilderMixin\nPurpose: Calibration workflows\nKey Features: Spectral calibration, flat field, smile correction\n\n\n\n\nSupporting Modules\n\nsensors.py: Ancillary sensor integration\ngeometry.py: Geometric corrections\natmos.py: Atmospheric corrections\nsnr.py: Signal-to-noise analysis\nmetadata.py: Metadata handling\n\n\n# Example: Module structure overview\nimport openhsi\n\n# Show main modules\nmodules = [attr for attr in dir(openhsi) if not attr.startswith('_')]\nprint(\"Available modules:\", modules)\n\n# Show classes in data module\nimport openhsi.data as data\ndata_classes = [attr for attr in dir(data) if attr[0].isupper()]\nprint(\"Classes in data module:\", data_classes)\n\nAvailable modules: ['cameras', 'capture', 'data', 'shared']\nClasses in data module: ['Array', 'Callable', 'CameraProperties', 'CircArrayBuffer', 'ContextManagers', 'DType', 'DataCube', 'DateTimeBuffer', 'EventTimer', 'Generic', 'Image', 'IterLen', 'Iterable', 'List', 'Optional', 'PartialFormatter', 'Path', 'ReindexCollection', 'Shape', 'Tuple', 'TypeVar', 'UNSET', 'Union', 'Unset']",
    "crumbs": [
      "Home",
      "OpenHSI Architecture Overview"
    ]
  },
  {
    "objectID": "architecture_overview.html#code-examples",
    "href": "architecture_overview.html#code-examples",
    "title": "OpenHSI Architecture Overview",
    "section": "Code Examples",
    "text": "Code Examples\n\nBasic Usage Example\n\n# Example: Basic camera setup and usage\nfrom openhsi.capture import SimulatedCamera\n\n# Create a simulated camera for demonstration\ncam = SimulatedCamera(img_path=\"assets/great_hall_slide.png\", \n                      n_lines=1024, \n                      processing_lvl = 2, \n                     json_path=\"assets/cam_settings.json\",\n                      cal_path=\"assets/cam_calibration.nc\"\n                     )\n\n# Show the inheritance chain\nprint(\"SimulatedCamera inherits from:\")\nfor i, cls in enumerate(cam.__class__.__mro__):\n    print(f\"  {i}: {cls.__name__} ({cls.__module__})\")\n\n# Show available methods from different layers\nmethods_by_layer = {\n    'CameraProperties': ['crop', 'fast_smile', 'dn2rad'],\n    'DataCube': ['put', 'save', 'show'],\n    'OpenHSI': ['capture', 'preview', 'connect']\n}\n\nfor layer, methods in methods_by_layer.items():\n    available = [m for m in methods if hasattr(cam, m)]\n    print(f\"{layer} methods available: {available}\")\n\nAllocated 480.78 MB of RAM. There was 28095.53 MB available.\nSimulatedCamera inherits from:\n  0: SimulatedCamera (openhsi.capture)\n  1: OpenHSI (openhsi.capture)\n  2: DataCube (openhsi.data)\n  3: CameraProperties (openhsi.data)\n  4: object (builtins)\nCameraProperties methods available: ['crop', 'fast_smile', 'dn2rad']\nDataCube methods available: ['put', 'save', 'show']\nOpenHSI methods available: []\n\n\n\n\nProcessing Pipeline Example\n\n# Example: Custom processing pipeline\nimport numpy as np\n\n# Create sample data\nsample_data = np.random.randint(0, 4096, (100, 200, 150), dtype=np.uint16)\n\n# Set up camera with custom processing\ncam = SimulatedCamera(img_path=\"assets/great_hall_slide.png\", \n                      n_lines=1024, \n                      processing_lvl = 4, \n                     json_path=\"assets/cam_settings.json\",\n                      cal_path=\"assets/cam_calibration.nc\"\n                     )\nprint(f\"Processing level: {cam.proc_lvl}\")\nprint(f\"Transform list: {cam.tfm_list}\")\n\n# Show how transforms would be applied\nif hasattr(cam, 'tfm_list'):\n    print(\"Transform pipeline:\")\n    for i, transform in enumerate(cam.tfm_list or []):\n        print(f\"  {i+1}. {transform}\")\n\nAllocated 480.78 MB of RAM. There was 27964.42 MB available.\nProcessing level: 4\nTransform list: [&lt;bound method CameraProperties.crop of DataCube: shape = (905, 136), Processing level = 4\n&gt;, &lt;bound method CameraProperties.fast_smile of DataCube: shape = (905, 136), Processing level = 4\n&gt;, &lt;bound method CameraProperties.fast_bin of DataCube: shape = (905, 136), Processing level = 4\n&gt;, &lt;bound method CameraProperties.dn2rad of DataCube: shape = (905, 136), Processing level = 4\n&gt;]\nTransform pipeline:\n  1. &lt;bound method CameraProperties.crop of DataCube: shape = (905, 136), Processing level = 4\n&gt;\n  2. &lt;bound method CameraProperties.fast_smile of DataCube: shape = (905, 136), Processing level = 4\n&gt;\n  3. &lt;bound method CameraProperties.fast_bin of DataCube: shape = (905, 136), Processing level = 4\n&gt;\n  4. &lt;bound method CameraProperties.dn2rad of DataCube: shape = (905, 136), Processing level = 4\n&gt;\n\n\n\n\nMixin Composition Example\n\n# Example: How mixins add functionality\nfrom openhsi.calibrate import SettingsBuilderMixin\n\n# Show mixin methods\nmixin_methods = [method for method in dir(SettingsBuilderMixin) \n                if not method.startswith('_') and callable(getattr(SettingsBuilderMixin, method))]\nprint(\"SettingsBuilderMixin methods:\", mixin_methods)\n\n# Example of dynamic composition (simplified)\ndef demonstrate_mixin_composition():\n    \"\"\"Show how mixins are composed with base classes\"\"\"\n    \n    # This is conceptually how camera classes with calibration are created:\n    # CalibrationCamera = type('CalibrationCamera', \n    #                         (SimulatedCamera, SettingsBuilderMixin), {})\n    \n    print(\"Mixin composition adds calibration methods to any camera class\")\n    print(\"Result: Camera + Calibration = Full featured calibration camera\")\n\ndemonstrate_mixin_composition()\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nSettingsBuilderMixin methods: ['fit_HgAr_lines', 'fit_emission_lines', 'retake_HgAr', 'retake_emission_lines', 'retake_flat_field', 'update_intsphere_cube', 'update_intsphere_fit', 'update_resolution', 'update_row_minmax', 'update_smile_shifts', 'update_window_across_track', 'update_window_along_track']\nMixin composition adds calibration methods to any camera class\nResult: Camera + Calibration = Full featured calibration camera",
    "crumbs": [
      "Home",
      "OpenHSI Architecture Overview"
    ]
  },
  {
    "objectID": "tutorials/custom_pipeline.html",
    "href": "tutorials/custom_pipeline.html",
    "title": "Customising the OpenHSI Processing Pipeline",
    "section": "",
    "text": "Warning\n\n\n\nThis tutorial is a work in progress.\nYou can customise the level of processing you want in real-time. The API for doing this in post-processing is exactly the same and is based on CameraProperties.load_next_tfms which every class dealing with datacubes will inherit.\nThere are prebuilt recipies for setting the real-time processing pipeline (and for post-processing raw datacubes). You may want to customise this, for example, to implement direct georeferencing.",
    "crumbs": [
      "Home",
      "tutorials",
      "Customising the OpenHSI Processing Pipeline"
    ]
  },
  {
    "objectID": "tutorials/custom_pipeline.html#the-pipeline",
    "href": "tutorials/custom_pipeline.html#the-pipeline",
    "title": "Customising the OpenHSI Processing Pipeline",
    "section": "The Pipeline",
    "text": "The Pipeline\nEach callable within the pipeline is expected to ingest an array and output an array. The final array from the pipeline is then stored in a DataCube buffer as the along-axis index is incremented.\nfrom openhsi.capture import ProcessDatacube\n\ndc2process = ProcessDatacube(fname = \"path_to_datacube_file.nc\", processing_lvl=4,\n                             json_path=json_path, cal_path=cal_path)\ndc2process.load_next_tfms([proced_dc.dn2rad])\ndc2process.collect()",
    "crumbs": [
      "Home",
      "tutorials",
      "Customising the OpenHSI Processing Pipeline"
    ]
  },
  {
    "objectID": "tutorials/installing_linux.html",
    "href": "tutorials/installing_linux.html",
    "title": "Installing OpenHSI on Linux",
    "section": "",
    "text": "Warning\n\n\n\nThis tutorial is a work in progress.",
    "crumbs": [
      "Home",
      "tutorials",
      "Installing OpenHSI on Linux"
    ]
  },
  {
    "objectID": "tutorials/installing_linux.html#requirements",
    "href": "tutorials/installing_linux.html#requirements",
    "title": "Installing OpenHSI on Linux",
    "section": "Requirements",
    "text": "Requirements\n\nRaspberry Pi Running Raspberry Pi OS (Raspbian) or Ubuntu 18.04 (using full 64 bit versions).\n\nA Raspberry Pi 4 with 8 GB RAM is recommended.\n\nJetson development boards (E.g. Xavier) will also work. Some additional packages may be needed.\nDebian based system\n\nrecommended using Ubuntu 18.04 and 20.04\n\n\n\n\n\n\n\n\nImportant\n\n\n\nUsing openhsi requires generating large datacubes which take up lots of RAM. It is recommended to get a Raspberry Pi 4 with as much RAM as possible, especially if you want to capture raw data. Otherwise, you will need to a processing_lvl that includes binning to reduce the datacube size.\n\n\nA Raspberry Pi does not have the same processing power as a typical laptop/computer and so expect the framerate to drop slightly. You want a lightweight operating system so the camera can run as fast as possible. If you have 8 GB of RAM, definitely get the Raspbian 64 bit OS so you can use &gt; 3 GB per process. Another benefit of running on ARM64 bit is that there are prebuilt OpenCV Python wheels - if your project requires opencv-python, then 64 bit is needed.\n\n\n\n\n\n\nNote\n\n\n\nThe latest 64 bit Rasberry Pi OS (formerly Raspbian) is based on Debian 11 Bullseye (equivalent to Ubuntu 21+) and so some packages may not be updated to this platform yet. Also systemd is used to run scripts on startup.\n\n\n\n\n\n\n\n\nNote\n\n\n\nTutorial assumes you have a working OS on your system.\n\n\n\n\n\n\n\n\nTip\n\n\n\nHyperspectral datacubes are really large so best to save them to an external SSD.",
    "crumbs": [
      "Home",
      "tutorials",
      "Installing OpenHSI on Linux"
    ]
  },
  {
    "objectID": "tutorials/installing_linux.html#python-environment",
    "href": "tutorials/installing_linux.html#python-environment",
    "title": "Installing OpenHSI on Linux",
    "section": "1. Python Environment",
    "text": "1. Python Environment\n\nInstalling Pyenv\nThe system python (on raspi in particular) is not always the most user friendly if any packages are not available pre-compiled, so using an alternative environment is recommended. A good way to get a independent python environment is using pyenv. Of course any modern python install (py&gt;3.7) should work.\nTo install pyenv you can use the following commands:\nsudo apt-get update; sudo apt-get install make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev\n\ncurl https://pyenv.run | bash\n\necho 'export PATH=\"$HOME/.pyenv/bin:$PATH\"' &gt;&gt; ~/.bashrc\necho 'eval \"$(pyenv init -)\"' &gt;&gt; ~/.bashrc\necho 'eval \"$(pyenv virtualenv-init -)\"' &gt;&gt; ~/.bashrc\necho \"export PYENV_VIRTUALENV_DISABLE_PROMPT=1\" &gt;&gt;~/.bashrc\n\nsource ~/.bashrc\nWith pyenv ready, you can easily install essentially any version of Python, from a vanilla cpython too miniforge too pypy. For OpenHSI, Miniforge is a good option as the base environment for minimal effort. You can also get it without pyenv if you prefer at https://github.com/conda-forge/miniforge.\n\n\n\n\n\n\nNote\n\n\n\nhttps://pythonspeed.com/articles/conda-vs-pip/ - some notes on conda vs pip.\n\n\n\n\nInstall NodeJS\nThe Bokeh plotting library depends on nodeJS and is used to run OpenHSI interactive tools in a web browser.\nsudo apt install nodejs\n\n\nMiniforge with pyenv\npyenv install miniforge3\nAnd then use conda later.\n\nMiniforge - Manual\nMiniforge3 is great lightweight option to get a new python environment up and running that will not interfere with the system one. You can download this from here https://github.com/conda-forge/miniforge#download and then install follow instructions.\nwget \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh\n\n\n\ncpython with pyenv\npyenv install 3.8.12\nAnd the use pip later.\n\n\nInstall Jupyter Lab\nAll the interactive tools in openhsi require Jupyter Notebooks because it uses web tools. To install, follow the steps in https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html\nIn short, it boils down to conda install -c conda-forge jupyterlab or pip install jupyterlab.\nYou will also need some Jupyter Extensions. Navigate to the Jupyter Extensions tab and search for jupyterlab-manager, and jupyter_bokeh.",
    "crumbs": [
      "Home",
      "tutorials",
      "Installing OpenHSI on Linux"
    ]
  },
  {
    "objectID": "tutorials/installing_linux.html#install-openhsi",
    "href": "tutorials/installing_linux.html#install-openhsi",
    "title": "Installing OpenHSI on Linux",
    "section": "2. Install OpenHSI",
    "text": "2. Install OpenHSI\n\nvia conda\nCreate a new environment and install OpenHSI (including python dependancies and 6SV, except for cameras). You can change python version as required as well.\npyenv activate miniforge3\nconda create -n openhsi python=3.8 openhsi\nconda activate openhsi\nor using environment.yml in the main repo\nconda env create -f environment.yml\n\n\nvia pip\n\n\n\n\n\n\nTip\n\n\n\nIf using raspi system python, some dependencies need to be installed using sudo apt install python-opencv etc.\n\n\nTo create a new environment and install OpenHSI.\npyenv virtualenv 3.8.12 openhsi\npyenv activate openhsi\npip install openhsi\n\nInstall 6SV (only needed for pip install)\ngit clone https://github.com/robintw/6S.git\ncd 6S\ncmake -D CMAKE_INSTALL_PREFIX=/usr/local .",
    "crumbs": [
      "Home",
      "tutorials",
      "Installing OpenHSI on Linux"
    ]
  },
  {
    "objectID": "tutorials/installing_linux.html#install-camera-sdk",
    "href": "tutorials/installing_linux.html#install-camera-sdk",
    "title": "Installing OpenHSI on Linux",
    "section": "3. Install Camera SDK",
    "text": "3. Install Camera SDK\n\nInstall Lucid Vision Arena SDK (Sydney Photonics/Robonation OpenHSI)\nThe Robonation OpenHSI uses the detector from Lucid Vision Labs Inc. Their full SDK is required to use the sensor with the OpenHSI libary. This can be acquired from https://thinklucid.com/downloads-hub/.\nTo start the install, unzip the tar.gz package, i.e:\ntar -xf  ArenaSDK_&lt;version&gt;_Linux_&lt;platform&gt;.tar.gz\nAnd run this configiuration script/installer:\ncd ArenaSDK_Linux_&lt;platform&gt;\nsudo sh Arena_SDK_&lt;platform&gt;.conf\n\n\n\n\n\n\nImportant\n\n\n\nYou must keep the SDK files in the same location that you ran this command, or re-run the configiuration script. No files are copied or moved, just locations recorded for the arena python api to find.\n\n\nAnd then install the arena python library (this is the same on a conda based or pip based python env):\npip install --no-deps arena_api-&lt;version&gt;-py3-none-any.whl\n\nEnable Jumbo Packets\nTo ensure optimal performance you need to make sure your GigE link is setup for jumbo packets.\nOn Ubuntu system this can be done using (you may want to set this up to occur on startup):\n    sudo ip link set eth0 mtu 9000\n\n\n\n\n\n\nNote\n\n\n\neth0 should be the adapter name the camera is connected to, it may be different on your system.\n\n\n\n\nJumbo Packets on a Pi4\nUsing Raspberry Pi OS, some additional effort is required. https://support.thinklucid.com/knowledgebase/jumbo-frames-on-raspberry-pi/\n\n\n\nInstall Spinnaker SDK (FLIR based OpenHSI)\nDownload and install the Spinnaker SDK and python wheel. - https://www.flir.com.au/products/spinnaker-sdk/\n\n\n\n\n\n\nTip\n\n\n\nOn an Intel/Amd, you want amd64/x86_64 version on raspi you want aarch64 or arm64.\n\n\nThe 20.04 version has the follow dependancies:\nsudo apt install libusb-1.0-0 libavcodec58 libavformat58 libswscale5 libswresample3 libavutil56 qt5-default\nand then installing the FLIR SDK.\nsudo sh install_spinnaker.sh\npip install --no-deps spinnaker_python-&lt;your needed version&gt;.whl\nOne of the prompts will ask you if you want to increase the USB buffer. Answer yes to this option.\nMore information on getting the FLIR SDK on a Raspberry Pi and Jetson is here https://www.flir.com.au/support-center/iis/machine-vision/application-note/using-spinnaker-on-arm-and-embedded-systems/\n\nOn Raspbian 64 bit\nYou will need to install Spinnaker however you’ll find that spinview-qt will not install because it is missing a dependency which we actually don’t require in openhsi. If you want to use a GUI called SpinView to view Camera output, then installing spinview-qt will let you use it but I haven’t yet figured out how on Raspbian 64 bit. It’s something to do with the QT5 package no longer being included in Debian 11+ package sources.\nsudo apt install qtbase5-dev qtchooser qt5-qmake qtbase5-dev-tools\nTo proceed without spinview-qt, follow the hacks found in https://askubuntu.com/questions/1335184/qt5-default-not-in-ubuntu-21-04. You extract the offending package, change a line stating the dependency, then repackaged the files.\nMore information on getting the FLIR SDK on a Raspberry Pi and Jetson is here https://www.flir.com.au/support-center/iis/machine-vision/application-note/using-spinnaker-on-arm-and-embedded-systems/\n\n\n\nInstall Ximea SDK (Ximea based OpenHSI)\nNavigate to https://www.ximea.com/support/wiki/apis/Python and click on the Linux installation instructions. This will bring you to the page https://www.ximea.com/support/wiki/apis/XIMEA_Linux_Software_Package#Installation which lists everything you need to install the Ximea API.\nA 64 bit OS is required to run the GUI called XIMEA CamTool\nwget https://www.ximea.com/downloads/recent/XIMEA_Linux_SP.tgz\ntar xzf XIMEA_Linux_SP.tgz\ncd package\n./install\n#./install -pcie # -pcie flag needed for PCIE cameras\nIf the build fails, try run the following first:\nsudo apt-get update && sudo apt-get install build-essential linux-headers-\"$(uname -r)\" \n\nIncrease USB memory limit\nThis is from the XIMEA page https://www.ximea.com/support/wiki/apis/Raspberry_Pi_4_Benchmarks#Increase-USB-memory-limit\nThe default memory limit for buffers allocated for USB devices (usually 16MB) is too low for XIMEA USB cameras.This can be changed permanently following these steps:\n\nOpen the file /boot/firmware/cmdline.txt in your preferred texteditor\nAppend the following (in the same line): usbcore.usbfs_memory_mb=0\nReboot the system\nCheck if the changes are applied. The following command should print 0:\n\ncat /sys/module/usbcore/parameters/usbfs_memory_mb",
    "crumbs": [
      "Home",
      "tutorials",
      "Installing OpenHSI on Linux"
    ]
  },
  {
    "objectID": "tutorials/installing_linux.html#additional-setup-for-sensor-package-on-raspberry-pi-optional",
    "href": "tutorials/installing_linux.html#additional-setup-for-sensor-package-on-raspberry-pi-optional",
    "title": "Installing OpenHSI on Linux",
    "section": "4. Additional Setup for sensor package on Raspberry Pi (optional)",
    "text": "4. Additional Setup for sensor package on Raspberry Pi (optional)\n\n4.1 Enable I2C and UART\nOpen the Raspberry Pi config tool.\nsudo raspi-config\nAnd enable I2C and UART. For more instructions see https://askubuntu.com/questions/1273700/enable-spi-and-i2c-on-ubuntu-20-04-raspberry-pi. After enabling them, add the user pi to the group.\nsudo usermod -a -G dialout pi\nsudo usermod -a -G i2c pi\nNow, to use UART at high speed, we need to change a few more settings documented here https://www.abelectronics.co.uk/kb/article/1035/serial-port-setup-in-raspberry-pi-os\n\n\n4.2 Disable serial login shell\nremove console=serial0,115200 from /boot/cmdline.txt then reboot. This will allow us to run a higher baud rate.\n\n\n4.3 Use PI0UART instead of miniUART\nAdd to end of file /boot/config.txt we need to add a configuration to /boot/config.txt.\nsudo echo 'dtoverlay=disable-bt' &gt;&gt; /boot/config.txt\nThen in the terminal\nsudo systemctl disable hciuart\nsudo reboot\n\n\n\n\n\n\nNote\n\n\n\nThe trade off with allowing higher baud rates is no bluetooth.\n\n\n\n\n4.4 Controlling GPIO pins\nIf you are running this on a Raspberry Pi 4, you’ll need to install the GPIO library. pip install RPi.GPIO\nIf you are running this on a Jetson, you’ll need the corresponding Jetson version. pip install Jetson.GPIO\n\n\n4.5 Installing the Battery Hat software\nTo deploy the Raspberry Pi on a remote platform, it needs power. We use the X728 version 2.1 battery hat from Suptronics which lets us turn on and safe shutdown the Pi while indicating power left.\nSince we are using Debian 11, we need to use systemd to enable /etc/rc-local which the battery hat scripts use. Follow the steps in https://www.linuxbabe.com/linux-server/how-to-enable-etcrc-local-with-systemd\nThe software for the battery hat can be found here: https://wiki.geekworm.com/X728-Software Be aware that you need to make some changes to get them to install Change python-smbus to python3-smbus.\nsudo apt-get install python3-smbus\npip install smbus\nAdd PY_VERSION=3 on line 86 in x728-v2.1.sh to force the installer to use Python 3. Python 2 is deprecated so convert x728pld.py to Python 3 syntax. All the print statements need ( and ).\n\n\n4.6 - Mounting SSD without sudo\nThe reason why we want to mount the SSD without sudo is that we can then write to it without sudo.\nFirst, identify your SSD using sudo fdisk -l I followed the steps in https://blog.onetwentyseven001.com/mounting-without-sudo/index.html",
    "crumbs": [
      "Home",
      "tutorials",
      "Installing OpenHSI on Linux"
    ]
  },
  {
    "objectID": "tutorials/installing_linux.html#ready-to-go",
    "href": "tutorials/installing_linux.html#ready-to-go",
    "title": "Installing OpenHSI on Linux",
    "section": "Ready to go!",
    "text": "Ready to go!\nCongratulations, you now have a system that can run your openhsi!",
    "crumbs": [
      "Home",
      "tutorials",
      "Installing OpenHSI on Linux"
    ]
  },
  {
    "objectID": "api/calibrate.html",
    "href": "api/calibrate.html",
    "title": "calibrate",
    "section": "",
    "text": "Tip\n\n\n\nThis module can be imported using from openhsi.calibrate import *\n\n\n\nsource\n\n\n\n sum_gaussians (x:&lt;built-infunctionarray&gt;, *args)\n\nCompute the summed Gaussians given the array indicies x and mushed arguments of amplitude, peak position, peak width, constant\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nx\narray\nindicies\n\n\nargs\nVAR_POSITIONAL\n“amplitude, peak position, peak width, constant”\n\n\nReturns\narray\nsummed Gaussian curves array\n\n\n\n\nsource\n\n\n\n\n SettingsBuilderMixin ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\n\n\n create_settings_builder (clsname:str, cam_class:type)\n\nCreate a SettingsBuilder class called clsname based on your chosen cam_class.\n\n\n\n\nType\nDetails\n\n\n\n\nclsname\nstr\n\n\n\ncam_class\ntype\nCamera Class\n\n\nReturns\ntype\n“SettingsBuilder Class”\n\n\n\n\nsource\n\n\n\n\n SettingsBuilderMetaclass (clsname:str, cam_class, attrs)\n\n*type(object) -&gt; the object’s type type(name, bases, dict, **kwds) -&gt; a new type*\n\n\n\nThere are a few ways to create a SettingsBuilder class that words for your custom camera. (They involve Python metaclasses and mixins)\nFor example, you can then create a SettingsBuilder class that works for your custom camera by doing one of the following.\n\n\n\n\n\n\nNote\n\n\n\nBelow we use the ‘SimulatedCamera’ class. When calibrating a real camera you would replace SimulatedCamera with your camera class.\n\n\n\n# using helper function\nSettingsBuilder = create_settings_builder(\"SettingsBuilder\",SimulatedCamera)\n\n# using Metaclasses\nSettingsBuilder = SettingsBuilderMetaclass(\"SettingsBuilder\",SimulatedCamera,{})\n\n# initialising\nsb = SettingsBuilder(json_path=\"../assets/cam_settings.json\", \n                     cal_path=\"../assets/cam_calibration.nc\")\n\nAllocated 17.48 MB of RAM. There was 1463.06 MB available.\n\n\n\n# pure mixin\nclass CalibrateOpenHSI(SettingsBuilderMixin, SimulatedCamera):\n    pass\n\nsb = CalibrateOpenHSI(mode=\"flat\",json_path=\"../assets/cam_settings.json\", cal_path=\"../assets/cam_calibration.nc\")\n\nAllocated 17.48 MB of RAM. There was 1450.69 MB available.\n\n\n\n\n\nWe assume the x axis (or detector columns) are used for the spectral channels, the rows correspond to the cross-track dimension and are limited by the optics (slit). The useable area is cropped out (windowing can also be used to reduce data intake).\n\nsource\n\n\n\n SettingsBuilderMixin.retake_flat_field (show:bool=True)\n\nTake and store an image of with the OpenHSI slit illuminated but a uniform light source.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nshow\nbool\nTrue\nflag to show taken image\n\n\nReturns\nImage\n\n\n\n\n\n\nsource\n\n\n\n\n SettingsBuilderMixin.update_row_minmax (edgezone:int=4, show:bool=True)\n\nFind edges of slit in flat field images and determine region to crop.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nedgezone\nint\n4\nnumber of pixel buffer to add to crop region\n\n\nshow\nbool\nTrue\nflag to show plot of slice and edges identified\n\n\nReturns\nCurve\n\n\n\n\n\n\nhvimg=sb.retake_flat_field(show=True)\nhvimg.opts(width=400,height=400)\nprint(sb.calibration[\"flat_field_pic\"].max())\nhvimg\n\n255\n\n\n\n\n\n\n  \n\n\n\n\n\nsb.update_row_minmax()\n\nLocs row_min: 7 and row_max: 912\n\n\n\n\n\n\n  \n\n\n\n\n\nsb.update_resolution()\n\n\n\n\n\nThe emissions lines, which should be straight vertical, appear slightly curved. This is smile error (error in the spectral dimension).\n\nsb.mode_change(\"HgAr\")\n\n\nsource\n\n\n\n SettingsBuilderMixin.retake_emission_lines (show:bool=True,\n                                             nframes:int=10)\n\nTake and store an image with camera illuminated with a calibration source.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nshow\nbool\nTrue\nflag to show\n\n\nnframes\nint\n10\nnumber of frames to average for image\n\n\nReturns\nImage\n\n\n\n\n\n\nsource\n\n\n\n\n SettingsBuilderMixin.retake_HgAr (show:bool=True, nframes:int=10)\n\nTake and store an image with OpenHSI camera illuminated with a HgAr calibration source.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nshow\nbool\nTrue\nflag to show\n\n\nnframes\nint\n10\nnumber of frames to average for image\n\n\nReturns\nImage\n\n\n\n\n\n\nsource\n\n\n\n\n SettingsBuilderMixin.update_smile_shifts (show=True)\n\nDetermine Smile and shifts to correct from spectral lines image.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nshow\nbool\nTrue\nflag to show plot of smile shifts for each cross track pixel.\n\n\nReturns\nCurve\n\n\n\n\n\n\nhvimg=sb.retake_HgAr(show=True, nframes=1)\nhvimg.opts(width=400,height=400)\nprint(sb.calibration[\"HgAr_pic\"].max())\nhvimg\n\n255.0\n\n\n\n\n\n\n  \n\n\n\n\n\nsb.update_smile_shifts()\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nTo do this, peaks in the HgAr spectrum are found, refined by curve-fitting with Gaussians. The location of the peaks then allow for interpolation to get the map from array (column) index to wavelength (nm).\n\nsource\n\n\n\n SettingsBuilderMixin.fit_emission_lines (brightest_peaks:list,\n                                          emission_lines:list,\n                                          top_k:int=10,\n                                          filter_window:int=1,\n                                          interactive_peak_id:bool=False,\n                                          find_peaks_height:int=10,\n                                          prominence:float=0.2,\n                                          width:float=1.5,\n                                          distance:int=10,\n                                          max_match_error:float=2.0,\n                                          verbose:bool=False)\n\nFinds the index to wavelength map given a spectra and a list of emission lines. To filter the spectra, set filter_window to an odd number &gt; 1.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nbrightest_peaks\nlist\n\nlist of wavelength for the brightest peaks in spectral lines image\n\n\nemission_lines\nlist\n\nlist of emission lines to match\n\n\ntop_k\nint\n10\nhow many peaks to use in fit\n\n\nfilter_window\nint\n1\nfilter window for scipy.signal.savgol_filter. Needs to be odd.\n\n\ninteractive_peak_id\nbool\nFalse\nflag to interactively confirm wavelength of peaks\n\n\nfind_peaks_height\nint\n10\nanything above this value is free game for a peak\n\n\nprominence\nfloat\n0.2\nprominence for scipy.signal.find_peaks\n\n\nwidth\nfloat\n1.5\npeak width for scipy.signal.find_peaks\n\n\ndistance\nint\n10\ndistance for scipy.signal.find_peaks\n\n\nmax_match_error\nfloat\n2.0\nmax diff between peak estimate wavelength and wavelength from line list\n\n\nverbose\nbool\nFalse\nmore detailed diagnostic messages\n\n\nReturns\nCurve\n\n\n\n\n\n\nsource\n\n\n\n\n SettingsBuilderMixin.fit_HgAr_lines (brightest_peaks:list=[435.833,\n                                      546.074, 763.511], top_k:int=10,\n                                      filter_window:int=1,\n                                      interactive_peak_id:bool=False,\n                                      find_peaks_height:int=10,\n                                      prominence:float=0.2,\n                                      width:float=1.5, distance:int=10,\n                                      max_match_error:float=2.0,\n                                      verbose:bool=False)\n\nFinds the index to wavelength map given a spectra and a list of emission lines. To filter the spectra, set filter_window to an odd number &gt; 1.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nbrightest_peaks\nlist\n[435.833, 546.074, 763.511]\nlist of wavelength for the brightest peaks in spectral lines image\n\n\ntop_k\nint\n10\nhow many peaks to use in fit\n\n\nfilter_window\nint\n1\nfilter window for scipy.signal.savgol_filter. Needs to be odd.\n\n\ninteractive_peak_id\nbool\nFalse\nflag to interactively confirm wavelength of peaks\n\n\nfind_peaks_height\nint\n10\nanything above this value is free game for a peak\n\n\nprominence\nfloat\n0.2\nprominence for scipy.signal.find_peaks\n\n\nwidth\nfloat\n1.5\npeak width for scipy.signal.find_peaks\n\n\ndistance\nint\n10\ndistance for scipy.signal.find_peaks\n\n\nmax_match_error\nfloat\n2.0\nmax diff between peak estimate wavelength and wavelength from line list\n\n\nverbose\nbool\nFalse\nmore detailed diagnostic messages\n\n\nReturns\nCurve\n\n\n\n\n\n\nsb.fit_HgAr_lines(top_k=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nEach column in our camera frame (after smile correction) corresponds to a particular wavelength. The interpolation between column index and wavelength is slightly nonlinear which is to be expected from the diffraction grating - however it is linear to good approximation. Applying a linear interpolation gives an absolute error of $$3 nm whereas the a cubic interpolation used here gives an absolute error of \\(\\pm\\) 0.3 nm (approximately the spacing between each column). Using higher order polynomials doesn’t improve the error due to overfitting.\nFor fast real time processing, the fast binning procedure assumes a linear interpolation because the binning algorithm consists of a single broadcasted summation with no additional memory allocation overhead. A slower more accurate spectral binning procedure is also provided using the cubic interpolation described here and requires hundreds of temporary arrays to be allocated each time. Binning can also be done in post processing after collecting raw data.\n\nsource\n\n\n\n\n SettingsBuilderMixin.update_intsphere_fit\n                                            (spec_rad_ref_data:str='../ass\n                                            ets/112704-1-1_1nm_data.csv', \n                                            spec_rad_ref_luminance:float=5\n                                            2020.0, show:bool=True)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nspec_rad_ref_data\nstr\n../assets/112704-1-1_1nm_data.csv\npath to integrating sphere cal file\n\n\nspec_rad_ref_luminance\nfloat\n52020.0\nreference luminance for integrating sphere\n\n\nshow\nbool\nTrue\nflag to show plot\n\n\nReturns\nCurve\n\n\n\n\n\n\nfig = sb.update_intsphere_fit()\n#sb.dump() # resave the settings and calibration files\n\n\n\n\n\n\n\n\n\n\n\n\n4D datacube with coordinates of cross-track, wavelength, exposure, and luminance.\n\n\n\n\n\n\nWarning\n\n\n\nNeeds testing!\n\n\n\nsource\n\n\n\n SettingsBuilderMixin.update_intsphere_cube (exposures:List,\n                                             luminances:List,\n                                             nframes:int=10,\n                                             lum_chg_func:Callable=&lt;built-\n                                             in function print&gt;,\n                                             interactive:bool=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nexposures\nList\n\nexposure times for the camera to iterate over\n\n\nluminances\nList\n\nluminance values for the integrating sphere to iterate over\n\n\nnframes\nint\n10\nhow many frames to average over\n\n\nlum_chg_func\nCallable\nprint\ncalled on each luminance value before collection starts\n\n\ninteractive\nbool\nFalse\nif you want to manually press enter each luminance iteration\n\n\n\n\nluminances=[0, 1_000, 5_000, 10_000, 20_000, 40_000]\nexposures = [0, 5, 8, 10, 15, 20]\nsb.calibration[\"rad_ref\"] = cam.update_intsphere_cube(exposures, luminances, noframe=50, lum_chg_func=spt.selectPreset)\n\n# remove saturated images\ncam.calibration[\"rad_ref\"] = cam.calibration[\"rad_ref\"].where(\n    ~(np.sum((cam.calibration[\"rad_ref\"][:, :, :, :, :] == 255), axis=(1, 2)) &gt; 1000)\n)\n\nWhen you are happy with the calibration, dump the updates.\n\n# save at the end\ncam.dump(json_path=json_path_target, cal_path=cal_path_target)\n\n\n\n\nClass to interact with the Spectra PT integrating sphere.\n\nsource\n\n\n\n\n SpectraPTController (lum_preset_dict:Dict[int,int]={0: 1, 1000: 2, 2000:\n                      3, 3000: 4, 4000: 5, 5000: 6, 6000: 7, 7000: 8,\n                      8000: 9, 9000: 10, 10000: 11, 20000: 12, 25000: 13,\n                      30000: 14, 35000: 15, 40000: 16},\n                      host:str='localhost', port:int=3434)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\n\n\n SpectraPTController.client (msg:str)\n\n\nsource\n\n\n\n\n SpectraPTController.selectPreset (lumtarget:float)\n\n\nsource\n\n\n\n\n SpectraPTController.turnOnLamp ()\n\n\nsource\n\n\n\n\n SpectraPTController.turnOffLamp ()",
    "crumbs": [
      "Home",
      "api",
      "calibrate"
    ]
  },
  {
    "objectID": "api/calibrate.html#find-illuminated-sensor-area",
    "href": "api/calibrate.html#find-illuminated-sensor-area",
    "title": "calibrate",
    "section": "",
    "text": "We assume the x axis (or detector columns) are used for the spectral channels, the rows correspond to the cross-track dimension and are limited by the optics (slit). The useable area is cropped out (windowing can also be used to reduce data intake).\n\nsource\n\n\n\n SettingsBuilderMixin.retake_flat_field (show:bool=True)\n\nTake and store an image of with the OpenHSI slit illuminated but a uniform light source.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nshow\nbool\nTrue\nflag to show taken image\n\n\nReturns\nImage\n\n\n\n\n\n\nsource\n\n\n\n\n SettingsBuilderMixin.update_row_minmax (edgezone:int=4, show:bool=True)\n\nFind edges of slit in flat field images and determine region to crop.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nedgezone\nint\n4\nnumber of pixel buffer to add to crop region\n\n\nshow\nbool\nTrue\nflag to show plot of slice and edges identified\n\n\nReturns\nCurve\n\n\n\n\n\n\nhvimg=sb.retake_flat_field(show=True)\nhvimg.opts(width=400,height=400)\nprint(sb.calibration[\"flat_field_pic\"].max())\nhvimg\n\n255\n\n\n\n\n\n\n  \n\n\n\n\n\nsb.update_row_minmax()\n\nLocs row_min: 7 and row_max: 912\n\n\n\n\n\n\n  \n\n\n\n\n\nsb.update_resolution()",
    "crumbs": [
      "Home",
      "api",
      "calibrate"
    ]
  },
  {
    "objectID": "api/calibrate.html#smile-correction",
    "href": "api/calibrate.html#smile-correction",
    "title": "calibrate",
    "section": "",
    "text": "The emissions lines, which should be straight vertical, appear slightly curved. This is smile error (error in the spectral dimension).\n\nsb.mode_change(\"HgAr\")\n\n\nsource\n\n\n\n SettingsBuilderMixin.retake_emission_lines (show:bool=True,\n                                             nframes:int=10)\n\nTake and store an image with camera illuminated with a calibration source.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nshow\nbool\nTrue\nflag to show\n\n\nnframes\nint\n10\nnumber of frames to average for image\n\n\nReturns\nImage\n\n\n\n\n\n\nsource\n\n\n\n\n SettingsBuilderMixin.retake_HgAr (show:bool=True, nframes:int=10)\n\nTake and store an image with OpenHSI camera illuminated with a HgAr calibration source.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nshow\nbool\nTrue\nflag to show\n\n\nnframes\nint\n10\nnumber of frames to average for image\n\n\nReturns\nImage\n\n\n\n\n\n\nsource\n\n\n\n\n SettingsBuilderMixin.update_smile_shifts (show=True)\n\nDetermine Smile and shifts to correct from spectral lines image.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nshow\nbool\nTrue\nflag to show plot of smile shifts for each cross track pixel.\n\n\nReturns\nCurve\n\n\n\n\n\n\nhvimg=sb.retake_HgAr(show=True, nframes=1)\nhvimg.opts(width=400,height=400)\nprint(sb.calibration[\"HgAr_pic\"].max())\nhvimg\n\n255.0\n\n\n\n\n\n\n  \n\n\n\n\n\nsb.update_smile_shifts()",
    "crumbs": [
      "Home",
      "api",
      "calibrate"
    ]
  },
  {
    "objectID": "api/calibrate.html#map-the-spectral-axis-to-wavelengths",
    "href": "api/calibrate.html#map-the-spectral-axis-to-wavelengths",
    "title": "calibrate",
    "section": "",
    "text": "To do this, peaks in the HgAr spectrum are found, refined by curve-fitting with Gaussians. The location of the peaks then allow for interpolation to get the map from array (column) index to wavelength (nm).\n\nsource\n\n\n\n SettingsBuilderMixin.fit_emission_lines (brightest_peaks:list,\n                                          emission_lines:list,\n                                          top_k:int=10,\n                                          filter_window:int=1,\n                                          interactive_peak_id:bool=False,\n                                          find_peaks_height:int=10,\n                                          prominence:float=0.2,\n                                          width:float=1.5,\n                                          distance:int=10,\n                                          max_match_error:float=2.0,\n                                          verbose:bool=False)\n\nFinds the index to wavelength map given a spectra and a list of emission lines. To filter the spectra, set filter_window to an odd number &gt; 1.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nbrightest_peaks\nlist\n\nlist of wavelength for the brightest peaks in spectral lines image\n\n\nemission_lines\nlist\n\nlist of emission lines to match\n\n\ntop_k\nint\n10\nhow many peaks to use in fit\n\n\nfilter_window\nint\n1\nfilter window for scipy.signal.savgol_filter. Needs to be odd.\n\n\ninteractive_peak_id\nbool\nFalse\nflag to interactively confirm wavelength of peaks\n\n\nfind_peaks_height\nint\n10\nanything above this value is free game for a peak\n\n\nprominence\nfloat\n0.2\nprominence for scipy.signal.find_peaks\n\n\nwidth\nfloat\n1.5\npeak width for scipy.signal.find_peaks\n\n\ndistance\nint\n10\ndistance for scipy.signal.find_peaks\n\n\nmax_match_error\nfloat\n2.0\nmax diff between peak estimate wavelength and wavelength from line list\n\n\nverbose\nbool\nFalse\nmore detailed diagnostic messages\n\n\nReturns\nCurve\n\n\n\n\n\n\nsource\n\n\n\n\n SettingsBuilderMixin.fit_HgAr_lines (brightest_peaks:list=[435.833,\n                                      546.074, 763.511], top_k:int=10,\n                                      filter_window:int=1,\n                                      interactive_peak_id:bool=False,\n                                      find_peaks_height:int=10,\n                                      prominence:float=0.2,\n                                      width:float=1.5, distance:int=10,\n                                      max_match_error:float=2.0,\n                                      verbose:bool=False)\n\nFinds the index to wavelength map given a spectra and a list of emission lines. To filter the spectra, set filter_window to an odd number &gt; 1.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nbrightest_peaks\nlist\n[435.833, 546.074, 763.511]\nlist of wavelength for the brightest peaks in spectral lines image\n\n\ntop_k\nint\n10\nhow many peaks to use in fit\n\n\nfilter_window\nint\n1\nfilter window for scipy.signal.savgol_filter. Needs to be odd.\n\n\ninteractive_peak_id\nbool\nFalse\nflag to interactively confirm wavelength of peaks\n\n\nfind_peaks_height\nint\n10\nanything above this value is free game for a peak\n\n\nprominence\nfloat\n0.2\nprominence for scipy.signal.find_peaks\n\n\nwidth\nfloat\n1.5\npeak width for scipy.signal.find_peaks\n\n\ndistance\nint\n10\ndistance for scipy.signal.find_peaks\n\n\nmax_match_error\nfloat\n2.0\nmax diff between peak estimate wavelength and wavelength from line list\n\n\nverbose\nbool\nFalse\nmore detailed diagnostic messages\n\n\nReturns\nCurve\n\n\n\n\n\n\nsb.fit_HgAr_lines(top_k=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nEach column in our camera frame (after smile correction) corresponds to a particular wavelength. The interpolation between column index and wavelength is slightly nonlinear which is to be expected from the diffraction grating - however it is linear to good approximation. Applying a linear interpolation gives an absolute error of $$3 nm whereas the a cubic interpolation used here gives an absolute error of \\(\\pm\\) 0.3 nm (approximately the spacing between each column). Using higher order polynomials doesn’t improve the error due to overfitting.\nFor fast real time processing, the fast binning procedure assumes a linear interpolation because the binning algorithm consists of a single broadcasted summation with no additional memory allocation overhead. A slower more accurate spectral binning procedure is also provided using the cubic interpolation described here and requires hundreds of temporary arrays to be allocated each time. Binning can also be done in post processing after collecting raw data.\n\nsource\n\n\n\n\n SettingsBuilderMixin.update_intsphere_fit\n                                            (spec_rad_ref_data:str='../ass\n                                            ets/112704-1-1_1nm_data.csv', \n                                            spec_rad_ref_luminance:float=5\n                                            2020.0, show:bool=True)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nspec_rad_ref_data\nstr\n../assets/112704-1-1_1nm_data.csv\npath to integrating sphere cal file\n\n\nspec_rad_ref_luminance\nfloat\n52020.0\nreference luminance for integrating sphere\n\n\nshow\nbool\nTrue\nflag to show plot\n\n\nReturns\nCurve\n\n\n\n\n\n\nfig = sb.update_intsphere_fit()\n#sb.dump() # resave the settings and calibration files",
    "crumbs": [
      "Home",
      "api",
      "calibrate"
    ]
  },
  {
    "objectID": "api/calibrate.html#integrating-sphere-data",
    "href": "api/calibrate.html#integrating-sphere-data",
    "title": "calibrate",
    "section": "",
    "text": "4D datacube with coordinates of cross-track, wavelength, exposure, and luminance.\n\n\n\n\n\n\nWarning\n\n\n\nNeeds testing!\n\n\n\nsource\n\n\n\n SettingsBuilderMixin.update_intsphere_cube (exposures:List,\n                                             luminances:List,\n                                             nframes:int=10,\n                                             lum_chg_func:Callable=&lt;built-\n                                             in function print&gt;,\n                                             interactive:bool=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nexposures\nList\n\nexposure times for the camera to iterate over\n\n\nluminances\nList\n\nluminance values for the integrating sphere to iterate over\n\n\nnframes\nint\n10\nhow many frames to average over\n\n\nlum_chg_func\nCallable\nprint\ncalled on each luminance value before collection starts\n\n\ninteractive\nbool\nFalse\nif you want to manually press enter each luminance iteration\n\n\n\n\nluminances=[0, 1_000, 5_000, 10_000, 20_000, 40_000]\nexposures = [0, 5, 8, 10, 15, 20]\nsb.calibration[\"rad_ref\"] = cam.update_intsphere_cube(exposures, luminances, noframe=50, lum_chg_func=spt.selectPreset)\n\n# remove saturated images\ncam.calibration[\"rad_ref\"] = cam.calibration[\"rad_ref\"].where(\n    ~(np.sum((cam.calibration[\"rad_ref\"][:, :, :, :, :] == 255), axis=(1, 2)) &gt; 1000)\n)\n\nWhen you are happy with the calibration, dump the updates.\n\n# save at the end\ncam.dump(json_path=json_path_target, cal_path=cal_path_target)\n\n\n\n\nClass to interact with the Spectra PT integrating sphere.\n\nsource\n\n\n\n\n SpectraPTController (lum_preset_dict:Dict[int,int]={0: 1, 1000: 2, 2000:\n                      3, 3000: 4, 4000: 5, 5000: 6, 6000: 7, 7000: 8,\n                      8000: 9, 9000: 10, 10000: 11, 20000: 12, 25000: 13,\n                      30000: 14, 35000: 15, 40000: 16},\n                      host:str='localhost', port:int=3434)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\n\n\n SpectraPTController.client (msg:str)\n\n\nsource\n\n\n\n\n SpectraPTController.selectPreset (lumtarget:float)\n\n\nsource\n\n\n\n\n SpectraPTController.turnOnLamp ()\n\n\nsource\n\n\n\n\n SpectraPTController.turnOffLamp ()",
    "crumbs": [
      "Home",
      "api",
      "calibrate"
    ]
  },
  {
    "objectID": "api/cameras/ximea.html",
    "href": "api/cameras/ximea.html",
    "title": "Ximea",
    "section": "",
    "text": "Tip\n\n\n\nThis module can be imported using from openhsi.cameras import *\n\n\nWrapper class and example code for getting images from the OpenHSI.\n\n\n\n\n\n\nTip\n\n\n\nTo use the camera, you will need some calibration files. You can also generate these files following this guide which uses the calibrate module.\n\n\n\n\nUsed for the OpenHSI Camera Mark I with a Ximea detetor (with IMX252 sensor, e.g. MX031CG-SY).\nMake sure you install the Ximea API beforehand in the instructions https://www.ximea.com/support/wiki/apis/Python\n\nsource\n\n\n\n XimeaCamera (exposure_ms:float=10, serial_num:str=None)\n\nCore functionality for Ximea cameras\n\nsource\n\n\n\n\n XimeaCameraBase (exposure_ms:float=10, serial_num:str=None)\n\nCore functionality for Ximea cameras\n\ndef run_ximea():\n    with XimeaCamera(n_lines=128, exposure_ms=1, processing_lvl = -1, cal_path=\"\",json_path='../assets/cam_settings_ximea.json') as cam:\n        cam.start_cam()\n        for i in tqdm(range(cam.n_lines)):\n            cam.put(cam.get_img())\n        cam.stop_cam()\n\n\n#fig = cam.show(robust=True)    \n#fig\n\n\nwith XimeaCamera(n_lines=128, exposure_ms=1, processing_lvl = -1, cal_path=\"\",json_path='../assets/cam_settings_ximea.json') as cam:\n    cam.collect()\n\nAllocated 251.56 MB of RAM. There was 12433.93 MB available.\n\n\nxiAPI: ---- xiOpenDevice API:V4.27.07.00 started ----\nxiAPI: EAL_IF_xiFAPI_Top::InitializeDevice sn:CEMAU2105019 name:MC031MG-SY-UB\nxiAPI: FGTL_SetParam_to_CAL error from CAL: -1015, addr:x27317e\nxiAPI: XiApiToGentlParamModel Auto bandwidth measurement finished (396MBps). Safe limit set to: 317MBps\nxiAPI: FGTL_SetParam_to_CAL error from CAL: -10009, addr:x201380\nxiAPI: ---- Device opened. Model:MC031MG-SY-UB SN:CEMAU2105019 FwF1:01.31 API:V4.27.07.00 ----\n\n\nConnected to device b'CEMAU2105019'\n\n\nxiAPI: xiFAPI_Device::AllocateBuffers Allocating buffers. Count:346 OneBuff:756 KiB All:256 MiB Frm:x10c0047\nxiAPI: xiAPI error: Expected XI_OK in:../API/xiFAPI/camera_model/XiApiToGentlParamModel.cpp GetHDR/Line:622\nxiAPI: Failed to change thread scheduler, check user limit for realtime priority.\n100%|█████████████████████████████████████████████████████████████████████| 128/128 [00:00&lt;00:00, 163.96it/s]\nxiAPI: xiFAPI_Device::AcquisitionStop - ignored: acquisition is not running\nxiAPI: xiCloseDevice\n\n\n\n\n\nExport cameras using the SharedOpenHSI class.\n\nsource\n\n\n\n\n SharedXimeaCamera (exposure_ms:float=10, serial_num:str=None)\n\nCore functionality for Ximea cameras",
    "crumbs": [
      "Home",
      "cameras",
      "Ximea"
    ]
  },
  {
    "objectID": "api/cameras/ximea.html#ximea-camera",
    "href": "api/cameras/ximea.html#ximea-camera",
    "title": "Ximea",
    "section": "",
    "text": "Used for the OpenHSI Camera Mark I with a Ximea detetor (with IMX252 sensor, e.g. MX031CG-SY).\nMake sure you install the Ximea API beforehand in the instructions https://www.ximea.com/support/wiki/apis/Python\n\nsource\n\n\n\n XimeaCamera (exposure_ms:float=10, serial_num:str=None)\n\nCore functionality for Ximea cameras\n\nsource\n\n\n\n\n XimeaCameraBase (exposure_ms:float=10, serial_num:str=None)\n\nCore functionality for Ximea cameras\n\ndef run_ximea():\n    with XimeaCamera(n_lines=128, exposure_ms=1, processing_lvl = -1, cal_path=\"\",json_path='../assets/cam_settings_ximea.json') as cam:\n        cam.start_cam()\n        for i in tqdm(range(cam.n_lines)):\n            cam.put(cam.get_img())\n        cam.stop_cam()\n\n\n#fig = cam.show(robust=True)    \n#fig\n\n\nwith XimeaCamera(n_lines=128, exposure_ms=1, processing_lvl = -1, cal_path=\"\",json_path='../assets/cam_settings_ximea.json') as cam:\n    cam.collect()\n\nAllocated 251.56 MB of RAM. There was 12433.93 MB available.\n\n\nxiAPI: ---- xiOpenDevice API:V4.27.07.00 started ----\nxiAPI: EAL_IF_xiFAPI_Top::InitializeDevice sn:CEMAU2105019 name:MC031MG-SY-UB\nxiAPI: FGTL_SetParam_to_CAL error from CAL: -1015, addr:x27317e\nxiAPI: XiApiToGentlParamModel Auto bandwidth measurement finished (396MBps). Safe limit set to: 317MBps\nxiAPI: FGTL_SetParam_to_CAL error from CAL: -10009, addr:x201380\nxiAPI: ---- Device opened. Model:MC031MG-SY-UB SN:CEMAU2105019 FwF1:01.31 API:V4.27.07.00 ----\n\n\nConnected to device b'CEMAU2105019'\n\n\nxiAPI: xiFAPI_Device::AllocateBuffers Allocating buffers. Count:346 OneBuff:756 KiB All:256 MiB Frm:x10c0047\nxiAPI: xiAPI error: Expected XI_OK in:../API/xiFAPI/camera_model/XiApiToGentlParamModel.cpp GetHDR/Line:622\nxiAPI: Failed to change thread scheduler, check user limit for realtime priority.\n100%|█████████████████████████████████████████████████████████████████████| 128/128 [00:00&lt;00:00, 163.96it/s]\nxiAPI: xiFAPI_Device::AcquisitionStop - ignored: acquisition is not running\nxiAPI: xiCloseDevice\n\n\n\n\n\nExport cameras using the SharedOpenHSI class.\n\nsource\n\n\n\n\n SharedXimeaCamera (exposure_ms:float=10, serial_num:str=None)\n\nCore functionality for Ximea cameras",
    "crumbs": [
      "Home",
      "cameras",
      "Ximea"
    ]
  },
  {
    "objectID": "api/cameras/simulated.html",
    "href": "api/cameras/simulated.html",
    "title": "Simulated Camera",
    "section": "",
    "text": "Tip\n\n\n\nThis module can be imported using from openhsi.cameras import *\n\n\nSimulated camera implementation for testing and development purposes. The simulated camera can:\n\nGenerate hyperspectral data from RGB images using CIE XYZ matching functions\nSimulate HgAr calibration lamp spectra\nGenerate flat field data using blackbody radiation\nProvide consistent interface matching hardware cameras\n\n\n\nThe SimulatedCamera provides a software-only implementation that follows the same interface as hardware cameras. It’s useful for:\n\nTesting camera functionality without hardware\nDevelopment and debugging\nGenerating synthetic hyperspectral data\nCalibration testing with known spectra\n\n\nsource\n\n\n\n SimulatedCameraBase (img_path:str=None, mode:str=None)\n\nCore functionality for simulated camera\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimg_path\nstr\nNone\nPath to an RGB image file\n\n\nmode\nstr\nNone\nDefault is to generate lines from the RGB image. Other options are HgAr and flat to simulate the HgAr spectrum and a flat field respectively.\n\n\n\n\nsource\n\n\n\n\n SimulatedCamera (img_path:str=None, mode:str=None)\n\nSimulated camera using an RGB image as input. Hyperspectral data is produced using CIE XYZ matching functions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimg_path\nstr\nNone\nPath to an RGB image file\n\n\nmode\nstr\nNone\nDefault is to generate lines from the RGB image. Other options are HgAr and flat to simulate the HgAr spectrum and a flat field respectively.\n\n\n\n\nsource\n\n\n\n\n SimulatedCameraBase.mode_change (mode:str=None)\n\nSwitch between simulating HgAr, flat field, or neither.\n\nsource\n\n\n\n\n SimulatedCameraBase.rgb2xyz_matching_funcs (rgb:numpy.ndarray)\n\nconvert an RGB value to a pseudo-spectra with the CIE XYZ matching functions.\n\nwith SimulatedCamera(img_path=\"../../assets/great_hall_slide.png\",\n                     n_lines=1606, \n                     processing_lvl = 2, \n                     json_path=\"../../assets/cam_settings.json\",\n                     cal_path=\"../../assets/cam_calibration.nc\") as cam:\n    cam.collect()\n    fig = cam.show(plot_lib=\"bokeh\",hist_eq=True)\n\nAllocated 754.04 MB of RAM. There was 31658.28 MB available.\n\n\n100%|██████████████████████████████████████| 1606/1606 [00:08&lt;00:00, 184.10it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nfig.opts(width=400,height=300,title=\"simulated hyperspectral datacube\")\n\n\n\n\n\n  \n\n\n\n\nEach RGB value is converted into a pseudo-spectra by using the CIE XYZ matching functions.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\nWe will use the Sun’s blackbody radiation for this.\n\nsource\n\n\n\n\n SimulatedCameraBase.gen_flat ()\n\nsimulated blackbody radiation\n\nwith SimulatedCamera(mode=\"flat\", \n                     n_lines=128, \n                     processing_lvl = -1, \n                     json_path=\"../../assets/cam_settings.json\",\n                     cal_path=\"../../assets/cam_calibration.nc\",\n                     ) as cam:\n    cam.collect()\n    fig = cam.show(plot_lib=\"bokeh\")\n\nfig.opts(width=256,height=400)\n\nAllocated 139.86 MB of RAM. There was 31190.12 MB available.\n\n\n100%|███████████████████████████████████████| 128/128 [00:00&lt;00:00, 3281.47it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n  \n\n\n\n\nIf we look at each simulated picture as it goes into the datacube, it looks like this a blackbody spectrum along the wavelength axis. There are also top and bottom black bars to simulate the rows that would get illuminated in a real camera.\n\n# Create the image plot\nimg = hv.Image(cam.dc.data[:,0,:]).opts(\n    cmap='gray',\n    xlabel=\"wavelength index\",\n    ylabel=\"cross-track\",\n    width=600,\n    height=400,\n    colorbar=True\n)\n\nimg\n\n\n\n\n\n  \n\n\n\n\n\n\n\nFor testing wavelength calibration.\n\nsource\n\n\n\n\n SimulatedCameraBase.gen_sim_spectra ()\n\nsimulated picture of a HgAr lamp\n\nwith SimulatedCamera(mode=\"HgAr\", n_lines=128, processing_lvl = -1, \n                     json_path=\"../../assets/cam_settings.json\",\n                     cal_path=\"../../assets/cam_calibration.nc\",\n                     ) as cam:\n    cam.collect()\n\nAllocated 139.86 MB of RAM. There was 31465.22 MB available.\n\n\n100%|███████████████████████████████████████| 128/128 [00:00&lt;00:00, 7084.32it/s]\n\n\nWe can see the emission lines in roughly the spot where a real HgAr spectral line should fall. The intensity of each emission line is also roughly simulated.\n\n# Create the image plot\nimg = hv.Image(cam.dc.data[:,0,:]).opts(\n    cmap='gray',\n    xlabel=\"wavelength index\",\n    ylabel=\"cross-track\",\n    width=600,\n    height=400,\n    colorbar=True\n)\n\nimg\n\n\n\n\n\n  \n\n\n\n\n\nsource\n\n\n\n\n SharedSimulatedCamera (img_path:str=None, mode:str=None)\n\nCore functionality for simulated camera\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimg_path\nstr\nNone\nPath to an RGB image file\n\n\nmode\nstr\nNone\nDefault is to generate lines from the RGB image. Other options are HgAr and flat to simulate the HgAr spectrum and a flat field respectively.\n\n\n\nDue to requiring double the amount of memory and more to facilitate saving in a separate process, make sure your datacubes can fit in your RAM. Have not tested this but I would suggest choosing n_lines &lt;= 1/3 the amount used using the regular OpenHSI.",
    "crumbs": [
      "Home",
      "cameras",
      "Simulated Camera"
    ]
  },
  {
    "objectID": "api/cameras/simulated.html#simulated-camera",
    "href": "api/cameras/simulated.html#simulated-camera",
    "title": "Simulated Camera",
    "section": "",
    "text": "The SimulatedCamera provides a software-only implementation that follows the same interface as hardware cameras. It’s useful for:\n\nTesting camera functionality without hardware\nDevelopment and debugging\nGenerating synthetic hyperspectral data\nCalibration testing with known spectra\n\n\nsource\n\n\n\n SimulatedCameraBase (img_path:str=None, mode:str=None)\n\nCore functionality for simulated camera\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimg_path\nstr\nNone\nPath to an RGB image file\n\n\nmode\nstr\nNone\nDefault is to generate lines from the RGB image. Other options are HgAr and flat to simulate the HgAr spectrum and a flat field respectively.\n\n\n\n\nsource\n\n\n\n\n SimulatedCamera (img_path:str=None, mode:str=None)\n\nSimulated camera using an RGB image as input. Hyperspectral data is produced using CIE XYZ matching functions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimg_path\nstr\nNone\nPath to an RGB image file\n\n\nmode\nstr\nNone\nDefault is to generate lines from the RGB image. Other options are HgAr and flat to simulate the HgAr spectrum and a flat field respectively.\n\n\n\n\nsource\n\n\n\n\n SimulatedCameraBase.mode_change (mode:str=None)\n\nSwitch between simulating HgAr, flat field, or neither.\n\nsource\n\n\n\n\n SimulatedCameraBase.rgb2xyz_matching_funcs (rgb:numpy.ndarray)\n\nconvert an RGB value to a pseudo-spectra with the CIE XYZ matching functions.\n\nwith SimulatedCamera(img_path=\"../../assets/great_hall_slide.png\",\n                     n_lines=1606, \n                     processing_lvl = 2, \n                     json_path=\"../../assets/cam_settings.json\",\n                     cal_path=\"../../assets/cam_calibration.nc\") as cam:\n    cam.collect()\n    fig = cam.show(plot_lib=\"bokeh\",hist_eq=True)\n\nAllocated 754.04 MB of RAM. There was 31658.28 MB available.\n\n\n100%|██████████████████████████████████████| 1606/1606 [00:08&lt;00:00, 184.10it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nfig.opts(width=400,height=300,title=\"simulated hyperspectral datacube\")\n\n\n\n\n\n  \n\n\n\n\nEach RGB value is converted into a pseudo-spectra by using the CIE XYZ matching functions.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\nWe will use the Sun’s blackbody radiation for this.\n\nsource\n\n\n\n\n SimulatedCameraBase.gen_flat ()\n\nsimulated blackbody radiation\n\nwith SimulatedCamera(mode=\"flat\", \n                     n_lines=128, \n                     processing_lvl = -1, \n                     json_path=\"../../assets/cam_settings.json\",\n                     cal_path=\"../../assets/cam_calibration.nc\",\n                     ) as cam:\n    cam.collect()\n    fig = cam.show(plot_lib=\"bokeh\")\n\nfig.opts(width=256,height=400)\n\nAllocated 139.86 MB of RAM. There was 31190.12 MB available.\n\n\n100%|███████████████████████████████████████| 128/128 [00:00&lt;00:00, 3281.47it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n  \n\n\n\n\nIf we look at each simulated picture as it goes into the datacube, it looks like this a blackbody spectrum along the wavelength axis. There are also top and bottom black bars to simulate the rows that would get illuminated in a real camera.\n\n# Create the image plot\nimg = hv.Image(cam.dc.data[:,0,:]).opts(\n    cmap='gray',\n    xlabel=\"wavelength index\",\n    ylabel=\"cross-track\",\n    width=600,\n    height=400,\n    colorbar=True\n)\n\nimg\n\n\n\n\n\n  \n\n\n\n\n\n\n\nFor testing wavelength calibration.\n\nsource\n\n\n\n\n SimulatedCameraBase.gen_sim_spectra ()\n\nsimulated picture of a HgAr lamp\n\nwith SimulatedCamera(mode=\"HgAr\", n_lines=128, processing_lvl = -1, \n                     json_path=\"../../assets/cam_settings.json\",\n                     cal_path=\"../../assets/cam_calibration.nc\",\n                     ) as cam:\n    cam.collect()\n\nAllocated 139.86 MB of RAM. There was 31465.22 MB available.\n\n\n100%|███████████████████████████████████████| 128/128 [00:00&lt;00:00, 7084.32it/s]\n\n\nWe can see the emission lines in roughly the spot where a real HgAr spectral line should fall. The intensity of each emission line is also roughly simulated.\n\n# Create the image plot\nimg = hv.Image(cam.dc.data[:,0,:]).opts(\n    cmap='gray',\n    xlabel=\"wavelength index\",\n    ylabel=\"cross-track\",\n    width=600,\n    height=400,\n    colorbar=True\n)\n\nimg\n\n\n\n\n\n  \n\n\n\n\n\nsource\n\n\n\n\n SharedSimulatedCamera (img_path:str=None, mode:str=None)\n\nCore functionality for simulated camera\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimg_path\nstr\nNone\nPath to an RGB image file\n\n\nmode\nstr\nNone\nDefault is to generate lines from the RGB image. Other options are HgAr and flat to simulate the HgAr spectrum and a flat field respectively.\n\n\n\nDue to requiring double the amount of memory and more to facilitate saving in a separate process, make sure your datacubes can fit in your RAM. Have not tested this but I would suggest choosing n_lines &lt;= 1/3 the amount used using the regular OpenHSI.",
    "crumbs": [
      "Home",
      "cameras",
      "Simulated Camera"
    ]
  },
  {
    "objectID": "api/cameras/lucidvision.html",
    "href": "api/cameras/lucidvision.html",
    "title": "Lucid Vision Cameras",
    "section": "",
    "text": "Tip\n\n\n\nThis module can be imported using from openhsi.cameras import LucidCamera\n\n\nWrapper class and example code for getting images from the OpenHSI.\n\n\n\n\n\n\nTip\n\n\n\nTo use the camera, you will need some calibration files. You can also generate these files following this guide which uses the calibrate module.\n\n\nYou can see a picture I held up to the webcam.\n\n\nUsed for the OpenHSI Camera from Sydney Photonics.\nMake sure you installthe Lucid Vision Labs Arena SDK and python api beforehand. These can be found here https://thinklucid.com/downloads-hub/\nAny keyword-value pair arguments must match the those avaliable in settings file. LucidCamera expects the ones listed below:\n\nbinxy: number of pixels to bin in (x,y) direction\nwin_resolution: size of area on detector to readout (width, height)\nwin_offset: offsets (x,y) from edge of detector for a selective\nexposure_ms: is the camera exposure time to use\npixel_format: format of pixels readout sensor, ie Mono8, Mono10, Mono10Packed, Mono12, Mono12Packed, Mono16\nmac_addr: The mac address of the GigE sensor as string i.e. “1c:0f:af:01:7b:a0”\n\n\nsource\n\n\n\n LucidCamera ()\n\n*Core functionality for Lucid Vision Lab cameras\nAny keyword-value pair arguments must match the those avaliable in settings file. LucidCamera expects the ones listed below:\n\nbinxy: number of pixels to bin in (x,y) direction\nwin_resolution: size of area on detector to readout (width, height)\nwin_offset: offsets (x,y) from edge of detector for a selective\nexposure_ms: is the camera exposure time to use\npixel_format: format of pixels readout sensor, ie Mono8, Mono10, Mono10p, Mono10Packed, Mono12, Mono12p, Mono12Packed, Mono16\nmac_addr: str = “1c:0f:af:01:7b:a0”,*\n\n\nsource\n\n\n\n\n LucidCameraBase ()\n\n*Core functionality for Lucid Vision Lab cameras\nAny keyword-value pair arguments must match the those avaliable in settings file. LucidCamera expects the ones listed below:\n\nbinxy: number of pixels to bin in (x,y) direction\nwin_resolution: size of area on detector to readout (width, height)\nwin_offset: offsets (x,y) from edge of detector for a selective\nexposure_ms: is the camera exposure time to use\npixel_format: format of pixels readout sensor, ie Mono8, Mono10, Mono10p, Mono10Packed, Mono12, Mono12p, Mono12Packed, Mono16\nmac_addr: str = “1c:0f:af:01:7b:a0”,*\n\n\njson_path='cals/OpenHSI-07/OpenHSI-07_settings_Mono8_bin1.json'\ncal_path='cals/OpenHSI-07/OpenHSI-07_calibration_Mono8_bin1.pkl'\n\nwith LucidCamera(n_lines=1000, \n                 processing_lvl = 2, \n                 cal_path=cal_path,json_path=json_path,\n                 exposure_ms=10\n                ) as cam:\n    cam.collect()\n     \ncam.show(hist_eq=True)\n\n100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:11&lt;00:00, 88.62it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\nExport cameras using the SharedOpenHSI class.\n\nsource\n\n\n\n\n SharedLucidCamera ()\n\n*Core functionality for Lucid Vision Lab cameras\nAny keyword-value pair arguments must match the those avaliable in settings file. LucidCamera expects the ones listed below:\n\nbinxy: number of pixels to bin in (x,y) direction\nwin_resolution: size of area on detector to readout (width, height)\nwin_offset: offsets (x,y) from edge of detector for a selective\nexposure_ms: is the camera exposure time to use\npixel_format: format of pixels readout sensor, ie Mono8, Mono10, Mono10p, Mono10Packed, Mono12, Mono12p, Mono12Packed, Mono16\nmac_addr: str = “1c:0f:af:01:7b:a0”,*",
    "crumbs": [
      "Home",
      "cameras",
      "Lucid Vision Cameras"
    ]
  },
  {
    "objectID": "api/cameras/lucidvision.html#lucid-vision-labs-camera",
    "href": "api/cameras/lucidvision.html#lucid-vision-labs-camera",
    "title": "Lucid Vision Cameras",
    "section": "",
    "text": "Used for the OpenHSI Camera from Sydney Photonics.\nMake sure you installthe Lucid Vision Labs Arena SDK and python api beforehand. These can be found here https://thinklucid.com/downloads-hub/\nAny keyword-value pair arguments must match the those avaliable in settings file. LucidCamera expects the ones listed below:\n\nbinxy: number of pixels to bin in (x,y) direction\nwin_resolution: size of area on detector to readout (width, height)\nwin_offset: offsets (x,y) from edge of detector for a selective\nexposure_ms: is the camera exposure time to use\npixel_format: format of pixels readout sensor, ie Mono8, Mono10, Mono10Packed, Mono12, Mono12Packed, Mono16\nmac_addr: The mac address of the GigE sensor as string i.e. “1c:0f:af:01:7b:a0”\n\n\nsource\n\n\n\n LucidCamera ()\n\n*Core functionality for Lucid Vision Lab cameras\nAny keyword-value pair arguments must match the those avaliable in settings file. LucidCamera expects the ones listed below:\n\nbinxy: number of pixels to bin in (x,y) direction\nwin_resolution: size of area on detector to readout (width, height)\nwin_offset: offsets (x,y) from edge of detector for a selective\nexposure_ms: is the camera exposure time to use\npixel_format: format of pixels readout sensor, ie Mono8, Mono10, Mono10p, Mono10Packed, Mono12, Mono12p, Mono12Packed, Mono16\nmac_addr: str = “1c:0f:af:01:7b:a0”,*\n\n\nsource\n\n\n\n\n LucidCameraBase ()\n\n*Core functionality for Lucid Vision Lab cameras\nAny keyword-value pair arguments must match the those avaliable in settings file. LucidCamera expects the ones listed below:\n\nbinxy: number of pixels to bin in (x,y) direction\nwin_resolution: size of area on detector to readout (width, height)\nwin_offset: offsets (x,y) from edge of detector for a selective\nexposure_ms: is the camera exposure time to use\npixel_format: format of pixels readout sensor, ie Mono8, Mono10, Mono10p, Mono10Packed, Mono12, Mono12p, Mono12Packed, Mono16\nmac_addr: str = “1c:0f:af:01:7b:a0”,*\n\n\njson_path='cals/OpenHSI-07/OpenHSI-07_settings_Mono8_bin1.json'\ncal_path='cals/OpenHSI-07/OpenHSI-07_calibration_Mono8_bin1.pkl'\n\nwith LucidCamera(n_lines=1000, \n                 processing_lvl = 2, \n                 cal_path=cal_path,json_path=json_path,\n                 exposure_ms=10\n                ) as cam:\n    cam.collect()\n     \ncam.show(hist_eq=True)\n\n100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:11&lt;00:00, 88.62it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\nExport cameras using the SharedOpenHSI class.\n\nsource\n\n\n\n\n SharedLucidCamera ()\n\n*Core functionality for Lucid Vision Lab cameras\nAny keyword-value pair arguments must match the those avaliable in settings file. LucidCamera expects the ones listed below:\n\nbinxy: number of pixels to bin in (x,y) direction\nwin_resolution: size of area on detector to readout (width, height)\nwin_offset: offsets (x,y) from edge of detector for a selective\nexposure_ms: is the camera exposure time to use\npixel_format: format of pixels readout sensor, ie Mono8, Mono10, Mono10p, Mono10Packed, Mono12, Mono12p, Mono12Packed, Mono16\nmac_addr: str = “1c:0f:af:01:7b:a0”,*",
    "crumbs": [
      "Home",
      "cameras",
      "Lucid Vision Cameras"
    ]
  },
  {
    "objectID": "api/metadata.html",
    "href": "api/metadata.html",
    "title": "metadata",
    "section": "",
    "text": "Metadata Editor\n\n\n\n\n\n\nTip\n\n\n\nThis module can be imported using from openhsi.metadata import *\n\n\n“As metadata are shared between National and International repositories, it is becoming increasing important to be able to unambiguously identify and refer to specific records. This is facilitated by including an identifier in the metadata. Some mechanism must exist for ensuring that these identifiers are unique. This is accomplished by specifying the naming authority or namespace for the identifier. It is the responsibility of the manager of the namespace to ensure that the identifiers in that namespace are unique. Identifying the Metadata Convention being used in the file and providing a link to more complete metadata, possibly using a different convention, are also important.”\nhttps://wiki.esipfed.org/Attribute_Convention_for_Data_Discovery_1-3\n“As metadata are shared between National and International repositories, it is becoming increasing important to be able to unambiguously identify and refer to specific records. This is facilitated by including an identifier in the metadata. Some mechanism must exist for ensuring that these identifiers are unique. This is accomplished by specifying the naming authority or namespace for the identifier. It is the responsibility of the manager of the namespace to ensure that the identifiers in that namespace are unique. Identifying the Metadata Convention being used in the file and providing a link to more complete metadata, possibly using a different convention, are also important.”\nhttps://wiki.esipfed.org/Attribute_Convention_for_Data_Discovery_1-3\n\nsource\n\nbuild_section_widgets\n\n build_section_widgets (section_name:str='', fields:List[dict]=[{}],\n                        cols:int=3)\n\nConstruct text input widgets and place them into cols columns\nStandard name table https://cfconventions.org/Data/cf-standard-names/current/build/cf-standard-name-table.html\n\nsource\n\n\nbuild_variables_widgets\n\n build_variables_widgets (ds:xarray.core.dataset.Dataset, cols:int=3)\n\nsearch inside NetCDF coordinates for metadata. Layout widgets with specified cols.\n\nsource\n\n\nMetadataEditor\n\n MetadataEditor ()\n\nInteractive ISO 19115-2 Metadata Editor\n\nsource\n\n\nMetadataEditor.__call__\n\n MetadataEditor.__call__ ()\n\nprovide the dashboard\n\nsource\n\n\nMetadataEditor.update_export\n\n MetadataEditor.update_export ()\n\nsetup the json export button callback\n\nsource\n\n\nMetadataEditor.update_extract\n\n MetadataEditor.update_extract ()\n\nsetup the load file button callback\n\nsource\n\n\nMetadataEditor.update_save\n\n MetadataEditor.update_save ()\n\nsetup the in-place file update button callback\n\nsource\n\n\nMetadataEditor.make_widgets\n\n MetadataEditor.make_widgets ()\n\ninit widgets!\n\n\n\n\n\n\nWarning\n\n\n\nThis web page was generated from a Jupyter notebook and not all interactivity will work on this website. Download the notebook and run locally.\n\n\n\nme = MetadataEditor()\nme()",
    "crumbs": [
      "Home",
      "api",
      "metadata"
    ]
  },
  {
    "objectID": "api/geometry.html",
    "href": "api/geometry.html",
    "title": "geometry",
    "section": "",
    "text": "Tip\n\n\n\nThis module can be imported using from openhsi.geometry import *\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis is still under development. Stay tuned\n\n\n\nproced_dc = GeorectifyDatacube(fname = \"../may_trial_dataset_26th/2021_05_26-03_26_26_ELC.nc\", \n                              DEM_path = None)\n\n# add functions to a pipeline\nidentity = lambda x: x\nproced_dc.load_next_tfms([identity])\n\n# run the first line through the processing pipeline and put into output datacube buffer\nproced_dc.put(proced_dc.get_img())\nplt.imshow(proced_dc.dc.data[:,0,:])\n\nAllocated 572.06 MB of RAM for the load buffer. There was 5107.05 MB available.\nAllocated 572.06 MB of RAM. There was 4560.30 MB available.\nidentity applied",
    "crumbs": [
      "Home",
      "api",
      "geometry"
    ]
  },
  {
    "objectID": "api/data.html",
    "href": "api/data.html",
    "title": "data",
    "section": "",
    "text": "Tip\n\n\n\nThis module can be imported using from openhsi.data import *",
    "crumbs": [
      "Home",
      "api",
      "data"
    ]
  },
  {
    "objectID": "api/data.html#generic-circular-buffer-on-numpy.ndarrays",
    "href": "api/data.html#generic-circular-buffer-on-numpy.ndarrays",
    "title": "data",
    "section": "Generic Circular Buffer on numpy.ndarrays",
    "text": "Generic Circular Buffer on numpy.ndarrays\nThe base functionality is implemented on a generic circular buffer. The datatype dtype can be modified as desired but the default is set to store uint16 digital numbers.\n\nsource\n\nCircArrayBuffer\n\n CircArrayBuffer (size:tuple=(100, 100), axis:int=0, dtype:type=&lt;class\n                  'numpy.uint8'&gt;, show_func:Callable[[numpy.ndarray],Forwa\n                  rdRef('plot')]=None)\n\nCircular FIFO Buffer implementation on ndarrays. Each put/get is a (n-1)darray.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsize\ntuple\n(100, 100)\nShape of n-dim circular buffer to preallocate\n\n\naxis\nint\n0\nWhich axis to traverse when filling the buffer\n\n\ndtype\ntype\nuint8\nBuffer numpy data type\n\n\nshow_func\nCallable\nNone\nCustom plotting function if desired\n\n\n\n\nsource\n\n\nCircArrayBuffer.put\n\n CircArrayBuffer.put (line:numpy.ndarray)\n\nWrites a (n-1)darray into the buffer\n\nsource\n\n\nCircArrayBuffer.get\n\n CircArrayBuffer.get ()\n\nReads the oldest (n-1)darray from the buffer\n\nsource\n\n\nCircArrayBuffer.show\n\n CircArrayBuffer.show ()\n\nDisplay the data\nFor example, we can write to a 1D array\n\ncib = CircArrayBuffer(size=(7,),axis=0)\nfor i in range(9):\n    cib.put(i)\n    cib.show()\n\nfor i in range(9):\n    print(i,cib.get())\n\n#(7) [0 0 0 0 0 0 0]\n#(7) [0 1 0 0 0 0 0]\n#(7) [0 1 2 0 0 0 0]\n#(7) [0 1 2 3 0 0 0]\n#(7) [0 1 2 3 4 0 0]\n#(7) [0 1 2 3 4 5 0]\n#(7) [0 1 2 3 4 5 6]\n#(7) [7 1 2 3 4 5 6]\n#(7) [7 8 2 3 4 5 6]\n0 2\n1 3\n2 4\n3 5\n4 6\n5 7\n6 8\n7 None\n8 None\n\n\nOr a 2D array\n\nplots_list = []\n\ncib = CircArrayBuffer(size=(4,4),axis=0)\ncib.put(1) # scalars are broadcasted to a 1D array\nfor i in range(5):\n    cib.put(cib.get()+1)\n    plots_list.append( cib.show().opts(colorbar=True,title=f\"i={i}\") )\n\nhv.Layout(plots_list).cols(3)",
    "crumbs": [
      "Home",
      "api",
      "data"
    ]
  },
  {
    "objectID": "api/data.html#loading-camera-settings-and-calibration-files",
    "href": "api/data.html#loading-camera-settings-and-calibration-files",
    "title": "data",
    "section": "Loading Camera Settings and Calibration Files",
    "text": "Loading Camera Settings and Calibration Files\nThe OpenHSI camera has a settings dictionary which contains these fields: - camera_id is your camera name, - row_slice indicates which rows are illuminated and we crop out the rest, - resolution is the full pixel resolution given by the camera without cropping, and - fwhm_nm specifies the size of spectral bands in nanometers, - exposure_ms is the camera exposure time last used, - luminance is the reference luminance to convert digital numbers to luminance, - longitude is the longitude degrees east, - latitude is the latitude degrees north, - datetime_str is the UTC time at time of data collection, - altitude is the altitude above sea level (assuming target is at sea level) measured in km, - radiosonde_station_num is the station number from http://weather.uwyo.edu/upperair/sounding.html, - radiosonde_region is the region code from http://weather.uwyo.edu/upperair/sounding.html, and - sixs_path is the path to the 6SV executable. - binxy number of pixels to bin in (x,y) direction - win_offset offsets (x,y) from edge of detector for a selective readout window (used in combination with a win_resolution less than full detector size). - win_resolution size of area on detector to readout (width, height) - pixel_format format of pixels readout sensor, ie 8bit, 10bit, 12bit\nThe settings dictionary may also contain additional camera specific fields: - mac_addr is GigE camera mac address - used by Lucid Vision Sensors - serial_num serial number of detector - used by Ximea and FLIR Sensors\nThe pickle file is a dictionary with these fields: - camera_id is your camera name, - HgAr_pic is a picture of a mercury argon lamp’s spectral lines for wavelength calibration, - flat_field_pic is a picture of a well lit for calculating the illuminated area, - smile_shifts is an array of pixel shifts needed to correct for smile error, - wavelengths_linear is an array of wavelengths after linear interpolation, - wavelengths is an array of wavelengths after cubic interpolation, - rad_ref is a 4D datacube with coordinates of cross-track, wavelength, exposure, and luminance, - sfit is the spline fit function from the integrating sphere calibration, and - rad_fit is the interpolated function of the expected radiance at sensor computed using 6SV.\nThese files are unique to each OpenHSI camera.\n\nsource\n\nCameraProperties\n\n CameraProperties (json_path:str=None, cal_path:str=None,\n                   print_settings:bool=False, **kwargs)\n\nSave and load OpenHSI camera settings and calibration\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\njson_path\nstr\nNone\nPath to settings file\n\n\ncal_path\nstr\nNone\nPath to calibration file\n\n\nprint_settings\nbool\nFalse\nPrint out settings file contents\n\n\nkwargs\nVAR_KEYWORD\n\n\n\n\n\n\nsource\n\n\nCameraProperties.dump\n\n CameraProperties.dump (json_path:str=None, cal_path:str=None,\n                        use_pickle:bool=False)\n\nSave the settings and calibration files\nFor example, the contents of CameraProperties consists of two dictionaries. To produce the files cam_settings.json and cam_calibration.nc, follow the steps outlined in the calibration module.\n\n#collapse_output\n\ncam_prop = CameraProperties(cal_path=\"../assets/cam_calibration.nc\")\ncam_prop\n\nsettings = \n{}\n\ncalibration = \n{'HgAr_pic': array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], shape=(924, 1240)),\n 'flat_field_pic': array([[ 9,  7,  9, ...,  2,  3,  1],\n       [ 8,  7,  8, ...,  3,  3,  4],\n       [ 8,  7, 10, ...,  6,  7,  6],\n       ...,\n       [ 8,  6,  7, ...,  2,  3,  1],\n       [ 3,  3,  3, ...,  1,  2,  2],\n       [ 1,  1,  1, ...,  1,  1,  1]], shape=(924, 1240), dtype=uint8),\n 'rad_fit': &lt;scipy.interpolate._interpolate.interp1d object&gt;,\n 'rad_ref': &lt;xarray.DataArray 'rad_ref' (variable: 1, cross_track: 905,\n                             wavelength_index: 1240, exposure: 2, luminance: 2)&gt; Size: 18MB\n[4488800 values with dtype=int32]\nCoordinates:\n  * cross_track       (cross_track) int32 4kB 0 1 2 3 4 ... 900 901 902 903 904\n  * wavelength_index  (wavelength_index) int32 5kB 0 1 2 3 ... 1237 1238 1239\n  * exposure          (exposure) int32 8B 10 20\n  * luminance         (luminance) int32 8B 0 10000\n  * variable          (variable) &lt;U8 32B 'datacube',\n 'sfit': &lt;scipy.interpolate._interpolate.interp1d object&gt;,\n 'smile_shifts': array([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n       9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n       9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n       9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n       9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n       9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 8, 8,\n       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 7, 7, 7, 7,\n       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 6,\n       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4,\n       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3,\n       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n       3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0], dtype=int16),\n 'spec_rad_ref_luminance': np.int64(52020),\n 'wavelengths': array([418.37123297, 418.74477592, 419.11877271, ..., 893.95644796,\n       894.09194015, 894.22658974], shape=(1231,)),\n 'wavelengths_linear': array([419.81906422, 420.25228866, 420.68551311, ..., 951.81868076,\n       952.2519052 , 952.68512965], shape=(1231,))}\n\n\n\n# # Show the integrating sphere calibration references\ncam_prop.calibration[\"rad_ref\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'rad_ref' (variable: 1, cross_track: 905,\n                             wavelength_index: 1240, exposure: 2, luminance: 2)&gt; Size: 18MB\n[4488800 values with dtype=int32]\nCoordinates:\n  * cross_track       (cross_track) int32 4kB 0 1 2 3 4 ... 900 901 902 903 904\n  * wavelength_index  (wavelength_index) int32 5kB 0 1 2 3 ... 1237 1238 1239\n  * exposure          (exposure) int32 8B 10 20\n  * luminance         (luminance) int32 8B 0 10000\n  * variable          (variable) &lt;U8 32B 'datacube'xarray.DataArray'rad_ref'variable: 1cross_track: 905wavelength_index: 1240exposure: 2luminance: 2...[4488800 values with dtype=int32]Coordinates: (5)cross_track(cross_track)int320 1 2 3 4 5 ... 900 901 902 903 904array([  0,   1,   2, ..., 902, 903, 904], shape=(905,), dtype=int32)wavelength_index(wavelength_index)int320 1 2 3 4 ... 1236 1237 1238 1239array([   0,    1,    2, ..., 1237, 1238, 1239], shape=(1240,), dtype=int32)exposure(exposure)int3210 20array([10, 20], dtype=int32)luminance(luminance)int320 10000array([    0, 10000], dtype=int32)variable(variable)&lt;U8'datacube'array(['datacube'], dtype='&lt;U8')Indexes: (5)cross_trackPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       895, 896, 897, 898, 899, 900, 901, 902, 903, 904],\n      dtype='int32', name='cross_track', length=905))wavelength_indexPandasIndexPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239],\n      dtype='int32', name='wavelength_index', length=1240))exposurePandasIndexPandasIndex(Index([10, 20], dtype='int32', name='exposure'))luminancePandasIndexPandasIndex(Index([0, 10000], dtype='int32', name='luminance'))variablePandasIndexPandasIndex(Index(['datacube'], dtype='object', name='variable'))Attributes: (0)\n\n\n\n\nTransforms\nWe can apply a number of transforms to the camera’s raw data and these tranforms are used to modify the processing level during data collection. For example, we can perform a fast smile correction and wavelength binning during operation. With more processing, this is easily extended to obtain radiance and reflectance.\nSome transforms require some setup which is done using CameraProperties.tfm_setup. This method also allows one to tack on an additional setup function with the argument more_setup which takes in any callable which can mutate the CameraProperties class.\n\nsource\n\n\nCameraProperties.tfm_setup\n\n CameraProperties.tfm_setup\n                             (more_setup:Callable[[__main__.CameraProperti\n                             es],NoneType]=None, dtype:Union[numpy.uint8,n\n                             umpy.uint16,numpy.float32]=&lt;class\n                             'numpy.uint16'&gt;, lvl:int=0)\n\nSetup for transforms\n\nsource\n\n\nCameraProperties.crop\n\n CameraProperties.crop (x:numpy.ndarray)\n\nCrops to illuminated area\n\nsource\n\n\nCameraProperties.fast_smile\n\n CameraProperties.fast_smile (x:numpy.ndarray)\n\nApply the fast smile correction procedure\n\nsource\n\n\nCameraProperties.fast_bin\n\n CameraProperties.fast_bin (x:numpy.ndarray)\n\nChanges the view of the datacube so that everything that needs to be binned is in the last axis. The last axis is then binned.\n\nsource\n\n\nCameraProperties.slow_bin\n\n CameraProperties.slow_bin (x:numpy.ndarray)\n\nBins spectral bands accounting for the slight nonlinearity in the index-wavelength map\n\nsource\n\n\nCameraProperties.dn2rad\n\n CameraProperties.dn2rad (x:__main__.Array['λ,x',numpy.uint16])\n\nConverts digital numbers to radiance (uW/cm^2/sr/nm). Use after cropping to useable area.\n\nsource\n\n\nCameraProperties.rad2ref_6SV\n\n CameraProperties.rad2ref_6SV (x:__main__.Array['λ,x',numpy.float32])\n\n\nsource\n\n\nCameraProperties.set_processing_lvl\n\n CameraProperties.set_processing_lvl (lvl:int=-1,\n                                      custom_tfms:List[Callable[[numpy.nda\n                                      rray],numpy.ndarray]]=None)\n\nDefine the output lvl of the transform pipeline. Predefined recipies include: -1: do not apply any transforms (default), 0 : raw digital numbers cropped to useable sensor area, 1 : crop + fast smile, 2 : crop + fast smile + fast binning, 3 : crop + fast smile + slow binning, 4 : crop + fast smile + fast binning + conversion to radiance in units of uW/cm^2/sr/nm, 5 : crop + fast smile + radiance + fast binning, 6 : crop + fast smile + fast binning + radiance + reflectance, 7 : crop + fast smile + radiance + slow binning, 8 : crop + fast smile + radiance + slow binning + reflectance.\nYou can add your own tranform by monkey patching the CameraProperties class.\n@patch\ndef identity(self:CameraProperties, x:np.ndarray) -&gt; np.ndarray:\n    \"\"\"The identity tranform\"\"\"\n    return x\nIf you don’t require any camera settings or calibration files, a valid transform can be any Callable that takens in a 2D np.ndarray and returns a 2D np.ndarray.",
    "crumbs": [
      "Home",
      "api",
      "data"
    ]
  },
  {
    "objectID": "api/data.html#pipeline-for-composing-transforms",
    "href": "api/data.html#pipeline-for-composing-transforms",
    "title": "data",
    "section": "Pipeline for Composing Transforms",
    "text": "Pipeline for Composing Transforms\nDepending on the level of processing that one wants to do real-time, a number of transforms need to be composed in sequential order. To make this easy to customise, you can use the pipeline method and pass in a raw camera frame and an ordered list of transforms.\nTo make the transforms pipeline easy to use and customise, you can use the CameraProperties.set_processing_lvl method.\n\nsource\n\nCameraProperties.pipeline\n\n CameraProperties.pipeline (x:numpy.ndarray)\n\nCompose a list of transforms and apply to x.\n\n# if wavelength calibration is changed, this needs to be updated\n\ncam_prop = CameraProperties(\"../assets/cam_settings.json\",\"../assets/cam_calibration.nc\")\n\ncam_prop.set_processing_lvl(-1) # raw digital numbers\ntest_eq( (924,1240), np.shape( cam_prop.pipeline(cam_prop.calibration[\"HgAr_pic\"])) )\n\ncam_prop.set_processing_lvl(0) # cropped\ntest_eq( (905, 1240), np.shape( cam_prop.pipeline(cam_prop.calibration[\"HgAr_pic\"])) )\n\ncam_prop.set_processing_lvl(1) # fast smile corrected\ntest_eq( (905, 1231), np.shape( cam_prop.pipeline(cam_prop.calibration[\"HgAr_pic\"])) )\n\ncam_prop.set_processing_lvl(2) # fast binned\ntest_eq( (905, 136),  np.shape( cam_prop.pipeline(cam_prop.calibration[\"HgAr_pic\"])) )\n\ncam_prop.set_processing_lvl(4) # radiance\ntest_eq( (905, 136),  np.shape( cam_prop.pipeline(cam_prop.calibration[\"HgAr_pic\"])) )\n\n# cam_prop.set_processing_lvl(6) # reflectance\n# test_eq( (452,108),  np.shape( cam_prop.pipeline(cam_prop.calibration[\"HgAr_pic\"])) )\n\n# cam_prop.set_processing_lvl(5) # radiance conversion moved earlier in pipeline\n# test_eq( (452,108),  np.shape( cam_prop.pipeline(cam_prop.calibration[\"HgAr_pic\"])) )\n\ncam_prop = CameraProperties(\"../assets/cam_settings.json\",\"../assets/cam_calibration.nc\")  \ncam_prop.set_processing_lvl(3) # slow binned\ntest_eq( (905, 118),  np.shape( cam_prop.pipeline(cam_prop.calibration[\"HgAr_pic\"])) )",
    "crumbs": [
      "Home",
      "api",
      "data"
    ]
  },
  {
    "objectID": "api/data.html#buffer-for-data-collection",
    "href": "api/data.html#buffer-for-data-collection",
    "title": "data",
    "section": "Buffer for Data Collection",
    "text": "Buffer for Data Collection\nDataCube takes a line with coordinates of wavelength (x-axis) against cross-track (y-axis), and stores the smile corrected version in its CircArrayBuffer.\nFor facilitate near real-timing processing, a fast smile correction procedure is used. An option to use a fast binning procedure is also available. When using these two procedures, the overhead is roughly 2 ms on a Jetson board.\nInstead of preallocating another buffer for another data collect, one can use the circular nature of the DataCube and use the internal buffer again without modification - just use DataCube.put like normal.\n\nStorage Allocation\nAll data buffers are preallocated so it’s no secret that hyperspectral datacubes are memory hungry. For reference:\n\n\n\n\n\n\n\n\n\n\nalong-track pixels\nwavelength binning\nRAM needed\ntime to collect at 10 ms exposure\ntime to save to SSD\n\n\n\n\n4096\n4 nm\n≈ 800 MB\n≈ 55 s\n≈ 3 s\n\n\n1024\nno binning\n≈ 4 GB\n≈ 14 s\n≈ 15 s\n\n\n\nIn reality, it is very difficult to work with raw data without binning due to massive RAM usage and extended times to save the NetCDF file to disk which hinders making real-time analysis. The frame rate (at 10 s exposure) with binning drops the frame rate to from 90 fps to 75 fps. In our experimentation, using a SSD mounted into a M.2 slot on a Jetson board provided the fastest experience. When using other development boards such as a Raspberry Pi 4, the USB 3.0 port is recommended over the USB 2.0 port.\n\nsource\n\n\nDateTimeBuffer\n\n DateTimeBuffer (n:int=16)\n\nRecords timestamps in UTC time as datetime64[ns] for efficient storage and conversion.\n\nsource\n\n\nDateTimeBuffer.update\n\n DateTimeBuffer.update ()\n\nStore current UTC time with nanosecond precision.\n\ntimestamps = DateTimeBuffer(16)\nfor i in range(8):\n    timestamps.update()\n    \nprint(f\"{timestamps[0].strftime('%Y_%m_%d')}\")\n\ntimestamps.data\n\n2025_06_19\n\n\narray(['2025-06-19T04:52:51.078638000', '2025-06-19T04:52:51.078642000',\n       '2025-06-19T04:52:51.078642000', '2025-06-19T04:52:51.078643000',\n       '2025-06-19T04:52:51.078643000', '2025-06-19T04:52:51.078644000',\n       '2025-06-19T04:52:51.078644000', '2025-06-19T04:52:51.078644000',\n       '1970-01-01T00:00:00.000000008', '1970-01-01T00:00:00.000000009',\n       '1970-01-01T00:00:00.000000010', '1970-01-01T00:00:00.000000011',\n       '1970-01-01T00:00:00.000000012', '1970-01-01T00:00:00.000000013',\n       '1970-01-01T00:00:00.000000014', '1970-01-01T00:00:00.000000015'],\n      dtype='datetime64[ns]')\n\n\nSince datacubes can be incredibly demanding on RAM, our implementation includes a safety check so it’s not possible to accidentally allocate more memory than there is available memory. You can bypass this by using warn_mem_use=False although this is not recommended. For systems with adaquate swap management, this can work well but for development boards such as the Raspberry Pi 4, allocating more than the available memory will hang the operating system and you can’t do anything but forcibly unpower the board. (Learned from experience…)\n\n\n\n\n\n\nWarning\n\n\n\nAllocating more RAM than there is available is not recommended.\n\n\nThere are a few options to decrease your RAM usage: 1. Decrease n_lines, and 2. Use a processing_lvl&gt;=$$2 which includes real-time binning.\n\n\n\n\n\n\nTip\n\n\n\nYou can also close other programs running in the background which take up memory. For example, running the code in Jupyter Notebooks require a browser open which uses a significant chunk of RAM. You can experiment will smaller datacubes in a notebook but then run production code from a script instead if you do not require interactive widgets.\n\n\nIf you are trying to allocate $$80% of your available RAM, there will be a prompt to confirm if you want to continue. Respond with y if you want to continue or respond n to stop.\n\nsource\n\n\nDataCube\n\n DataCube (n_lines:int=16, processing_lvl:int=-1, warn_mem_use:bool=True,\n           json_path:str=None, cal_path:str=None,\n           print_settings:bool=False)\n\nFacilitates the collection, viewing, and saving of hyperspectral datacubes.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_lines\nint\n16\nHow many along-track pixels desired\n\n\nprocessing_lvl\nint\n-1\nDesired real time processing level\n\n\nwarn_mem_use\nbool\nTrue\nRaise error if trying to allocate too much memory (&gt; 80% of available RAM)\n\n\njson_path\nstr\nNone\n\n\n\ncal_path\nstr\nNone\n\n\n\nprint_settings\nbool\nFalse\n\n\n\n\n\nsource\n\n\nDataCube.put\n\n DataCube.put (x:numpy.ndarray)\n\nApplies the composed tranforms and writes the 2D array into the data cube. Stores a timestamp for each push.\n\nsource\n\n\nDataCube.to_xarray\n\n DataCube.to_xarray (metadata:dict=None, old_style:bool=False)\n\nConvert DataCube data to xarray Dataset format and stores at DataCube.nc, also returns the Dataset instead\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmetadata\ndict\nNone\nOptional metadata dictionary to include as attrs\n\n\nold_style\nbool\nFalse\nOrder of axis: True for (cross-track, along-track, wavelength), False for (wavelength, cross-track, along-track)\n\n\nReturns\nDataset\n\n\n\n\n\n\nsource\n\n\nDataCube.save\n\n DataCube.save (save_dir:str, preconfig_meta_path:str=None, prefix:str='',\n                suffix:str='', old_style:bool=False)\n\nSaves to a NetCDF file (and RGB representation) to directory dir_path in folder given by date with file name given by UTC time.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsave_dir\nstr\n\nPath to folder where all datacubes will be saved at\n\n\npreconfig_meta_path\nstr\nNone\nPath to a .json file that includes metadata fields to be saved inside datacube\n\n\nprefix\nstr\n\nPrepend a custom prefix to your file name\n\n\nsuffix\nstr\n\nAppend a custom suffix to your file name\n\n\nold_style\nbool\nFalse\nOrder of axis: True for (cross-track, along-track, wavelength), False for (wavelength, cross-track, along-track)\n\n\nReturns\nTuple\n\n\n\n\n\n\nsource\n\n\nDataCube.show\n\n DataCube.show (plot_lib:str='bokeh', red_nm:Tuple[float,float]=(640,\n                670), green_nm:Tuple[float,float]=(530, 590),\n                blue_nm:Tuple[float,float]=(450, 510),\n                robust:Union[bool,int]=False, hist_eq:bool=False,\n                quick_imshow:bool=False)\n\nGenerate an RGB image from chosen RGB wavelength bands with histogram equalisation or percentile options. RGB channels are created by averaging over specified wavelength ranges rather than single wavelengths for better SNR. The plotting backend can be specified by plot_lib and can be “bokeh” or “matplotlib”. quick_imshow is used for saving figures quickly but cannot be used to make interactive plots. Automatically creates xarray Dataset if not already available via to_xarray() method.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nplot_lib\nstr\nbokeh\nPlotting backend. This can be ‘bokeh’ or ‘matplotlib’\n\n\nred_nm\nTuple\n(640, 670)\nWavelength band in nm to use as the red channel (start, end)\n\n\ngreen_nm\nTuple\n(530, 590)\nWavelength band in nm to use as the green channel (start, end)\n\n\nblue_nm\nTuple\n(450, 510)\nWavelength band in nm to use as the blue channel (start, end)\n\n\nrobust\nUnion\nFalse\nSaturated linear stretch. E.g. setting robust to 2 will show the 2-98% percentile. Setting to True will default to robust=2. Robust to outliers\n\n\nhist_eq\nbool\nFalse\nChoose to plot using histogram equilisation\n\n\nquick_imshow\nbool\nFalse\nUsed to skip holoviews and use matplotlib for a static plot\n\n\nReturns\nPIL.Image\n\na bokeh or matplotlib plot\n\n\n\n\nsource\n\n\nDataCube.load_nc\n\n DataCube.load_nc (nc_path:str, old_style:bool=False,\n                   warn_mem_use:bool=True)\n\nLazy load a NetCDF datacube into the DataCube buffer.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnc_path\nstr\n\nPath to a NetCDF4 file\n\n\nold_style\nbool\nFalse\nOnly for backwards compatibility for datacubes created before first release\n\n\nwarn_mem_use\nbool\nTrue\nRaise error if trying to allocate too much memory (&gt; 80% of available RAM)\n\n\n\n\nsource\n\n\nDataCube.show\n\n DataCube.show (plot_lib:str='bokeh', red_nm:Tuple[float,float]=(640,\n                670), green_nm:Tuple[float,float]=(530, 590),\n                blue_nm:Tuple[float,float]=(450, 510),\n                robust:Union[bool,int]=False, hist_eq:bool=False,\n                quick_imshow:bool=False)\n\nGenerate an RGB image from chosen RGB wavelength bands with histogram equalisation or percentile options. RGB channels are created by averaging over specified wavelength ranges rather than single wavelengths for better SNR. The plotting backend can be specified by plot_lib and can be “bokeh” or “matplotlib”. quick_imshow is used for saving figures quickly but cannot be used to make interactive plots. Automatically creates xarray Dataset if not already available via to_xarray() method.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nplot_lib\nstr\nbokeh\nPlotting backend. This can be ‘bokeh’ or ‘matplotlib’\n\n\nred_nm\nTuple\n(640, 670)\nWavelength band in nm to use as the red channel (start, end)\n\n\ngreen_nm\nTuple\n(530, 590)\nWavelength band in nm to use as the green channel (start, end)\n\n\nblue_nm\nTuple\n(450, 510)\nWavelength band in nm to use as the blue channel (start, end)\n\n\nrobust\nUnion\nFalse\nSaturated linear stretch. E.g. setting robust to 2 will show the 2-98% percentile. Setting to True will default to robust=2. Robust to outliers\n\n\nhist_eq\nbool\nFalse\nChoose to plot using histogram equilisation\n\n\nquick_imshow\nbool\nFalse\nUsed to skip holoviews and use matplotlib for a static plot\n\n\nReturns\nPIL.Image\n\na bokeh or matplotlib plot\n\n\n\nload_nc expects the datacube to have coordinates (wavelength, cross-track, along-track). This is a format that can be viewed in ENVI and QGIS. If you have a datacube with coordinates (cross-track, along-track, wavelength), then set the parameter old_style=True.\nThe plot_lib argument hs Choose matplotlib if you want to use Choose bokeh if you want to compose plots together and use interactive tools.\n\nn = 256\n\ndc = DataCube(n_lines=n,processing_lvl=2,json_path=\"../assets/cam_settings.json\",cal_path=\"../assets/cam_calibration.nc\")\n\nnp.random.seed(0)  # keeps notebook from changing this data everytime run.\n\nfor i in range(200):\n    dc.put( np.random.randint(0,255,dc.settings[\"resolution\"]) )\n\ndc.show(\"bokeh\")\n\nAllocated 120.20 MB of RAM. There was 30718.33 MB available.",
    "crumbs": [
      "Home",
      "api",
      "data"
    ]
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Contributing",
    "section": "",
    "text": "Clone the repository (this is a shallow clone, remove depth tag for full history, much larger due to notebooks).\ngit clone --depth 1 git@github.com:openhsi/openhsi.git\ncd openhsi\nYou will need to install nbdev (v2) to extract the library and produce the documentation files from the notebooks. This should be done in the enviroment setup below.\nBefore anything else, please install the git hooks that run automatic scripts during each commit and merge to strip the notebooks of superfluous metadata (and avoid merge conflicts). After cloning the repository and setting enviroment, run the following command inside it:\nnbdev_install_hooks\n\n\n\nWe recommend miniconda (https://docs.conda.io/en/latest/miniconda.html) or Miniforge (https://github.com/conda-forge/miniforge) as the base eniviroment for minimal effort. However any modern python install (py&gt;3.6) should work.\n\n\nInstall all python dependancies for OpenHSI (excepts cameras) and 6SV.\nconda env create -f environment.yml\nconda develop .\n\n\n\npip install -e .\n\n\ngit clone https://github.com/robintw/6S.git\ncd 6S\ncmake -D CMAKE_INSTALL_PREFIX=/usr/local .\n\n\n\n\nThe Robonation OpenHSI uses the detectr fromLucid Vision systems. The full SDK is required to use the sensor with the OpenHSI libary. This can be aquirred from https://thinklucid.com/downloads-hub/.\nTo ensure opitmal performace you need to make sure your GigE link is setup for jumbo packets.\nOn Ubuntu system this can be done using (you may want to set this up to occur on startup):\nsudo ip link set eth0 mtu 9000\n\n\n\n\n\n    sudo setcap cap_sys_nice+ep readlink -f $(which python)\n    sudo setcap cap_sys_nice+ep readlink -f $(which jupyter)\n\n\n\n\n\nAny cell in the notebook marked with #export in the first line, will be extracted to generate the library. All other cells are used to create the documentation and to define tests.\nTo hide cells from appearing in the documentation, mark the cell with #hide in the first line. That’s it!\n\n\n\nAny cells you mark as #| export in the first line is automatically extracted. All other cells will appear in the documentation. If you don’t want cells to appear in the documentation, mark the first line with #hide. To extract the library, the terminal command is\nnbdev_export\n\n\n\nDocs are automatically created from the notebooks and deployed to GitHub Pages. To preview them use the terminal command\nnbdev_preview\n\n\n\nFirst fastforward your copy to include the latest change.\ngit pull\nPush your changes as usual.\ngit add .\ngit commit -m \"commit message\"\ngit push\n\n\n\n\nEnsure the bug was not already reported by searching on GitHub under Issues.\nIf you’re unable to find an open issue addressing the problem, open a new one. Be sure to include a title and clear description, as much relevant information as possible, and a code sample or an executable test case demonstrating the expected behavior that is not occurring.\nBe sure to add the complete error messages.\n\n\n\n\nOpen a new GitHub pull request with the patch.\nEnsure that your PR includes a test that fails without your patch, and pass with it.\nEnsure the PR description clearly describes the problem and solution. Include the relevant issue number if applicable.\n\n\n\n\n\n\nKeep each PR focused. While it’s more convenient, do not combine several unrelated fixes together. Create as many branches as needing to keep each PR focused.\nDo not mix style changes/fixes with “functional” changes. It’s very difficult to review such PRs and it most likely get rejected.\nDo not add/remove vertical whitespace. Preserve the original style of the file you edit as much as you can.\nDo not turn an already submitted PR into your development playground. If after you submitted PR, you discovered that more work is needed - close the PR, do the required work and then submit a new PR. Otherwise each of your commits requires attention from maintainers of the project.\nIf, however, you submitted a PR and received a request for changes, you should proceed with comm",
    "crumbs": [
      "Home",
      "Contributing"
    ]
  },
  {
    "objectID": "contributing.html#how-to-get-started",
    "href": "contributing.html#how-to-get-started",
    "title": "Contributing",
    "section": "",
    "text": "Clone the repository (this is a shallow clone, remove depth tag for full history, much larger due to notebooks).\ngit clone --depth 1 git@github.com:openhsi/openhsi.git\ncd openhsi\nYou will need to install nbdev (v2) to extract the library and produce the documentation files from the notebooks. This should be done in the enviroment setup below.\nBefore anything else, please install the git hooks that run automatic scripts during each commit and merge to strip the notebooks of superfluous metadata (and avoid merge conflicts). After cloning the repository and setting enviroment, run the following command inside it:\nnbdev_install_hooks",
    "crumbs": [
      "Home",
      "Contributing"
    ]
  },
  {
    "objectID": "contributing.html#setuping-up-python-enviroment",
    "href": "contributing.html#setuping-up-python-enviroment",
    "title": "Contributing",
    "section": "",
    "text": "We recommend miniconda (https://docs.conda.io/en/latest/miniconda.html) or Miniforge (https://github.com/conda-forge/miniforge) as the base eniviroment for minimal effort. However any modern python install (py&gt;3.6) should work.\n\n\nInstall all python dependancies for OpenHSI (excepts cameras) and 6SV.\nconda env create -f environment.yml\nconda develop .\n\n\n\npip install -e .\n\n\ngit clone https://github.com/robintw/6S.git\ncd 6S\ncmake -D CMAKE_INSTALL_PREFIX=/usr/local .\n\n\n\n\nThe Robonation OpenHSI uses the detectr fromLucid Vision systems. The full SDK is required to use the sensor with the OpenHSI libary. This can be aquirred from https://thinklucid.com/downloads-hub/.\nTo ensure opitmal performace you need to make sure your GigE link is setup for jumbo packets.\nOn Ubuntu system this can be done using (you may want to set this up to occur on startup):\nsudo ip link set eth0 mtu 9000\n\n\n\n\n\n    sudo setcap cap_sys_nice+ep readlink -f $(which python)\n    sudo setcap cap_sys_nice+ep readlink -f $(which jupyter)",
    "crumbs": [
      "Home",
      "Contributing"
    ]
  },
  {
    "objectID": "contributing.html#a-note-on-how-the-automation-tools-are-set-up",
    "href": "contributing.html#a-note-on-how-the-automation-tools-are-set-up",
    "title": "Contributing",
    "section": "",
    "text": "Any cell in the notebook marked with #export in the first line, will be extracted to generate the library. All other cells are used to create the documentation and to define tests.\nTo hide cells from appearing in the documentation, mark the cell with #hide in the first line. That’s it!",
    "crumbs": [
      "Home",
      "Contributing"
    ]
  },
  {
    "objectID": "contributing.html#extracting-library",
    "href": "contributing.html#extracting-library",
    "title": "Contributing",
    "section": "",
    "text": "Any cells you mark as #| export in the first line is automatically extracted. All other cells will appear in the documentation. If you don’t want cells to appear in the documentation, mark the first line with #hide. To extract the library, the terminal command is\nnbdev_export",
    "crumbs": [
      "Home",
      "Contributing"
    ]
  },
  {
    "objectID": "contributing.html#documentation",
    "href": "contributing.html#documentation",
    "title": "Contributing",
    "section": "",
    "text": "Docs are automatically created from the notebooks and deployed to GitHub Pages. To preview them use the terminal command\nnbdev_preview",
    "crumbs": [
      "Home",
      "Contributing"
    ]
  },
  {
    "objectID": "contributing.html#updating-changes-to-github",
    "href": "contributing.html#updating-changes-to-github",
    "title": "Contributing",
    "section": "",
    "text": "First fastforward your copy to include the latest change.\ngit pull\nPush your changes as usual.\ngit add .\ngit commit -m \"commit message\"\ngit push",
    "crumbs": [
      "Home",
      "Contributing"
    ]
  },
  {
    "objectID": "contributing.html#did-you-find-a-bug",
    "href": "contributing.html#did-you-find-a-bug",
    "title": "Contributing",
    "section": "",
    "text": "Ensure the bug was not already reported by searching on GitHub under Issues.\nIf you’re unable to find an open issue addressing the problem, open a new one. Be sure to include a title and clear description, as much relevant information as possible, and a code sample or an executable test case demonstrating the expected behavior that is not occurring.\nBe sure to add the complete error messages.\n\n\n\n\nOpen a new GitHub pull request with the patch.\nEnsure that your PR includes a test that fails without your patch, and pass with it.\nEnsure the PR description clearly describes the problem and solution. Include the relevant issue number if applicable.",
    "crumbs": [
      "Home",
      "Contributing"
    ]
  },
  {
    "objectID": "contributing.html#pr-submission-guidelines",
    "href": "contributing.html#pr-submission-guidelines",
    "title": "Contributing",
    "section": "",
    "text": "Keep each PR focused. While it’s more convenient, do not combine several unrelated fixes together. Create as many branches as needing to keep each PR focused.\nDo not mix style changes/fixes with “functional” changes. It’s very difficult to review such PRs and it most likely get rejected.\nDo not add/remove vertical whitespace. Preserve the original style of the file you edit as much as you can.\nDo not turn an already submitted PR into your development playground. If after you submitted PR, you discovered that more work is needed - close the PR, do the required work and then submit a new PR. Otherwise each of your commits requires attention from maintainers of the project.\nIf, however, you submitted a PR and received a request for changes, you should proceed with comm",
    "crumbs": [
      "Home",
      "Contributing"
    ]
  }
]