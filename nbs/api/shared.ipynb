{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: We want to collect datacubes nonstop. Saving is a blocking operation\n",
    "  which leaves gaps in the data we save on the order of a few seconds. The larger\n",
    "  the datacube, the longer this gap. We really don't want these gaps, yet we want\n",
    "  to save raw data. Saving raw data is extremely demanding (15 s of collect requires\n",
    "  15 s to save perhaps?). Can the saving be done in parallel? The following tries\n",
    "  to address this issue.\n",
    "output-file: shared.html\n",
    "title: shared\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp shared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "This module can be imported using `from openhsi.shared import *`\n",
    "\n",
    ":::\n",
    "\n",
    ":::{.callout-warning}\n",
    "\n",
    "Experimental\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# documentation extraction for class methods\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "# unit tests using test_eq(...)\n",
    "from fastcore.test import *\n",
    "\n",
    "# monkey patching class methods using @patch\n",
    "from fastcore.foundation import *\n",
    "from fastcore.foundation import patch\n",
    "\n",
    "# imitation of Julia's multiple dispatch using @typedispatch\n",
    "from fastcore.dispatch import typedispatch\n",
    "\n",
    "# bring forth **kwargs from an inherited class for documentation\n",
    "from fastcore.meta import delegates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "from fastcore.foundation import patch\n",
    "from fastcore.meta import delegates\n",
    "import numpy as np\n",
    "import ctypes\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import Iterable, Union, Callable, List, TypeVar, Generic, Tuple, Optional, Dict\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from openhsi.data import CameraProperties, CircArrayBuffer, DateTimeBuffer\n",
    "\n",
    "from ctypes import c_int32, c_uint32, c_float, c_uint16, c_uint8\n",
    "from multiprocessing import Process, Queue, Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class SharedCircArrayBuffer(CircArrayBuffer):\n",
    "    \"\"\"Circular FIFO Buffer implementation on multiprocessing.Array. Each put/get is a (n-1)darray.\"\"\"\n",
    "    \n",
    "    def __init__(self, size:tuple = (100,100), axis:int = 0, c_dtype:type = c_uint8, show_func:Callable[[np.ndarray],\"plot\"] = None):\n",
    "        \"\"\"Preallocate a array of `size` and type `c_dtype` and init write/read pointer. `c_dtype` needs to be from ctypes\"\"\"\n",
    "        \n",
    "        self.shared_data = Array(c_dtype, reduce(lambda x,y: x*y, size) )\n",
    "        self.data = np.frombuffer(self.shared_data.get_obj(),dtype=c_dtype)\n",
    "        self.data = self.data.reshape(size)\n",
    "        \n",
    "        self.size = size\n",
    "        self.axis = axis\n",
    "        self.write_pos = [slice(None,None,None) if i != axis else 0 for i in range(len(size)) ]\n",
    "        self.read_pos  = self.write_pos.copy()\n",
    "        self.slots_left = self.size[self.axis]\n",
    "        self.show_func = show_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@delegates()\n",
    "class SharedDataCube(CameraProperties):\n",
    "    \"\"\"Facilitates the collection, viewing, and saving of hyperspectral datacubes using\n",
    "    two `SharedCircArrayBuffer`s that swap when save is called.\"\"\"\n",
    "\n",
    "    def __init__(self, n_lines:int = 16, processing_lvl:int = -1, **kwargs):\n",
    "        \"\"\"Preallocate array buffers\"\"\"\n",
    "        self.n_lines = n_lines\n",
    "        self.proc_lvl = processing_lvl\n",
    "        super().__init__(**kwargs)\n",
    "        self.set_processing_lvl(processing_lvl)\n",
    "        self.dc_shape = (self.dc_shape[0],self.n_lines,self.dc_shape[1])\n",
    "        \n",
    "        self.dtype_out = c_uint8 if self.dtype_out is np.uint8 else self.dtype_out\n",
    "        self.dtype_out = c_uint16 if self.dtype_out is np.uint16 else self.dtype_out\n",
    "        self.dtype_out = c_int32 if self.dtype_out is np.int32 else self.dtype_out\n",
    "        self.dtype_out = c_float if self.dtype_out is np.float32 else self.dtype_out\n",
    "        \n",
    "        # Only one set of buffers can be used at a time\n",
    "        self.timestamps_swaps = [DateTimeBuffer(n_lines), DateTimeBuffer(n_lines)]\n",
    "        self.dc_swaps         = [SharedCircArrayBuffer(size=self.dc_shape, axis=1, c_dtype=self.dtype_out),\n",
    "                                 SharedCircArrayBuffer(size=self.dc_shape, axis=1, c_dtype=self.dtype_out)]\n",
    "        print(f\"Allocated {2*4*reduce(lambda x,y: x*y, self.dc_shape)/2**20:.02f} MB of RAM.\")\n",
    "        \n",
    "        self.current_swap = 0\n",
    "        self.timestamps   = self.timestamps_swaps[self.current_swap]\n",
    "        self.dc           = self.dc_swaps[self.current_swap]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"DataCube: shape = {self.dc_shape}, Processing level = {self.proc_lvl}\\n\"\n",
    "\n",
    "    def put(self, x:np.ndarray):\n",
    "        \"\"\"Applies the composed tranforms and writes the 2D array into the data cube. Stores a timestamp for each push.\"\"\"\n",
    "        self.timestamps.update()\n",
    "        self.dc.put( self.pipeline(x) )\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "def save(self:SharedDataCube, save_dir:str, preconfig_meta_path:str=None, prefix:str=\"\", suffix:str=\"\", old_style:bool=True) -> Process:\n",
    "    \"\"\"Saves to a NetCDF file (and RGB representation) to directory dir_path in folder given by date with file name given by UTC time.\n",
    "    Save is done in a separate multiprocess.Process.\"\"\"\n",
    "    if preconfig_meta_path is not None:\n",
    "        with open(preconfig_meta_path) as json_file:\n",
    "            attrs = json.load(json_file)\n",
    "    else: attrs = {}\n",
    "    \n",
    "    self.directory = Path(f\"{save_dir}/{self.timestamps[0].strftime('%Y_%m_%d')}/\").mkdir(parents=True, exist_ok=True)\n",
    "    self.directory = f\"{save_dir}/{self.timestamps[0].strftime('%Y_%m_%d')}\"\n",
    "    \n",
    "    wavelengths = self.binned_wavelengths if hasattr(self, \"binned_wavelengths\") else np.arange(self.dc.data.shape[2])\n",
    "    \n",
    "    if hasattr(self,\"cam_temperatures\"):\n",
    "        self.coords = dict(wavelength=([\"wavelength\"],wavelengths),\n",
    "                           x=([\"x\"],np.arange(self.dc.data.shape[0])),\n",
    "                           y=([\"y\"],np.arange(self.dc.data.shape[1])),\n",
    "                           time=([\"time\"],self.timestamps.data.astype(np.datetime64)),\n",
    "                           temperature=([\"temperature\"],self.cam_temperatures.data))\n",
    "    else:\n",
    "        self.coords = dict(wavelength=([\"wavelength\"],wavelengths),\n",
    "                           x=([\"x\"],np.arange(self.dc.data.shape[0])),\n",
    "                           y=([\"y\"],np.arange(self.dc.data.shape[1])),\n",
    "                           time=([\"time\"],self.timestamps.data.astype(np.datetime64)))\n",
    "        \n",
    "    fname = f\"{self.directory}/{prefix}{self.timestamps[0].strftime('%Y_%m_%d-%H_%M_%S')}{suffix}\"\n",
    "    \n",
    "    p = Process(target=save_shared_datacube, args=(fname,self.dc.shared_data,self.dtype_out,self.dc.size,self.coords,attrs,self.proc_lvl,old_style))\n",
    "    p.start()\n",
    "    print(f\"Saving {fname} in another process.\")\n",
    "    \n",
    "    self.current_swap = 0 if self.current_swap == 1 else 1\n",
    "    self.timestamps   = self.timestamps_swaps[self.current_swap]\n",
    "    self.dc           = self.dc_swaps[self.current_swap]\n",
    "    if hasattr(self,\"cam_temperatures\"):\n",
    "        self.cam_temperatures = self.cam_temps_swaps[self.current_swap]\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "def show(self:SharedDataCube,\n",
    "         plot_lib:str = \"bokeh\", # Plotting backend. This can be 'bokeh' or 'matplotlib'\n",
    "         red_nm:float = 640.,    # Wavelength in nm to use as the red\n",
    "         green_nm:float = 550.,  # Wavelength in nm to use as the green\n",
    "         blue_nm:float = 470.,   # Wavelength in nm to use as the blue\n",
    "         robust:bool = False,    # Choose to plot using the 2-98% percentile. Robust to outliers\n",
    "         hist_eq:bool = False,   # Choose to plot using histogram equilisation\n",
    "         quick_imshow:bool = False, # Used to skip holoviews and use matplotlib for a static plot\n",
    "         **plot_kwargs,          # Any other plotting options to be used in your plotting backend\n",
    "        ):\n",
    "    \"\"\"Generate an RGB image from chosen RGB wavelengths with histogram equalisation or percentile options.\n",
    "    The plotting backend can be specified by `plot_lib` and can be \"bokeh\" or \"matplotlib\".\n",
    "    Further customise your plot with `**plot_kwargs`. `quick_imshow` is used for saving figures quickly\n",
    "    but cannot be used to make interactive plots. \"\"\"\n",
    "\n",
    "    rgb = np.zeros( (*self.dc.data.shape[:2],3), dtype=np.float32)\n",
    "    if hasattr(self, \"binned_wavelengths\"):\n",
    "        rgb[...,0] = self.dc.data[:,:,np.argmin(np.abs(self.binned_wavelengths-red_nm))]\n",
    "        rgb[...,1] = self.dc.data[:,:,np.argmin(np.abs(self.binned_wavelengths-green_nm))]\n",
    "        rgb[...,2] = self.dc.data[:,:,np.argmin(np.abs(self.binned_wavelengths-blue_nm))]\n",
    "    else:\n",
    "        rgb[...,0] = self.dc.data[:,:,int(self.dc.data.shape[2] / 2)]\n",
    "        rgb[...,1] = self.dc.data[:,:,int(self.dc.data.shape[2] / 2)]\n",
    "        rgb[...,2] = self.dc.data[:,:,int(self.dc.data.shape[2] / 2)]\n",
    "\n",
    "    if robust and not hist_eq: # scale everything to the 2% and 98% percentile\n",
    "        vmax = np.nanpercentile(rgb, 98)\n",
    "        vmin = np.nanpercentile(rgb, 2)\n",
    "        rgb = ((rgb.astype(\"f8\") - vmin) / (vmax - vmin)).astype(\"f4\")\n",
    "        rgb = np.minimum(np.maximum(rgb, 0), 1)\n",
    "    elif hist_eq and not robust:\n",
    "        img_hist, bins = np.histogram(rgb.flatten(), 256, density=True)\n",
    "        cdf = img_hist.cumsum() # cumulative distribution function\n",
    "        cdf = 1. * cdf / cdf[-1] # normalize\n",
    "        img_eq = np.interp(rgb.flatten(), bins[:-1], cdf) # find new pixel values from linear interpolation of cdf\n",
    "        rgb = img_eq.reshape(rgb.shape)\n",
    "    elif robust and hist_eq:\n",
    "        warnings.warn(\"Cannot mix robust with histogram equalisation. No RGB adjustments will be made.\",stacklevel=2)\n",
    "        rgb /= np.max(rgb)\n",
    "    else:\n",
    "        rgb /= np.max(rgb)\n",
    "\n",
    "    if quick_imshow:\n",
    "        fig, ax = plt.subplots(figsize=(12,3))\n",
    "        ax.imshow(rgb,aspect=\"equal\"); ax.set_xlabel(\"along-track\"); ax.set_ylabel(\"cross-track\")\n",
    "        return fig\n",
    "\n",
    "    import holoviews as hv\n",
    "    hv.extension(plot_lib,logo=False)\n",
    "    rgb_hv = hv.RGB((np.arange(rgb.shape[1]),np.arange(rgb.shape[0]),\n",
    "                     rgb[:,:,0],rgb[:,:,1],rgb[:,:,2]))\n",
    "\n",
    "    if plot_lib == \"bokeh\":\n",
    "        return rgb_hv.opts(width=1000,height=250,frame_height=int(rgb.shape[0]//3)).opts(**plot_kwargs).opts(\n",
    "            xlabel=\"along-track\",ylabel=\"cross-track\",invert_yaxis=True)\n",
    "    else: # plot_lib == \"matplotlib\"\n",
    "        return rgb_hv.opts(fig_inches=22).opts(**plot_kwargs).opts(\n",
    "            xlabel=\"along-track\",ylabel=\"cross-track\",invert_yaxis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def save_shared_datacube(fname:str,          # NetCDF4 file name (without .nc)\n",
    "                         shared_array:Array, # multiprocessing.Array shared array \n",
    "                         c_dtype:type,       # numpy data type\n",
    "                         shape:Tuple,        # datacube numpy shape\n",
    "                         coords_dict:Dict,   # coordinates dictionary\n",
    "                         attrs_dict:Dict,    # metadata dictionary\n",
    "                         proc_lvl:int,       # processing level used\n",
    "                         old_style:bool=True,# order of axis\n",
    "                         savefig:bool=False  # save a preview figure of cube\n",
    "                        ):\n",
    "    \"\"\"Saves a NetCDF4 file given all the function parameters. Designed to be used with SharedOpenHSI which allocates a shared array.\"\"\"\n",
    "    \n",
    "    data = np.frombuffer(shared_array.get_obj(),dtype=c_dtype)\n",
    "    data = data.reshape(shape)\n",
    "\n",
    "#     nc = xr.Dataset(data_vars=dict(datacube=([\"wavelength\",\"x\",\"y\"],np.moveaxis(data, -1, 0) )),\n",
    "#                          coords=coords_dict, attrs=attrs_dict)  \n",
    "    \n",
    "    nc = xr.Dataset(data_vars=dict(datacube=([\"x\",\"y\",\"wavelength\"], data)),\n",
    "                    coords=coords_dict, \n",
    "                    attrs=attrs_dict)\n",
    "    \n",
    "    if old_style: # cross-track, along-track, wavelength\n",
    "        self.nc = xr.Dataset(data_vars=dict(datacube=([\"x\",\"y\",\"wavelength\"], self.dc.data)),\n",
    "                             coords=coords_dict, \n",
    "                             attrs=attrs_dict)  \n",
    "    else: # wavelength, cross-track, along-track\n",
    "        self.nc = xr.Dataset(data_vars=dict(datacube=([\"wavelength\",\"x\",\"y\"],np.moveaxis(self.dc.data, -1, 0) )),\n",
    "                             coords=coords_dict, \n",
    "                             attrs=attrs_dict)\n",
    "    \n",
    "    \"\"\"provide metadata to NetCDF coordinates\"\"\"\n",
    "    nc.x.attrs[\"long_name\"]   = \"cross-track\"\n",
    "    nc.x.attrs[\"units\"]       = \"pixels\"\n",
    "    nc.x.attrs[\"description\"] = \"cross-track spatial coordinates\"\n",
    "    nc.y.attrs[\"long_name\"]   = \"along-track\"\n",
    "    nc.y.attrs[\"units\"]       = \"pixels\"\n",
    "    nc.y.attrs[\"description\"] = \"along-track spatial coordinates\"\n",
    "    nc.time.attrs[\"long_name\"]   = \"along-track\"\n",
    "    nc.time.attrs[\"description\"] = \"along-track spatial coordinates\"\n",
    "    nc.wavelength.attrs[\"long_name\"]   = \"wavelength_nm\"\n",
    "    nc.wavelength.attrs[\"units\"]       = \"nanometers\"\n",
    "    nc.wavelength.attrs[\"description\"] = \"wavelength in nanometers.\"\n",
    "    \n",
    "    if \"temperature\" in coords_dict.keys():\n",
    "        nc.temperature.attrs[\"long_name\"] = \"camera temperature\"\n",
    "        nc.temperature.attrs[\"units\"] = \"degrees Celsius\"\n",
    "        nc.temperature.attrs[\"description\"] = \"temperature of sensor at time of image capture\"\n",
    "\n",
    "    nc.datacube.attrs[\"long_name\"]   = \"hyperspectral datacube\"\n",
    "    nc.datacube.attrs[\"units\"]       = \"digital number\"\n",
    "    if proc_lvl in (4,5,7): nc.datacube.attrs[\"units\"] = \"uW/cm^2/sr/nm\"\n",
    "    elif proc_lvl in (6,8): nc.datacube.attrs[\"units\"] = \"percentage reflectance\"\n",
    "    nc.datacube.attrs[\"description\"] = \"hyperspectral datacube\"\n",
    "    \n",
    "    nc.to_netcdf(fname+\".nc\")\n",
    "    \n",
    "    if savefig:\n",
    "        # quick save the histogram equalised RGB\n",
    "        rgb = np.zeros( (*shape[:2],3), dtype=np.float32)\n",
    "        rgb[...,0] = data[:,:,np.argmin(np.abs(coords_dict[\"wavelength\"][1]-640.))]\n",
    "        rgb[...,1] = data[:,:,np.argmin(np.abs(coords_dict[\"wavelength\"][1]-550.))]\n",
    "        rgb[...,2] = data[:,:,np.argmin(np.abs(coords_dict[\"wavelength\"][1]-470.))]\n",
    "        img_hist, bins = np.histogram(rgb.flatten(), 256, density=True)\n",
    "        cdf = img_hist.cumsum() # cumulative distribution function\n",
    "        cdf = 1. * cdf / cdf[-1] # normalize\n",
    "        img_eq = np.interp(rgb.flatten(), bins[:-1], cdf) # find new pixel values from linear interpolation of cdf\n",
    "        rgb = img_eq.reshape(rgb.shape)\n",
    "        fig, ax = plt.subplots(figsize=(12,3))\n",
    "        ax.imshow(rgb,aspect=\"equal\"); ax.set_xlabel(\"along-track\"); ax.set_ylabel(\"cross-track\")\n",
    "        fig.savefig(fname+\".png\",bbox_inches='tight', pad_inches=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenHSI using shared multiprocessing.Array in SharedDataCube\n",
    "\n",
    "`SharedOpenHSI` has the same API as `OpenHSI` with the addition of a camera temperature buffer that automatically swaps over when a save is called. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@delegates()\n",
    "class SharedOpenHSI(SharedDataCube):\n",
    "    \"\"\"Base Class for the OpenHSI Camera.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        super().set_processing_lvl(self.proc_lvl)\n",
    "        if callable(getattr(self,\"get_temp\",None)):\n",
    "            self.cam_temps_swaps  = [CircArrayBuffer(size=(self.n_lines,),dtype=np.float32),\n",
    "                                     CircArrayBuffer(size=(self.n_lines,),dtype=np.float32)]\n",
    "            self.cam_temperatures = self.cam_temps_swaps[self.current_swap]\n",
    "        \n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __close__(self):\n",
    "        self.stop_cam()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.stop_cam()\n",
    "        \n",
    "    def collect(self):\n",
    "        \"\"\"Collect the hyperspectral datacube.\"\"\"\n",
    "        #self.start_cam()\n",
    "        for i in tqdm(range(self.n_lines)):\n",
    "            self.put(self.get_img())\n",
    "            \n",
    "            if callable(getattr(self,\"get_temp\",None)):\n",
    "                self.cam_temperatures.put( self.get_temp() )\n",
    "        #self.stop_cam()\n",
    "        \n",
    "    def avgNimgs(self, n) -> np.ndarray:\n",
    "        \"\"\"Take `n` images and find the average\"\"\"\n",
    "        data = np.zeros(tuple(self.settings['resolution'])+(n,),np.float32)\n",
    "        \n",
    "        self.start_cam()\n",
    "        for f in range(n):\n",
    "            data[:,:,f]=self.get_img()\n",
    "        self.stop_cam()\n",
    "        return np.mean(data,axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [https://openhsi.github.io/openhsi/capture.html](https://openhsi.github.io/openhsi/capture.html) for an example of how to use `SharedOpenHSI` for you custom cameras. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Cameras\n",
    "\n",
    "These are exported by openhsi.cameras as `SharedXXXCamera` This should work just like standard `Camera`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hardware\n",
    "#| eval: false\n",
    "\n",
    "from openhsi.cameras import SharedXimeaCamera\n",
    "\n",
    "num_saved = 0\n",
    "with SharedXimeaCamera(n_lines=128, exposure_ms=1, processing_lvl = -1, cal_path=\"\",json_path='../assets/cam_settings_ximea.json') as cam:\n",
    "    for i in range(10):\n",
    "        cam.collect()\n",
    "        print(f\"collected from time: {cam.timestamps.data[0]} to {cam.timestamps.data[-1]}\")\n",
    "        if num_saved > 0:\n",
    "            p.join() # wait for the last process to finish so we don't modify the data when it's being saved \n",
    "            pass\n",
    "        \n",
    "        p = cam.save(\"../hyperspectral_experiments/temp\")\n",
    "        num_saved += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
